{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Wurzel","text":"<p>Wurzel is an open-source Python library built to address advanced Extract, Transform, Load (ETL) needs for Retrieval-Augmented Generation (RAG) systems. It is designed to streamline ETL processes while offering essential features like multi-tenancy, cloud-native deployment support, and job scheduling.</p> <p>The repository includes initial implementations for widely-used frameworks in the RAG ecosystem, such as Qdrant, Milvus, and Hugging Face, providing users with a strong starting point for building scalable and efficient RAG pipelines.</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Advanced ETL Pipelines: Tailored for the specific needs of RAG systems.</li> <li>Multi-Tenancy: Easily manage multiple tenants or projects within a single system.</li> <li>Cloud-Native Deployment: Designed for seamless integration with Kubernetes, Docker, and other cloud platforms.</li> <li>Scheduling Capabilities: Schedule and manage ETL tasks using built-in or external tools.</li> <li>Framework Integrations: Pre-built support for popular tools like Qdrant, Milvus, and Hugging Face.</li> <li>Type Security: By leveraging capabilities of pydantic and pandera we ensure type security</li> </ul>"},{"location":"backends/","title":"Backend Architecture in Wurzel","text":""},{"location":"backends/#what-are-backends","title":"What are Backends?","text":"<p>Backends in Wurzel are powerful abstractions that transform your pipeline definitions into executable configurations for different orchestration platforms. Think of them as translators that take your high-level pipeline logic and convert it into the specific format required by your target execution environment.</p>"},{"location":"backends/#why-backends-are-great","title":"Why Backends are Great","text":""},{"location":"backends/#write-once-deploy-anywhere","title":"\ud83d\ude80 Write Once, Deploy Anywhere","text":"<p>Define your data pipeline logic once using Wurzel's intuitive API, then deploy it to multiple platforms without rewriting code. Whether you need local development with DVC, cloud-native execution with Argo Workflows, or future platforms like GitHub Actions - your pipeline logic remains the same.</p>"},{"location":"backends/#platform-specific-optimization","title":"\ud83d\udd27 Platform-Specific Optimization","text":"<p>Each backend is specifically designed to leverage the unique capabilities of its target platform:</p> <ul> <li>DVC Backend: Optimizes for data versioning, experiment tracking, and reproducible ML workflows</li> <li>Argo Backend: Leverages Kubernetes-native features like horizontal scaling, resource management, and cloud-native scheduling</li> </ul>"},{"location":"backends/#environment-aware-configuration","title":"\ud83c\udfaf Environment-Aware Configuration","text":"<p>Backends automatically handle environment-specific concerns:</p> <ul> <li>Container orchestration and resource allocation</li> <li>Storage and artifact management</li> <li>Scheduling and triggering mechanisms</li> <li>Security and access control integration</li> </ul>"},{"location":"backends/#scalability-without-complexity","title":"\ud83d\udcc8 Scalability Without Complexity","text":"<p>Start with simple local execution and seamlessly scale to enterprise-grade orchestration platforms. Backends abstract away the complexity of different deployment targets while preserving the power and flexibility of each platform.</p>"},{"location":"backends/#how-backends-work","title":"How Backends Work","text":"<ol> <li>Pipeline Definition: You define your pipeline using Wurzel's step classes and the <code>WZ</code> utility</li> <li>Backend Selection: Choose the appropriate backend for your target environment</li> <li>Code Generation: The backend generates platform-specific configuration files</li> <li>Execution: Deploy and run using the native tools of your chosen platform</li> </ol>"},{"location":"backends/#generate-time-vs-runtime-configuration","title":"Generate-Time vs Runtime Configuration","text":"<p>Wurzel backends use a two-phase configuration model that separates concerns:</p>"},{"location":"backends/#generate-time-configuration-yaml","title":"Generate-Time Configuration (YAML)","text":"<p>At generate-time (<code>wurzel generate</code>), a <code>values.yaml</code> file configures the infrastructure and workflow structure:</p> <ul> <li>Container images and registries</li> <li>Kubernetes namespaces and service accounts</li> <li>Cron schedules and triggers</li> <li>Security contexts and resource limits</li> <li>Artifact storage (S3 buckets, endpoints)</li> <li>Data directories</li> </ul> <p>This configuration is baked into the generated artifacts (e.g., <code>cronworkflow.yaml</code>, <code>dvc.yaml</code>).</p>"},{"location":"backends/#runtime-configuration-environment-variables","title":"Runtime Configuration (Environment Variables)","text":"<p>At runtime (when the pipeline executes), step settings are read from environment variables:</p> <ul> <li><code>MANUALMARKDOWNSTEP__FOLDER_PATH</code> - where to read markdown files</li> <li><code>SIMPLESPLITTERSTEP__BATCH_SIZE</code> - processing batch size</li> <li><code>EMBEDDINGSTEP__MODEL_NAME</code> - which embedding model to use</li> </ul> <p>These can be changed without regenerating the workflow artifacts.</p>"},{"location":"backends/#why-this-separation","title":"Why This Separation?","text":"Aspect Generate-Time (YAML) Runtime (Env Vars) When applied <code>wurzel generate</code> Pipeline execution What it configures Infrastructure, workflow structure Step behavior, business logic Change frequency Rarely (infra changes) Often (per environment) Examples Container image, namespace, schedule Model paths, batch sizes, API keys <p>This allows you to:</p> <ul> <li>Generate workflow artifacts once and deploy to multiple environments</li> <li>Store sensitive runtime config in Kubernetes Secrets</li> <li>Change step behavior without rebuilding containers or regenerating workflows</li> </ul>"},{"location":"backends/#available-backends","title":"Available Backends","text":"<ul> <li>DVC Backend: For data versioning and ML experiment tracking</li> <li>Argo Workflows Backend: For Kubernetes-native pipeline orchestration</li> </ul>"},{"location":"backends/#future-backends","title":"Future Backends","text":"<p>Wurzel's extensible architecture supports adding new backends for:</p> <ul> <li>GitLab CI/CD: For generating <code>.gitlab-ci.yml</code> pipelines</li> <li>GitHub Actions: To produce <code>workflow.yml</code> for GitHub-native automation</li> <li>Apache Airflow: For DAG-based orchestration and scheduling</li> <li>LocalBackend: Execute steps locally without an external orchestrator</li> <li>Kubernetes CronJobs: Direct Kubernetes-native <code>CronJob</code> manifests</li> </ul>"},{"location":"backends/argoworkflows/","title":"Argo Workflows Backend","text":"<p>The Argo Workflows Backend transforms your Wurzel pipeline into Kubernetes-native Workflow or CronWorkflow YAML configurations, enabling cloud-native, scalable pipeline orchestration with optional scheduling capabilities.</p>"},{"location":"backends/argoworkflows/#overview","title":"Overview","text":"<p>Argo Workflows is a powerful, Kubernetes-native workflow engine that excels at container orchestration and parallel execution. The Argo Backend generates either:</p> <ul> <li><code>Workflow</code>: For one-time or manually triggered pipeline executions (when <code>schedule: null</code>)</li> <li><code>CronWorkflow</code>: For scheduled, recurring pipeline executions (when a cron schedule is provided)</li> </ul> <p>Both workflow types leverage Kubernetes' native scheduling and resource management capabilities.</p> <p>Generate-Time vs Runtime Configuration</p> <p>The Argo backend uses a two-phase configuration model:</p> <ul> <li>Generate-Time (YAML): A <code>values.yaml</code> file configures the workflow structure \u2014 container images, namespaces, schedules, security contexts, resource limits, and artifact storage. This is required when running <code>wurzel generate</code>.</li> <li>Runtime (Environment Variables): Step settings (e.g., <code>MANUALMARKDOWNSTEP__FOLDER_PATH</code>) are read from environment variables when the workflow executes in Kubernetes. These can be set via <code>container.env</code>, Secrets, or ConfigMaps in your <code>values.yaml</code>.</li> </ul> <p>This separation allows you to generate workflow manifests once and deploy them to different environments by changing only the runtime environment variables.</p>"},{"location":"backends/argoworkflows/#key-features","title":"Key Features","text":"<ul> <li>Cloud-Native Orchestration: Run pipelines natively on Kubernetes clusters</li> <li>Flexible Execution: Support for both one-time Workflows and scheduled CronWorkflows</li> <li>Horizontal Scaling: Automatically scale pipeline steps based on resource requirements</li> <li>Advanced Scheduling: Optional cron-based scheduling with fine-grained control</li> <li>Resource Management: Leverage Kubernetes resource limits and requests</li> <li>Artifact Management: Integrated S3-compatible artifact storage</li> <li>Service Integration: Seamless integration with Kubernetes services and secrets</li> </ul>"},{"location":"backends/argoworkflows/#usage","title":"Usage","text":""},{"location":"backends/argoworkflows/#installation","title":"Installation","text":"<p>Install Wurzel with Argo support:</p> <pre><code>pip install wurzel[argo]\n</code></pre>"},{"location":"backends/argoworkflows/#cli-usage","title":"CLI Usage","text":"<p>Generate an Argo Workflows configuration using a <code>values.yaml</code> file:</p> <pre><code># Generate a CronWorkflow with scheduled execution\nwurzel generate --backend ArgoBackend \\\n    --values values.yaml \\\n    --pipeline_name pipelinedemo \\\n    --output cronworkflow.yaml \\\n    examples.pipeline.pipelinedemo:pipeline\n\n# Or generate a one-time Workflow (set schedule: null in values.yaml)\nwurzel generate --backend ArgoBackend \\\n    --values values-no-schedule.yaml \\\n    --pipeline_name pipelinedemo \\\n    --output workflow.yaml \\\n    examples.pipeline.pipelinedemo:pipeline\n</code></pre> <p>Note</p> <p>The <code>--values</code> flag is required for the Argo backend. It specifies the YAML configuration file that defines the workflow structure.</p>"},{"location":"backends/argoworkflows/#values-file-configuration-generate-time","title":"Values File Configuration (Generate-Time)","text":"<p>The <code>values.yaml</code> file configures the workflow structure at generate-time. Here's a complete example:</p> <pre><code>workflows:\n  pipelinedemo:\n    # Workflow metadata\n    name: wurzel-pipeline\n    namespace: argo-workflows\n    schedule: \"0 4 * * *\"  # Cron schedule for CronWorkflow, or null for one-time Workflow\n    entrypoint: wurzel-pipeline\n    serviceAccountName: wurzel-service-account\n    dataDir: /data\n\n    # Workflow-level annotations\n    annotations:\n      sidecar.istio.io/inject: \"false\"\n\n    # Pod-level security context (applied to all pods)\n    podSecurityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      runAsGroup: 1000\n      fsGroup: 2000\n      fsGroupChangePolicy: Always  # or \"OnRootMismatch\"\n      supplementalGroups:\n        - 1000\n      seccompProfileType: RuntimeDefault\n\n    # Optional: Custom podSpecPatch for advanced use cases\n    # podSpecPatch: |\n    #   initContainers:\n    #     - name: custom-init\n    #       securityContext:\n    #         runAsNonRoot: true\n\n    # Container configuration\n    container:\n      image: ghcr.io/telekom/wurzel\n\n      # Container-level security context\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        runAsGroup: 1000\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        dropCapabilities:\n          - ALL\n        seccompProfileType: RuntimeDefault\n\n      # Resource requests and limits\n      resources:\n        cpu_request: \"100m\"\n        cpu_limit: \"500m\"\n        memory_request: \"128Mi\"\n        memory_limit: \"512Mi\"\n\n      # Runtime environment variables (step settings)\n      env:\n        MANUALMARKDOWNSTEP__FOLDER_PATH: \"examples/pipeline/demo-data\"\n        SIMPLESPLITTERSTEP__BATCH_SIZE: \"100\"\n\n      # Environment from Kubernetes Secrets/ConfigMaps\n      envFrom:\n        - kind: secret\n          name: wurzel-env-secret\n          prefix: \"\"\n          optional: true\n        - kind: configMap\n          name: wurzel-env-config\n          prefix: APP_\n          optional: true\n\n      # Reference existing secrets as env vars\n      secretRef:\n        - \"wurzel-secrets\"\n\n      # Reference existing configmaps as env vars\n      configMapRef:\n        - \"wurzel-config\"\n\n      # Mount secrets as files\n      mountSecrets:\n        - from: \"tls-secret\"\n          to: \"/etc/ssl\"\n          mappings:\n            - key: \"tls.crt\"\n              value: \"cert.pem\"\n            - key: \"tls.key\"\n              value: \"key.pem\"\n\n      # Tokenizer cache volume (for HuggingFace models)\n      tokenizerCache:\n        enabled: true\n        claimName: tokenizer-cache-pvc  # Used when createPvc: false\n        mountPath: /cache/huggingface\n        readOnly: true\n        # To auto-create a workflow-scoped PVC:\n        # createPvc: true\n        # storageSize: 10Gi\n        # storageClassName: standard\n        # accessModes: [\"ReadWriteOnce\"]\n\n    # S3 artifact storage configuration\n    artifacts:\n      bucket: wurzel-bucket\n      endpoint: s3.amazonaws.com\n      defaultMode: 509  # File permissions (decimal), e.g., 509 = 0o775\n</code></pre>"},{"location":"backends/argoworkflows/#workflow-vs-cronworkflow","title":"Workflow vs CronWorkflow","text":"<p>The Argo backend generates different workflow types based on the <code>schedule</code> configuration:</p>"},{"location":"backends/argoworkflows/#normal-workflow-one-time-execution","title":"Normal Workflow (One-Time Execution)","text":"<p>Set <code>schedule: null</code> (or omit it) to create a Workflow for manual or one-time execution:</p> <pre><code>workflows:\n  my-workflow:\n    name: my-pipeline\n    schedule: null  # Creates a Workflow, not a CronWorkflow\n</code></pre> <p>Use cases: - Manual pipeline execution triggered via Argo UI or CLI - Event-driven pipelines triggered by other workflows - One-time data processing tasks - CI/CD integration where external systems trigger execution</p> <p>Triggering: <pre><code># Submit the workflow manually\nargo submit workflow.yaml\n\n# Or trigger via kubectl\nkubectl create -f workflow.yaml\n</code></pre></p>"},{"location":"backends/argoworkflows/#cronworkflow-scheduled-execution","title":"CronWorkflow (Scheduled Execution)","text":"<p>Set a cron schedule string to create a CronWorkflow for recurring execution:</p> <pre><code>workflows:\n  my-cron-workflow:\n    name: my-scheduled-pipeline\n    schedule: \"0 4 * * *\"  # Creates a CronWorkflow that runs daily at 4 AM\n</code></pre> <p>Use cases: - Regularly scheduled data ingestion - Periodic model training or evaluation - Automated report generation - Scheduled data synchronization</p> <p>Common cron schedules: - <code>\"0 4 * * *\"</code> - Daily at 4 AM - <code>\"*/15 * * * *\"</code> - Every 15 minutes - <code>\"0 0 * * 0\"</code> - Weekly on Sundays at midnight - <code>\"0 0 1 * *\"</code> - Monthly on the 1<sup>st</sup> at midnight</p> <p>Monitoring: <pre><code># List all CronWorkflows\nargo cron list\n\n# View CronWorkflow details\nargo cron get my-scheduled-pipeline\n\n# List workflow runs from CronWorkflow\nargo list --label workflows.argoproj.io/cron-workflow=my-scheduled-pipeline\n</code></pre></p> <p>Choosing the Right Type</p> <ul> <li>Use Workflow (schedule: null) when you need explicit control over when pipelines run</li> <li>Use CronWorkflow (with schedule) for automated, time-based execution</li> <li>You can have both: a CronWorkflow for regular execution and a Workflow template for manual reruns</li> </ul>"},{"location":"backends/argoworkflows/#configuration-reference","title":"Configuration Reference","text":""},{"location":"backends/argoworkflows/#workflow-level-options","title":"Workflow-Level Options","text":"Field Type Default Description <code>name</code> string <code>wurzel</code> Name of the Workflow/CronWorkflow <code>namespace</code> string <code>argo-workflows</code> Kubernetes namespace <code>schedule</code> string <code>0 4 * * *</code> Cron schedule for CronWorkflow. Set to <code>null</code> to create a normal Workflow instead <code>entrypoint</code> string <code>wurzel-pipeline</code> DAG entrypoint name <code>serviceAccountName</code> string <code>wurzel-service-account</code> Kubernetes service account <code>dataDir</code> path <code>/usr/app</code> Data directory inside containers <code>annotations</code> map <code>{}</code> Workflow-level annotations <code>podSpecPatch</code> string <code>null</code> Custom pod spec patch (YAML string)"},{"location":"backends/argoworkflows/#pod-security-context-options","title":"Pod Security Context Options","text":"Field Type Default Description <code>runAsNonRoot</code> bool <code>true</code> Require non-root user <code>runAsUser</code> int <code>null</code> UID to run as <code>runAsGroup</code> int <code>null</code> GID to run as <code>fsGroup</code> int <code>null</code> Filesystem group <code>fsGroupChangePolicy</code> string <code>null</code> <code>Always</code> or <code>OnRootMismatch</code> <code>supplementalGroups</code> list[int] <code>[]</code> Additional group IDs <code>seccompProfileType</code> string <code>RuntimeDefault</code> Seccomp profile type"},{"location":"backends/argoworkflows/#container-security-context-options","title":"Container Security Context Options","text":"Field Type Default Description <code>runAsNonRoot</code> bool <code>true</code> Require non-root user <code>runAsUser</code> int <code>null</code> UID to run as <code>runAsGroup</code> int <code>null</code> GID to run as <code>allowPrivilegeEscalation</code> bool <code>false</code> Allow privilege escalation <code>readOnlyRootFilesystem</code> bool <code>null</code> Read-only root filesystem <code>dropCapabilities</code> list[str] <code>[\"ALL\"]</code> Linux capabilities to drop <code>seccompProfileType</code> string <code>RuntimeDefault</code> Seccomp profile type"},{"location":"backends/argoworkflows/#container-resources-options","title":"Container Resources Options","text":"Field Type Default Description <code>cpu_request</code> string <code>100m</code> CPU request <code>cpu_limit</code> string <code>500m</code> CPU limit <code>memory_request</code> string <code>128Mi</code> Memory request <code>memory_limit</code> string <code>512Mi</code> Memory limit"},{"location":"backends/argoworkflows/#tokenizer-cache-options","title":"Tokenizer Cache Options","text":"<p>The tokenizer cache configuration allows you to mount a PersistentVolumeClaim (PVC) containing pre-downloaded HuggingFace tokenizer models. This is useful for:</p> <ul> <li>Avoiding repeated model downloads in air-gapped environments</li> <li>Reducing startup time by using cached models</li> <li>Sharing model cache across workflow runs</li> </ul> Field Type Default Description <code>enabled</code> bool <code>false</code> Enable tokenizer cache volume mount <code>claimName</code> string <code>tokenizer-cache-pvc</code> PVC name for existing PVC (when <code>createPvc: false</code>) <code>mountPath</code> string <code>/cache/huggingface</code> Mount path inside container <code>readOnly</code> bool <code>true</code> Mount as read-only <code>createPvc</code> bool <code>false</code> Create PVC via <code>volumeClaimTemplates</code> (workflow-scoped) <code>storageSize</code> string <code>10Gi</code> Storage size (when <code>createPvc: true</code>) <code>storageClassName</code> string <code>null</code> Storage class name (when <code>createPvc: true</code>) <code>accessModes</code> list[str] <code>[\"ReadWriteOnce\"]</code> Access modes (when <code>createPvc: true</code>) <p>When enabled, the <code>HF_HOME</code> environment variable is automatically set to the <code>mountPath</code>, directing HuggingFace libraries to use the cached models.</p> <p>createPvc vs claimName</p> <ul> <li><code>createPvc: false</code> (default): Uses an existing PVC specified by <code>claimName</code>. You must create the PVC separately.</li> <li><code>createPvc: true</code>: Creates a workflow-scoped PVC via Argo's <code>volumeClaimTemplates</code>. The PVC is created when the workflow starts and deleted when it completes. This is useful for temporary caches but not for persistent model storage across runs.</li> </ul>"},{"location":"backends/argoworkflows/#s3-artifact-options","title":"S3 Artifact Options","text":"Field Type Default Description <code>bucket</code> string <code>wurzel-bucket</code> S3 bucket name <code>endpoint</code> string <code>s3.amazonaws.com</code> S3 endpoint URL <code>defaultMode</code> int <code>null</code> File permissions (decimal)"},{"location":"backends/argoworkflows/#runtime-environment-variables","title":"Runtime Environment Variables","text":"<p>Step settings are configured via environment variables at runtime (when the workflow executes). These can be set in three ways:</p> <ol> <li>Inline in <code>container.env</code>: Directly in the values file</li> <li>Via Kubernetes Secrets: Using <code>secretRef</code> or <code>envFrom</code> with <code>kind: secret</code></li> <li>Via Kubernetes ConfigMaps: Using <code>configMapRef</code> or <code>envFrom</code> with <code>kind: configMap</code></li> </ol> <pre><code>container:\n  # Option 1: Inline environment variables\n  env:\n    MANUALMARKDOWNSTEP__FOLDER_PATH: \"examples/pipeline/demo-data\"\n\n  # Option 2: From Secrets/ConfigMaps with optional prefix\n  envFrom:\n    - kind: secret\n      name: wurzel-secrets\n      prefix: \"\"  # No prefix\n      optional: true\n\n  # Option 3: Reference entire Secret/ConfigMap\n  secretRef:\n    - \"wurzel-secrets\"\n  configMapRef:\n    - \"wurzel-config\"\n</code></pre> <p>Inspecting Required Environment Variables</p> <p>Use <code>wurzel inspect</code> to see all environment variables required by your pipeline steps: <pre><code>wurzel inspect examples.pipeline.pipelinedemo:pipeline --gen-env\n</code></pre></p>"},{"location":"backends/argoworkflows/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the Argo backend directly in Python:</p> <pre><code>from wurzel.backend.backend_argo import ArgoBackend\nfrom wurzel.steps.embedding import EmbeddingStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.qdrant.step import QdrantConnectorStep\nfrom wurzel.utils import WZ\n\n# Define your pipeline\nsource = WZ(ManualMarkdownStep)\nembedding = WZ(EmbeddingStep)\nstep = WZ(QdrantConnectorStep)\nsource &gt;&gt; embedding &gt;&gt; step\npipeline = step\n\n# Generate Argo Workflows configuration (default config, no values file)\nbackend = ArgoBackend()\nargo_yaml = backend.generate_artifact(pipeline)\n\n# Or from values file (YAML must contain workflows.pipelinedemo):\n# from pathlib import Path\n# backend = ArgoBackend.from_values(files=[Path(\"values.yaml\")], workflow_name=\"pipelinedemo\")\n</code></pre>"},{"location":"backends/argoworkflows/#deploying-argo-workflows","title":"Deploying Argo Workflows","text":"<p>Once you've generated your Workflow or CronWorkflow YAML, deploy it to your Kubernetes cluster:</p>"},{"location":"backends/argoworkflows/#deploying-a-normal-workflow","title":"Deploying a Normal Workflow","text":"<pre><code># Apply the Workflow to your cluster\nkubectl apply -f workflow.yaml\n\n# Submit it for execution\nargo submit workflow.yaml\n\n# Or create and submit in one command\nkubectl create -f workflow.yaml\n</code></pre>"},{"location":"backends/argoworkflows/#deploying-a-cronworkflow","title":"Deploying a CronWorkflow","text":"<pre><code># Apply the CronWorkflow to your cluster (starts the cron schedule)\nkubectl apply -f cronworkflow.yaml\n\n# View CronWorkflow status\nargo cron get wurzel-pipeline\n\n# List CronWorkflows\nargo cron list\n</code></pre>"},{"location":"backends/argoworkflows/#monitoring-workflow-executions","title":"Monitoring Workflow Executions","text":"<pre><code># List all workflow executions\nargo list\n\n# Get detailed workflow status\nargo get &lt;workflow-name&gt;\n\n# View workflow logs\nargo logs &lt;workflow-name&gt;\n\n# Follow logs in real-time\nargo logs &lt;workflow-name&gt; -f\n\n# View logs for specific step\nargo logs &lt;workflow-name&gt; -c &lt;container-name&gt;\n</code></pre>"},{"location":"backends/argoworkflows/#benefits-for-cloud-native-pipelines","title":"Benefits for Cloud-Native Pipelines","text":""},{"location":"backends/argoworkflows/#kubernetes-native-execution","title":"Kubernetes-Native Execution","text":"<p>Leverage the full power of Kubernetes for container orchestration, resource management, and fault tolerance.</p>"},{"location":"backends/argoworkflows/#scalable-processing","title":"Scalable Processing","text":"<p>Automatically scale pipeline steps based on workload requirements, with support for parallel execution across multiple nodes.</p>"},{"location":"backends/argoworkflows/#enterprise-security","title":"Enterprise Security","text":"<p>Integrate with Kubernetes RBAC, service accounts, and network policies for enterprise-grade security.</p>"},{"location":"backends/argoworkflows/#cost-optimization","title":"Cost Optimization","text":"<p>Take advantage of Kubernetes features like node auto-scaling and spot instances to optimize infrastructure costs.</p>"},{"location":"backends/argoworkflows/#observability","title":"Observability","text":"<p>Built-in integration with Kubernetes monitoring tools and Argo's web UI for comprehensive pipeline observability.</p>"},{"location":"backends/argoworkflows/#multiple-values-files","title":"Multiple Values Files","text":"<p>You can use multiple values files for environment-specific overrides:</p> <pre><code># Base configuration + environment-specific overrides\nwurzel generate --backend ArgoBackend \\\n    --values base-values.yaml \\\n    --values production-values.yaml \\\n    --pipeline_name pipelinedemo \\\n    --output cronworkflow.yaml \\\n    examples.pipeline.pipelinedemo:pipeline\n</code></pre> <p>Later files override earlier ones using deep merge semantics.</p>"},{"location":"backends/argoworkflows/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster with Argo Workflows installed</li> <li>kubectl configured to access your cluster</li> <li>Appropriate RBAC permissions for workflow execution</li> <li>S3-compatible storage for artifacts (optional but recommended)</li> <li>A <code>values.yaml</code> file for generate-time configuration</li> </ul>"},{"location":"backends/argoworkflows/#learn-more","title":"Learn More","text":"<ul> <li>Argo Workflows Documentation</li> <li>Kubernetes Documentation</li> <li>Back to Backend Overview</li> </ul>"},{"location":"backends/dvc/","title":"DVC Backend","text":"<p>The DVC Backend transforms your Wurzel pipeline into Data Version Control (DVC) configuration files, enabling reproducible machine learning workflows with built-in data versioning and experiment tracking.</p>"},{"location":"backends/dvc/#overview","title":"Overview","text":"<p>DVC (Data Version Control) is a powerful tool for ML experiment management that works seamlessly with Git. The DVC Backend generates <code>dvc.yaml</code> files that define your pipeline stages, dependencies, and outputs in a format that DVC can execute and track.</p> <p>Generate-Time vs Runtime Configuration</p> <p>The DVC backend uses a two-phase configuration model:</p> <ul> <li>Generate-Time (YAML or Environment): A <code>values.yaml</code> file or environment variables configure the pipeline structure \u2014 data directories and environment encapsulation settings. This is used when running <code>wurzel generate</code>.</li> <li>Runtime (Environment Variables): Step settings (e.g., <code>MANUALMARKDOWNSTEP__FOLDER_PATH</code>) are read from environment variables when <code>dvc repro</code> executes the pipeline locally.</li> </ul> <p>This separation allows you to generate pipeline definitions once and run them in different environments by changing only the runtime environment variables.</p>"},{"location":"backends/dvc/#key-features","title":"Key Features","text":"<ul> <li>Data Versioning: Automatically track changes to datasets and model artifacts</li> <li>Reproducible Pipelines: Generate deterministic pipeline definitions</li> <li>Experiment Tracking: Compare different pipeline runs and their results</li> <li>Git Integration: Version control your pipeline configurations alongside your code</li> <li>Caching: Intelligent caching of intermediate results to speed up development</li> </ul>"},{"location":"backends/dvc/#usage","title":"Usage","text":""},{"location":"backends/dvc/#cli-usage","title":"CLI Usage","text":"<p>Generate a DVC pipeline configuration:</p> <pre><code># Install Wurzel\npip install wurzel\n\n# Generate dvc.yaml (default backend)\nwurzel generate examples.pipeline.pipelinedemo:pipeline\n\n# Explicitly specify DVC backend\nwurzel generate --backend DvcBackend --output dvc.yaml examples.pipeline.pipelinedemo:pipeline\n\n# Generate using a values file (recommended)\nwurzel generate --backend DvcBackend \\\n    --values values.yaml \\\n    --pipeline_name pipelinedemo \\\n    --output dvc.yaml \\\n    examples.pipeline.pipelinedemo:pipeline\n</code></pre>"},{"location":"backends/dvc/#values-file-configuration-generate-time","title":"Values File Configuration (Generate-Time)","text":"<p>The <code>values.yaml</code> file configures the pipeline structure at generate-time:</p> <pre><code>dvc:\n  pipelinedemo:\n    dataDir: \"./data\"        # Directory for step outputs\n    encapsulateEnv: true     # Whether to encapsulate environment in CLI calls\n</code></pre>"},{"location":"backends/dvc/#environment-configuration-generate-time-alternative","title":"Environment Configuration (Generate-Time Alternative)","text":"<p>Alternatively, configure the DVC backend using environment variables at generate-time:</p> <pre><code>export DVCBACKEND__DATA_DIR=./data\nexport DVCBACKEND__ENCAPSULATE_ENV=true\n</code></pre>"},{"location":"backends/dvc/#configuration-reference","title":"Configuration Reference","text":"Field Environment Variable Default Description <code>dataDir</code> <code>DVCBACKEND__DATA_DIR</code> <code>./data</code> Directory for step output artifacts <code>encapsulateEnv</code> <code>DVCBACKEND__ENCAPSULATE_ENV</code> <code>true</code> Whether to encapsulate environment in CLI calls"},{"location":"backends/dvc/#runtime-environment-variables","title":"Runtime Environment Variables","text":"<p>Step settings are configured via environment variables at runtime (when <code>dvc repro</code> executes). Set these before running your pipeline:</p> <pre><code># Step-specific settings (runtime)\nexport MANUALMARKDOWNSTEP__FOLDER_PATH=\"examples/pipeline/demo-data\"\nexport SIMPLESPLITTERSTEP__BATCH_SIZE=\"100\"\nexport SIMPLESPLITTERSTEP__NUM_THREADS=\"4\"\n\n# Run the pipeline\ndvc repro\n</code></pre> <p>Inspecting Required Environment Variables</p> <p>Use <code>wurzel inspect</code> to see all environment variables required by your pipeline steps: <pre><code>wurzel inspect examples.pipeline.pipelinedemo:pipeline --gen-env\n</code></pre></p>"},{"location":"backends/dvc/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the DVC backend directly in Python:</p> <pre><code>from wurzel.backend.backend_dvc import DvcBackend\nfrom wurzel.steps.embedding import EmbeddingStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.qdrant.step import QdrantConnectorStep\nfrom wurzel.utils import WZ\n\n# Define your pipeline\nsource = WZ(ManualMarkdownStep)\nembedding = WZ(EmbeddingStep)\nstep = WZ(QdrantConnectorStep)\nsource &gt;&gt; embedding &gt;&gt; step\npipeline = step\n\n# Option 1: Generate with default settings (no values file)\ndvc_yaml = DvcBackend().generate_artifact(pipeline)\n\n# Option 2: Generate from values file (YAML must contain dvc.pipelinedemo)\n# from pathlib import Path\n# values_file = Path(\"values.yaml\")\n# backend = DvcBackend.from_values(files=[values_file], workflow_name=\"pipelinedemo\")\n# dvc_yaml = backend.generate_artifact(pipeline)\n</code></pre>"},{"location":"backends/dvc/#running-dvc-pipelines","title":"Running DVC Pipelines","text":"<p>Once you've generated your <code>dvc.yaml</code> file, you can execute the pipeline using DVC:</p> <pre><code># Run the entire pipeline\ndvc repro\n\n# Run specific stages\ndvc repro &lt;stage_name&gt;\n\n# Show pipeline status\ndvc status\n\n# Compare experiments\ndvc plots show\n</code></pre>"},{"location":"backends/dvc/#benefits-for-ml-workflows","title":"Benefits for ML Workflows","text":""},{"location":"backends/dvc/#data-lineage","title":"Data Lineage","text":"<p>Track the complete history of your data transformations, making it easy to understand how your final model was created.</p>"},{"location":"backends/dvc/#experiment-reproducibility","title":"Experiment Reproducibility","text":"<p>Every pipeline run is completely reproducible, with DVC tracking all inputs, parameters, and outputs.</p>"},{"location":"backends/dvc/#collaborative-development","title":"Collaborative Development","text":"<p>Share pipeline definitions through Git while DVC handles the heavy lifting of data and model versioning.</p>"},{"location":"backends/dvc/#performance-optimization","title":"Performance Optimization","text":"<p>DVC's intelligent caching means you only recompute what's changed, dramatically speeding up iterative development.</p>"},{"location":"backends/dvc/#learn-more","title":"Learn More","text":"<ul> <li>DVC Documentation</li> <li>Back to Backend Overview</li> </ul>"},{"location":"datacontract/common/","title":"Data contracts","text":"<p>Data contracts are the primarily inputs and outputs of pipeline steps, e.g., Markdown documents.</p>"},{"location":"datacontract/common/#markdowndatacontract","title":"MarkdownDataContract","text":""},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract","title":"<code>MarkdownDataContract</code>","text":"<p>               Bases: <code>PydanticModel</code></p> <p>A data contract of the input/output of the various pipeline steps representing a document in Markdown format.</p> <p>The document consists have the Markdown body (document content) and additional metadata (keywords, url). The metadata is optional.</p> <p>Example 1 (with metadata): <pre><code>---\nkeywords: \"bread,butter\"\nurl: \"some/file/path.md\"\n---\n# Some title\n\nWith some more text.\n\n## And\n\n- Other\n- [Markdown content](#some-link)\n</code></pre></p> <p>Example 2 (without metadata): <pre><code># Another title\n\nAnother text.\n</code></pre></p> <p>Example 3 (with extra metadata fields) <pre><code>---\nkeywords: \"bread,butter\"\nurl: \"some/file/path.md\"\nmetadata:\n    token_len: 123\n    char_len: 550\n---\n# Some title\n\nA short text.\n</code></pre></p>"},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract-functions","title":"Functions","text":""},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract.from_dict_w_function","title":"<code>from_dict_w_function(doc, func)</code>  <code>classmethod</code>","text":"<p>Create a MarkdownDataContract from a dict and apply a custom func to test.</p>"},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract.from_file","title":"<code>from_file(path, url_prefix='')</code>  <code>classmethod</code>","text":"<p>Load MdContract from .md file and parse YAML metadata from header.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to a Markdown file.</p> required <code>url_prefix</code> <code>str</code> <p>Prefix to add to the URL if it is not specified in the metadata.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>MarkdownDataContract</code> <code>Self</code> <p>The file that was loaded</p> <p>Raises:</p> Type Description <code>YAMLError</code> <p>If the YAML metadata cannot be parsed.</p> <code>ValueError</code> <p>If the YAML metadata is not a dictionary.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>Welcome to the Wurzel Developer Guide! This comprehensive guide covers everything you need to know to get started with Wurzel, from installation to building your own pipeline steps.</p>"},{"location":"developer-guide/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>New to Wurzel? Follow this recommended path:</p> <ol> <li>Installation &amp; Setup - Get Wurzel running on your system</li> <li>Getting Started - Learn the basics and development workflow</li> <li>Building Pipelines - Create your first pipeline</li> <li>Creating Custom Steps - Build your own processing components</li> <li>Data Contracts - Understand type-safe data exchange</li> </ol>"},{"location":"developer-guide/#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":""},{"location":"developer-guide/#installation-environment","title":"Installation &amp; Environment","text":"<ul> <li>Installation &amp; Setup - Complete installation guide with Docker, dependencies, and troubleshooting</li> </ul>"},{"location":"developer-guide/#development-workflow","title":"Development Workflow","text":"<ul> <li>Getting Started - Development environment setup, testing, and code quality tools</li> </ul>"},{"location":"developer-guide/#core-concepts","title":"Core Concepts","text":"<ul> <li>Building Pipelines - How to define and structure pipelines</li> <li>Creating Custom Steps - Build your own data sources and processing steps</li> <li>Data Contracts - Type-safe data exchange between pipeline steps</li> </ul>"},{"location":"developer-guide/#common-tasks","title":"\ud83c\udfaf Common Tasks","text":"Task Documentation Install Wurzel Installation Guide Set up development environment Getting Started Create a pipeline Building Pipelines Build a custom step Creating Steps Add a data source Creating Steps Run tests Getting Started Generate documentation Getting Started"},{"location":"developer-guide/#external-resources","title":"\ud83d\udd17 External Resources","text":"<ul> <li>Backend Documentation - Detailed guides for DVC and Argo Workflows backends</li> <li>Wurzel Project Documentation - Auto-generated API documentation</li> </ul>"},{"location":"developer-guide/#need-help","title":"\ud83d\udca1 Need Help?","text":"<ul> <li>Check the troubleshooting section for common issues</li> <li>Review the examples for real-world usage patterns</li> <li>Consult the AI documentation for reference</li> </ul>"},{"location":"developer-guide/building-pipelines/","title":"Building Pipelines","text":"<p>Learn how to define and structure data processing pipelines in Wurzel using the intuitive chaining syntax and modular step architecture.</p>"},{"location":"developer-guide/building-pipelines/#what-is-a-wurzel-pipeline","title":"What is a Wurzel Pipeline?","text":"<p>A pipeline in Wurzel is a chain of processing steps that are connected and executed in sequence. Each step processes the output of the previous one, enabling modular, reusable, and optimally scheduled workflows.</p>"},{"location":"developer-guide/building-pipelines/#key-concepts","title":"Key Concepts","text":"<ul> <li>TypedStep: Individual processing units with defined input/output contracts</li> <li>Pipeline Chaining: Steps are connected using the <code>&gt;&gt;</code> operator</li> <li>Automatic Dependency Resolution: Wurzel determines execution order automatically</li> </ul>"},{"location":"developer-guide/building-pipelines/#basic-pipeline-structure","title":"Basic Pipeline Structure","text":""},{"location":"developer-guide/building-pipelines/#the-wz-utility","title":"The WZ Utility","text":"<p>Use <code>WZ(StepClass)</code> to create step instances and <code>&gt;&gt;</code> to chain them:</p> <pre><code>from wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\nsource = WZ(ManualMarkdownStep)\nembedding = WZ(EmbeddingStep)\nstorage = WZ(QdrantConnectorStep)\nsource &gt;&gt; embedding &gt;&gt; storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#defining-a-complete-pipeline","title":"Defining a Complete Pipeline","text":""},{"location":"developer-guide/building-pipelines/#basic-example","title":"Basic Example","text":"<p>Define a function that builds the chain and returns the last step. Wurzel runs upstream steps in order:</p> <pre><code>from wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef pipeline() -&gt; TypedStep:\n    md = WZ(ManualMarkdownStep)\n    embed = WZ(EmbeddingStep)\n    db = WZ(QdrantConnectorStep)\n    md &gt;&gt; embed &gt;&gt; db\n    return db\n</code></pre> <p>Execution order: ManualMarkdownStep \u2192 EmbeddingStep \u2192 QdrantConnectorStep.</p>"},{"location":"developer-guide/building-pipelines/#advanced-pipeline-patterns","title":"Advanced Pipeline Patterns","text":""},{"location":"developer-guide/building-pipelines/#branching","title":"Branching","text":"<p>One source can feed multiple downstream steps:</p> <pre><code>from wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.splitter import SimpleSplitterStep\nfrom wurzel.utils import WZ\n\n\ndef branching_pipeline() -&gt; TypedStep:\n    source = WZ(ManualMarkdownStep)\n    embedding = WZ(EmbeddingStep)\n    splitter = WZ(SimpleSplitterStep)\n    vector_db = WZ(QdrantConnectorStep)\n    source &gt;&gt; embedding &gt;&gt; vector_db\n    source &gt;&gt; splitter\n    return vector_db\n</code></pre>"},{"location":"developer-guide/building-pipelines/#conditional-processing","title":"Conditional Processing","text":"<p>Choose steps at build time:</p> <pre><code>from wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.embedding import TruncatedEmbeddingStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef conditional_pipeline(use_truncated: bool = False) -&gt; TypedStep:\n    source = WZ(ManualMarkdownStep)\n    processor = WZ(TruncatedEmbeddingStep) if use_truncated else WZ(EmbeddingStep)\n    storage = WZ(QdrantConnectorStep)\n    source &gt;&gt; processor &gt;&gt; storage\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"developer-guide/building-pipelines/#step-settings","title":"Step Settings","text":"<p>Each step can be configured through environment variables or settings classes. See Creating Custom Steps for details.</p>"},{"location":"developer-guide/building-pipelines/#pipeline-level-configuration","title":"Pipeline-Level Configuration","text":"<p>Use environment variables to choose steps:</p> <pre><code>import os\n\nfrom wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.embedding import TruncatedEmbeddingStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef configurable_pipeline() -&gt; TypedStep:\n    source = WZ(ManualMarkdownStep)\n    use_truncated = os.getenv(\"EMBEDDING_MODEL\", \"\").lower() == \"truncated\"\n    embedding = WZ(TruncatedEmbeddingStep) if use_truncated else WZ(EmbeddingStep)\n    storage = WZ(QdrantConnectorStep)\n    source &gt;&gt; embedding &gt;&gt; storage\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#testing-pipelines","title":"Testing Pipelines","text":"<pre><code>from wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef test_markdown_step():\n    step = WZ(ManualMarkdownStep)\n    result = step.run(None)\n    assert result is not None\n    assert isinstance(result, list)\n\n\ndef pipeline() -&gt; TypedStep:\n    md = WZ(ManualMarkdownStep)\n    embed = WZ(EmbeddingStep)\n    db = WZ(QdrantConnectorStep)\n    md &gt;&gt; embed &gt;&gt; db\n    return db\n\n\ndef test_complete_pipeline():\n    assert pipeline() is not None\n</code></pre>"},{"location":"developer-guide/building-pipelines/#pipeline-optimization","title":"Pipeline Optimization","text":""},{"location":"developer-guide/building-pipelines/#parallel-execution","title":"Parallel Execution","text":"<p>Independent branches can run in parallel (backend-dependent):</p> <pre><code>from wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef parallel_pipeline() -&gt; TypedStep:\n    source = WZ(ManualMarkdownStep)\n    embedding_a = WZ(EmbeddingStep)\n    embedding_b = WZ(EmbeddingStep)\n    storage = WZ(QdrantConnectorStep)\n    source &gt;&gt; embedding_a &gt;&gt; storage\n    source &gt;&gt; embedding_b\n    return storage\n</code></pre> <p>Steps cache outputs based on input changes; backends handle persistence.</p>"},{"location":"developer-guide/building-pipelines/#best-practices","title":"Best Practices","text":""},{"location":"developer-guide/building-pipelines/#pipeline-design","title":"Pipeline Design","text":"<ol> <li>Keep steps focused: Each step should have a single, clear responsibility</li> <li>Use meaningful names: Choose descriptive names for your pipeline functions and step variables</li> <li>Document data flow: Use comments to explain complex pipeline logic</li> <li>Handle errors gracefully: Implement proper error handling in custom steps</li> </ol>"},{"location":"developer-guide/building-pipelines/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Minimize data copying: Use efficient data structures and avoid unnecessary transformations</li> <li>Batch processing: Design steps to handle multiple items efficiently</li> <li>Resource management: Be mindful of memory usage in data-intensive steps</li> </ol>"},{"location":"developer-guide/building-pipelines/#common-patterns","title":"Common Patterns","text":""},{"location":"developer-guide/building-pipelines/#etl-style-extract-transform-load","title":"ETL-style (extract \u2192 transform \u2192 load)","text":"<pre><code>from wurzel.step import TypedStep\nfrom wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.splitter import SimpleSplitterStep\nfrom wurzel.utils import WZ\n\n\ndef etl_pipeline() -&gt; TypedStep:\n    extractor = WZ(ManualMarkdownStep)\n    transformer = WZ(SimpleSplitterStep)\n    loader = WZ(EmbeddingStep)\n    storage = WZ(QdrantConnectorStep)\n    extractor &gt;&gt; transformer &gt;&gt; loader &gt;&gt; storage\n    return storage\n</code></pre> <p>Same pattern works for document ML pipelines (load \u2192 split \u2192 embed \u2192 store).</p>"},{"location":"developer-guide/building-pipelines/#next-steps","title":"Next Steps","text":"<ul> <li>Create Custom Steps - Learn to build your own processing components</li> <li>Understand Data Contracts - Deep dive into type-safe data exchange</li> <li>Explore Backends - Deploy your pipelines to different platforms</li> </ul>"},{"location":"developer-guide/building-pipelines/#additional-resources","title":"Additional Resources","text":"<ul> <li>Step Examples - Real-world step implementations</li> <li>API Documentation - Complete API reference</li> <li>Backend Guides - Platform-specific deployment instructions</li> </ul>"},{"location":"developer-guide/cli/","title":"Cli Reference","text":""},{"location":"developer-guide/cli/#wurzel-cli-reference","title":"\ud83d\udda5\ufe0f Wurzel CLI Reference","text":"<p>The Wurzel CLI provides a powerful command-line interface for managing and executing ETL pipelines for RAG systems.</p>"},{"location":"developer-guide/cli/#quick-start","title":"Quick Start","text":"<pre><code># Install wurzel\npip install wurzel\n\n# Run a step\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep --inputs ./data --output ./out\n\n# Inspect a step\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate a pipeline\nwurzel generate examples.pipeline.pipelinedemo.pipeline\n</code></pre>"},{"location":"developer-guide/cli/#glossary","title":"Glossary","text":""},{"location":"developer-guide/cli/#PIPELINE","title":"PIPELINE","text":"<p>Module path to a chained pipeline (multiple steps combined with the <code>&gt;&gt;</code> operator). Example: <code>examples.pipeline.pipelinedemo.pipeline</code></p>"},{"location":"developer-guide/cli/#cli-commands-reference","title":"CLI Commands Reference","text":"<p>The following documentation is automatically generated from the Wurzel CLI code:</p>"},{"location":"developer-guide/cli/#wurzel","title":"wurzel","text":"<p>Global settings, main.</p>"},{"location":"developer-guide/cli/#usage","title":"Usage","text":"<p><code>wurzel [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"developer-guide/cli/#arguments","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"developer-guide/cli/#options","title":"Options","text":"Name Description Required Default <code>--log-level TEXT</code> [default: INFO] No - <code>--install-completion</code> Install completion for the current shell. No - <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation. No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#commands","title":"Commands","text":"Name Description <code>run</code> Run a step <code>inspect</code> Display information about a step <code>env</code> Inspect or validate environment variables... <code>generate</code> generate a pipeline"},{"location":"developer-guide/cli/#subcommands","title":"Subcommands","text":""},{"location":"developer-guide/cli/#wurzel-run","title":"<code>wurzel run</code>","text":"<p>Run a step</p>"},{"location":"developer-guide/cli/#usage_1","title":"Usage","text":"<p><code>wurzel run [OPTIONS] STEP</code></p>"},{"location":"developer-guide/cli/#arguments_1","title":"Arguments","text":"Name Description Required <code>STEP</code> module path to step Yes"},{"location":"developer-guide/cli/#options_1","title":"Options","text":"Name Description Required Default <code>-o, --output DIRECTORY</code> Folder with outputs  [default: -2026-02-10T12:27:34.622] No - <code>-i, --inputs DIRECTORY</code> input folders No - <code>-e, --executor TEXT</code> executor to use  [default: BaseStepExecutor] No - <code>--encapsulate-env / --no-encapsulate-env</code> [default: encapsulate-env] No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#wurzel-inspect","title":"<code>wurzel inspect</code>","text":"<p>Display information about a step</p>"},{"location":"developer-guide/cli/#usage_2","title":"Usage","text":"<p><code>wurzel inspect [OPTIONS] STEP</code></p>"},{"location":"developer-guide/cli/#arguments_2","title":"Arguments","text":"Name Description Required <code>STEP</code> module path to step Yes"},{"location":"developer-guide/cli/#options_2","title":"Options","text":"Name Description Required Default <code>--gen-env / --no-gen-env</code> [default: no-gen-env] No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#wurzel-env","title":"<code>wurzel env</code>","text":"<p>Inspect or validate environment variables for a pipeline</p>"},{"location":"developer-guide/cli/#usage_3","title":"Usage","text":"<p><code>wurzel env [OPTIONS] PIPELINE</code></p>"},{"location":"developer-guide/cli/#arguments_3","title":"Arguments","text":"Name Description Required <code>PIPELINE</code> module path to step or pipeline Yes"},{"location":"developer-guide/cli/#options_3","title":"Options","text":"Name Description Required Default <code>--include-optional / --only-required</code> display optional variables as well  [default: include-optional] No - <code>--gen-env</code> emit .env content instead of a table No - <code>--check</code> validate that required env vars are set No - <code>--allow-extra-fields</code> allow unknown settings when validating No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#wurzel-generate","title":"<code>wurzel generate</code>","text":"<p>generate a pipeline</p>"},{"location":"developer-guide/cli/#usage_4","title":"Usage","text":"<p><code>wurzel generate OPTIONS</code></p>"},{"location":"developer-guide/cli/#arguments_4","title":"Arguments","text":"Name Description Required <code>[PIPELINE]</code> module path to step or pipeline(which is a chained step) No"},{"location":"developer-guide/cli/#options_4","title":"Options","text":"Name Description Required Default <code>-b, --backend TEXT</code> backend to use  [default: DvcBackend] No - <code>-f, --values FILE</code> YAML values file(s) merged in order (Helm-style) No - <code>--pipeline-name TEXT</code> pipeline name to render from the provided values files No - <code>-o, --output FILE</code> write generated manifests to this file (stdout when omitted) No - <code>--list-backends</code> List all available backends and exit No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#usage-examples","title":"Usage Examples","text":""},{"location":"developer-guide/cli/#running-steps","title":"Running Steps","text":"<pre><code># Basic usage\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep \\\n    --inputs ./markdown-files \\\n    --output ./processed-output\n\n# With custom executor\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep \\\n    --inputs ./markdown-files \\\n    --output ./processed-output \\\n    --executor PrometheusStepExecutor\n\n# Multiple input folders\nwurzel run wurzel.steps.splitter.SimpleSplitterStep \\\n    --inputs ./docs \\\n    --inputs ./markdown \\\n    --inputs ./pdfs \\\n    --output ./split-output\n</code></pre>"},{"location":"developer-guide/cli/#inspecting-steps","title":"Inspecting Steps","text":"<pre><code># Basic inspection\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate environment file\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep --gen-env\n</code></pre>"},{"location":"developer-guide/cli/#managing-environment-variables","title":"Managing Environment Variables","text":"<p>Use the <code>wurzel env</code> helper to inspect or validate the variables your pipeline needs:</p> <pre><code># Show required env vars (toggle optional ones via --only-required)\nwurzel env examples.pipeline.pipelinedemo:pipeline --only-required\n\n# Generate a .env snippet (prefers values already present in your shell)\nwurzel env examples.pipeline.pipelinedemo:pipeline --gen-env &gt; .env.sample\n\n# Fail fast when something is missing\nwurzel env examples.pipeline.pipelinedemo:pipeline --check\n# Allow dynamically added settings (equivalent to setting ALLOW_EXTRA_SETTINGS)\nwurzel env examples.pipeline.pipelinedemo:pipeline --check --allow-extra-fields\n</code></pre>"},{"location":"developer-guide/cli/#generating-pipelines","title":"Generating Pipelines","text":"<p>The <code>wurzel generate</code> command creates backend-specific pipeline configurations from chained pipelines.</p> <p>Arguments: - <code>pipeline</code> - Module path to a chained pipeline (multiple steps combined with <code>&gt;&gt;</code> operator)</p> <p>Options: - <code>-b, --backend</code> - Backend to use (default: DvcBackend). Case-insensitive. - <code>--list-backends</code> - List all available backends and exit</p> <pre><code># List all available backends\nwurzel generate --list-backends\n\n# Generate from a chained pipeline\nwurzel generate examples.pipeline.pipelinedemo.pipeline\n\n# Generate with explicit backend (case-insensitive)\nwurzel generate myproject.pipelines.MyPipeline --backend DvcBackend\nwurzel generate myproject.pipelines.MyPipeline -b dvcbackend\n\n# Generate Argo Workflows pipeline (requires wurzel[argo])\nwurzel generate myproject.pipelines.MyPipeline --backend ArgoBackend\nwurzel generate myproject.pipelines.MyPipeline -b argobackend\n</code></pre> <p>Creating a Chained Pipeline:</p> <p>A chained pipeline is created by combining multiple steps with the <code>&gt;&gt;</code> operator:</p> <pre><code># In myproject/pipelines.py\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.splitter import SimpleSplitterStep\nfrom wurzel.utils import WZ\n\n# Wrap steps with WZ\nsource = WZ(ManualMarkdownStep)\nsplitter = WZ(SimpleSplitterStep)\n\n# Chain steps together\nsource &gt;&gt; splitter\n\n# Export the final step as the pipeline\npipeline = splitter\n</code></pre> <p>Then generate the pipeline configuration: <pre><code>wurzel generate myproject.pipelines.pipeline -b DvcBackend\n</code></pre></p>"},{"location":"developer-guide/cli/#step-auto-discovery","title":"Step Auto-Discovery","text":"<p>The CLI supports intelligent auto-completion for step names using TAB completion:</p> <pre><code>wurzel run &lt;TAB&gt;                    # Shows all available steps\nwurzel run wurzel.steps.&lt;TAB&gt;       # Shows wurzel built-in steps\nwurzel run mysteps.&lt;TAB&gt;            # Shows your custom steps\n</code></pre> <p>The auto-completion discovers:</p> <ol> <li>Built-in Wurzel steps - Available in the <code>wurzel.steps.*</code> namespace</li> <li>User-defined steps - TypedStep classes in your current project</li> </ol>"},{"location":"developer-guide/creating-steps/","title":"Creating Custom Steps","text":"<p>Learn how to build your own data processing steps in Wurzel, from simple data sources to complex transformation components.</p>"},{"location":"developer-guide/creating-steps/#step-types-overview","title":"Step Types Overview","text":"<p>Wurzel provides two main types of steps:</p> <ul> <li>Data Source Steps (WurzelTips): Entry points that ingest data from external sources</li> <li>Processing Steps (WurzelSteps): Transform data from upstream steps</li> </ul> <p>Both types follow the same <code>TypedStep</code> interface but serve different roles in your pipeline.</p>"},{"location":"developer-guide/creating-steps/#step-architecture","title":"Step Architecture","text":""},{"location":"developer-guide/creating-steps/#the-typedstep-interface","title":"The TypedStep Interface","text":"<p>All steps inherit from <code>TypedStep</code> with three type parameters: TSettings, TInput, TOutput. You implement:</p> <ul> <li><code>__init__</code> \u2014 setup (connections, resources)</li> <li><code>run(inpt)</code> \u2014 process input and return output</li> <li><code>finalize</code> \u2014 optional cleanup</li> </ul> <pre><code>from typing import Generic, TypeVar\n\nTSettings = TypeVar(\"TSettings\")\nTInput = TypeVar(\"TInput\")\nTOutput = TypeVar(\"TOutput\")\n\n\nclass TypedStep(Generic[TSettings, TInput, TOutput]):\n    def __init__(self) -&gt; None:\n        pass\n\n    def run(self, inpt: TInput) -&gt; TOutput:\n        raise NotImplementedError\n\n    def finalize(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"developer-guide/creating-steps/#step-lifecycle","title":"Step Lifecycle","text":"<ol> <li>Initialization (<code>__init__</code>): Setup connections, create resources</li> <li>Execution (<code>run</code>): Process data (may be called multiple times)</li> <li>Finalization (<code>finalize</code>): Cleanup resources, close connections</li> </ol> <p>\u26a0\ufe0f Important: The <code>run</code> method may be executed multiple times for different upstream dependencies. Put setup logic in <code>__init__</code>, not <code>run</code>.</p>"},{"location":"developer-guide/creating-steps/#creating-data-source-steps","title":"Creating Data Source Steps","text":"<p>Data source steps introduce data into your pipeline. They have <code>None</code> as their input type since they don't depend on previous steps.</p>"},{"location":"developer-guide/creating-steps/#basic-data-source","title":"Basic Data Source","text":"<pre><code>from pathlib import Path\n\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass MySettings(Settings):\n    DATA_PATH: Path = Path(\"./data\")\n    FILE_PATTERN: str = \"*.md\"\n\n\nclass MyDatasourceStep(TypedStep[MySettings, None, list[MarkdownDataContract]]):\n    def __init__(self):\n        super().__init__()\n        self.settings = MySettings()\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        documents = []\n        for file_path in self.settings.DATA_PATH.glob(self.settings.FILE_PATTERN):\n            content = file_path.read_text(encoding=\"utf-8\")\n            doc = MarkdownDataContract(\n                md=content,\n                url=str(file_path),\n                keywords=file_path.stem,\n                metadata={\"file_path\": str(file_path)},\n            )\n            documents.append(doc)\n        return documents\n</code></pre>"},{"location":"developer-guide/creating-steps/#advanced-data-source-with-database","title":"Advanced Data Source with Database","text":"<p>Shows a source step with resource cleanup in <code>finalize</code>:</p> <pre><code>import sqlite3\n\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass DatabaseSettings(Settings):\n    DB_PATH: str = \"data.db\"\n    TABLE_NAME: str = \"documents\"\n    QUERY: str = \"SELECT content, source, metadata FROM documents\"\n\n\nclass DatabaseSourceStep(TypedStep[DatabaseSettings, None, list[MarkdownDataContract]]):\n    def __init__(self):\n        super().__init__()\n        self.settings = DatabaseSettings()\n        self.connection = sqlite3.connect(self.settings.DB_PATH)\n        self.connection.row_factory = sqlite3.Row\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        cursor = self.connection.cursor()\n        cursor.execute(self.settings.QUERY)\n        documents = []\n        for row in cursor.fetchall():\n            meta = eval(row[\"metadata\"]) if row[\"metadata\"] else {}\n            doc = MarkdownDataContract(\n                md=row[\"content\"],\n                url=row[\"source\"],\n                keywords=\"\",\n                metadata=meta,\n            )\n            documents.append(doc)\n        return documents\n\n    def finalize(self) -&gt; None:\n        if self.connection:\n            self.connection.close()\n</code></pre>"},{"location":"developer-guide/creating-steps/#creating-processing-steps","title":"Creating Processing Steps","text":"<p>Processing steps transform data from upstream steps. They can filter, validate, transform, or enrich data.</p>"},{"location":"developer-guide/creating-steps/#filter-step","title":"Filter Step","text":"<pre><code>from wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass FilterSettings(Settings):\n    MIN_LENGTH: int = 100\n    MAX_LENGTH: int = 10000\n    REQUIRED_KEYWORDS: list[str] = []\n\n\nclass DocumentFilterStep(\n    TypedStep[FilterSettings, list[MarkdownDataContract], list[MarkdownDataContract]]\n):\n    def __init__(self):\n        super().__init__()\n        self.settings = FilterSettings()\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        filtered_docs = []\n        for doc in inpt:\n            if (\n                len(doc.md) &lt; self.settings.MIN_LENGTH\n                or len(doc.md) &gt; self.settings.MAX_LENGTH\n            ):\n                continue\n            if self.settings.REQUIRED_KEYWORDS and not all(\n                kw.lower() in doc.md.lower() for kw in self.settings.REQUIRED_KEYWORDS\n            ):\n                continue\n            filtered_docs.append(doc)\n        return filtered_docs\n</code></pre>"},{"location":"developer-guide/creating-steps/#transformation-step","title":"Transformation Step","text":"<pre><code>from wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass TransformSettings(Settings):\n    PREFIX: str = \"[processed] \"\n\n\nclass DocumentTransformStep(\n    TypedStep[TransformSettings, list[MarkdownDataContract], list[MarkdownDataContract]]\n):\n    def __init__(self):\n        super().__init__()\n        self.settings = TransformSettings()\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        return [\n            MarkdownDataContract(\n                md=self.settings.PREFIX + doc.md,\n                url=doc.url,\n                keywords=doc.keywords,\n                metadata=doc.metadata,\n            )\n            for doc in inpt\n        ]\n</code></pre>"},{"location":"developer-guide/creating-steps/#validation-step","title":"Validation Step","text":"<p>Uses a helper and raises after too many errors:</p> <pre><code>from wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass ValidationSettings(Settings):\n    CHECK_ENCODING: bool = True\n    CHECK_STRUCTURE: bool = True\n    MAX_ERRORS: int = 5\n\n\nclass DocumentValidationStep(\n    TypedStep[\n        ValidationSettings, list[MarkdownDataContract], list[MarkdownDataContract]\n    ]\n):\n    def __init__(self):\n        super().__init__()\n        self.settings = ValidationSettings()\n        self.error_count = 0\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        valid_docs = []\n        for doc in inpt:\n            if self._validate_document(doc):\n                valid_docs.append(doc)\n            else:\n                self.error_count += 1\n                if self.error_count &gt;= self.settings.MAX_ERRORS:\n                    raise RuntimeError(\n                        f\"Too many validation errors: {self.error_count}\"\n                    )\n        return valid_docs\n\n    def _validate_document(self, doc: MarkdownDataContract) -&gt; bool:\n        if self.settings.CHECK_ENCODING:\n            try:\n                doc.md.encode(\"utf-8\")\n            except UnicodeEncodeError:\n                return False\n        if self.settings.CHECK_STRUCTURE and (not doc.md.strip() or len(doc.md) &lt; 10):\n            return False\n        return True\n</code></pre>"},{"location":"developer-guide/creating-steps/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"developer-guide/creating-steps/#multi-input-steps","title":"Multi-Input Steps","text":"<p>Steps can collect data from multiple upstream runs (simplified: single input list here):</p> <pre><code>from wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass MergerSettings(Settings):\n    MERGE_STRATEGY: str = \"concatenate\"\n\n\nclass DocumentMergerStep(\n    TypedStep[MergerSettings, list[MarkdownDataContract], list[MarkdownDataContract]]\n):\n    def __init__(self):\n        super().__init__()\n        self.settings = MergerSettings()\n        self.collected_inputs: list[list[MarkdownDataContract]] = []\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        self.collected_inputs.append(inpt)\n        if self.settings.MERGE_STRATEGY == \"concatenate\":\n            all_docs: list[MarkdownDataContract] = []\n            for doc_list in self.collected_inputs:\n                all_docs.extend(doc_list)\n            return all_docs\n        return inpt\n</code></pre>"},{"location":"developer-guide/creating-steps/#stateful-processing","title":"Stateful Processing","text":"<p>State in <code>__init__</code> and optional cleanup in <code>finalize</code>:</p> <pre><code>import hashlib\nfrom collections import defaultdict\n\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import Settings, TypedStep\n\n\nclass DeduplicationSettings(Settings):\n    HASH_ALGORITHM: str = \"md5\"\n\n\nclass DeduplicationStep(\n    TypedStep[\n        DeduplicationSettings, list[MarkdownDataContract], list[MarkdownDataContract]\n    ]\n):\n    def __init__(self):\n        super().__init__()\n        self.settings = DeduplicationSettings()\n        self.seen_hashes: set[str] = set()\n        self.document_index: defaultdict[str, list[str]] = defaultdict(list)\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        unique_docs = []\n        for doc in inpt:\n            raw = doc.md.encode()\n            content_hash = (\n                hashlib.md5(raw).hexdigest()\n                if self.settings.HASH_ALGORITHM == \"md5\"\n                else hashlib.sha256(raw).hexdigest()\n            )\n            if content_hash not in self.seen_hashes:\n                self.seen_hashes.add(content_hash)\n                unique_docs.append(doc)\n                self.document_index[content_hash].append(doc.url)\n        return unique_docs\n\n    def finalize(self) -&gt; None:\n        _ = len(self.seen_hashes)\n        _ = sum(len(sources) - 1 for sources in self.document_index.values())\n</code></pre>"},{"location":"developer-guide/creating-steps/#step-settings-and-configuration","title":"Step Settings and Configuration","text":""},{"location":"developer-guide/creating-steps/#environment-variable-integration","title":"Environment Variable Integration","text":"<p>Settings fields map to env vars (prefix = step name in UPPERCASE, e.g. <code>MYSTEP__API_KEY</code>):</p> <pre><code>from wurzel.step import Settings\n\n\nclass APISettings(Settings):\n    API_KEY: str\n    BASE_URL: str = \"https://api.example.com\"\n    TIMEOUT: int = 30\n    MAX_RETRIES: int = 3\n</code></pre>"},{"location":"developer-guide/creating-steps/#nested-configuration","title":"Nested Configuration","text":"<p>Nested settings use <code>__</code> in env var names:</p> <pre><code>from wurzel.step import Settings\n\n\nclass DatabaseConfig(Settings):\n    HOST: str = \"localhost\"\n    PORT: int = 5432\n    DATABASE: str = \"wurzel\"\n    USERNAME: str = \"user\"\n    PASSWORD: str = \"password\"\n\n\nclass ProcessingConfig(Settings):\n    BATCH_SIZE: int = 100\n    PARALLEL_WORKERS: int = 4\n\n\nclass ComplexStepSettings(Settings):\n    database: DatabaseConfig = DatabaseConfig()\n    processing: ProcessingConfig = ProcessingConfig()\n    DEBUG_MODE: bool = False\n</code></pre>"},{"location":"developer-guide/creating-steps/#testing-custom-steps","title":"Testing Custom Steps","text":""},{"location":"developer-guide/creating-steps/#unit-testing","title":"Unit Testing","text":"<pre><code>from pathlib import Path\nfrom unittest.mock import patch\n\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import TypedStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef test_markdown_step_returns_list(tmp_path: Path):\n    (tmp_path / \"doc.md\").write_text(\"# Hello\")\n    with patch.dict(\"os.environ\", {\"MANUALMARKDOWNSTEP__FOLDER_PATH\": str(tmp_path)}):\n        step = WZ(ManualMarkdownStep)\n        result = step.run(None)\n    assert isinstance(result, list)\n    assert len(result) == 1\n    assert all(isinstance(d, MarkdownDataContract) for d in result)\n\n\ndef test_filter_step():\n    from wurzel.step import Settings\n\n    class FilterSettings(Settings):\n        MIN_LENGTH: int = 10\n\n    class SimpleFilterStep(\n        TypedStep[\n            FilterSettings, list[MarkdownDataContract], list[MarkdownDataContract]\n        ]\n    ):\n        def __init__(self):\n            super().__init__()\n            self.settings = FilterSettings()\n\n        def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n            return [d for d in inpt if len(d.md) &gt;= self.settings.MIN_LENGTH]\n\n    docs = [\n        MarkdownDataContract(md=\"Hi\", url=\"u1\", keywords=\"k1\"),\n        MarkdownDataContract(md=\"Long enough content here\", url=\"u2\", keywords=\"k2\"),\n    ]\n    step = WZ(SimpleFilterStep)\n    result = step.run(docs)\n    assert len(result) == 1\n    assert result[0].md == \"Long enough content here\"\n</code></pre>"},{"location":"developer-guide/creating-steps/#integration-testing","title":"Integration Testing","text":"<pre><code>from wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n\ndef test_pipeline_structure():\n    source = WZ(ManualMarkdownStep)\n    assert source is not None\n    assert hasattr(source, \"inputs\")\n</code></pre>"},{"location":"developer-guide/creating-steps/#best-practices","title":"Best Practices","text":""},{"location":"developer-guide/creating-steps/#error-handling","title":"Error Handling","text":"<p>Catch per-item errors and fail if too many:</p> <pre><code>from wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import NoSettings, TypedStep\n\n\nclass RobustProcessingStep(\n    TypedStep[NoSettings, list[MarkdownDataContract], list[MarkdownDataContract]]\n):\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        processed_docs = []\n        errors = []\n        for i, doc in enumerate(inpt):\n            try:\n                processed_docs.append(doc)\n            except Exception as e:\n                errors.append(f\"Failed to process document {i}: {e!s}\")\n        if errors and len(errors) &gt; len(inpt) * 0.5:\n            raise RuntimeError(f\"Too many processing errors: {len(errors)}/{len(inpt)}\")\n        return processed_docs\n</code></pre>"},{"location":"developer-guide/creating-steps/#resource-management","title":"Resource Management","text":"<p>Use <code>finalize()</code> to release connections and temp files:</p> <pre><code>import os\nfrom typing import Any\n\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.step import NoSettings, TypedStep\n\n\nclass ResourceManagedStep(\n    TypedStep[NoSettings, list[MarkdownDataContract], list[MarkdownDataContract]]\n):\n    def __init__(self):\n        super().__init__()\n        self.connection: Any = None\n        self.temp_files: list[str] = []\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        try:\n            return inpt\n        except Exception:\n            self._cleanup_resources()\n            raise\n\n    def finalize(self) -&gt; None:\n        self._cleanup_resources()\n\n    def _cleanup_resources(self) -&gt; None:\n        if self.connection is not None:\n            self.connection.close()\n            self.connection = None\n        for temp_file in self.temp_files:\n            try:\n                os.unlink(temp_file)\n            except OSError:\n                pass\n        self.temp_files.clear()\n</code></pre>"},{"location":"developer-guide/creating-steps/#next-steps","title":"Next Steps","text":"<ul> <li>Understand Data Contracts - Learn about type-safe data exchange</li> <li>Explore Backend Integration - Deploy your custom steps</li> </ul>"},{"location":"developer-guide/creating-steps/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation - Complete TypedStep API reference</li> <li>Built-in Steps - Examples of existing step implementations</li> <li>Testing Guidelines - Best practices for testing steps</li> </ul>"},{"location":"developer-guide/data-contracts/","title":"Data Contracts","text":"<p>Understand Wurzel's type-safe data exchange system that ensures data integrity and enables seamless communication between pipeline steps.</p>"},{"location":"developer-guide/data-contracts/#overview","title":"Overview","text":"<p>Wurzel implements a type-safe pipeline system where data flows between processing steps through strictly defined Data Contracts. These contracts ensure data integrity, enable automatic validation, and provide clear interfaces between pipeline components.</p>"},{"location":"developer-guide/data-contracts/#key-benefits","title":"Key Benefits","text":"<ul> <li>Type Safety: Compile-time and runtime validation</li> <li>Modularity: Interchangeable steps with clear interfaces</li> <li>Persistence: Automatic serialization between steps</li> <li>Scalability: Efficient DataFrame-based bulk processing</li> </ul>"},{"location":"developer-guide/data-contracts/#data-contract-fundamentals","title":"Data Contract Fundamentals","text":""},{"location":"developer-guide/data-contracts/#the-datamodel-interface","title":"The DataModel Interface","text":"<p>Contracts implement <code>DataModel</code>: save_to_path and load_from_path for persistence between steps.</p> <pre><code>from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Self, TypeVar\n\nT = TypeVar(\"T\", bound=\"DataModel\")\n\n\nclass DataModel(ABC):\n    @classmethod\n    @abstractmethod\n    def save_to_path(cls, path: Path, obj: Self | list[Self]) -&gt; Path: ...\n\n    @classmethod\n    @abstractmethod\n    def load_from_path(cls, path: Path, *args: Any) -&gt; Self: ...\n</code></pre> <p>This ensures all data types can be:</p> <ul> <li>Persisted to disk between pipeline steps</li> <li>Loaded automatically by the next step</li> <li>Validated for type correctness</li> </ul>"},{"location":"developer-guide/data-contracts/#supported-base-data-types","title":"Supported Base Data Types","text":"<p>Wurzel provides two concrete implementations of the <code>DataModel</code> interface:</p>"},{"location":"developer-guide/data-contracts/#pydanticmodel","title":"PydanticModel","text":"<p>For structured objects; inherits JSON save/load from the base. Custom override only if you need different behavior:</p> <pre><code>import json\nfrom pathlib import Path\n\nfrom wurzel.datacontract import PydanticModel\n\n\nclass DocumentMetadata(PydanticModel):\n    title: str\n    author: str\n    created_date: str\n    tags: list[str] = []\n\n    @classmethod\n    def save_to_path(\n        cls, path: Path, obj: \"DocumentMetadata | list[DocumentMetadata]\"\n    ) -&gt; Path:\n        path = path.with_suffix(\".json\")\n        single = [obj] if isinstance(obj, cls) else obj\n        path.write_text(json.dumps([m.model_dump() for m in single], indent=2))\n        return path\n\n    @classmethod\n    def load_from_path(\n        cls, path: Path, model_type: type[\"DocumentMetadata\"]\n    ) -&gt; \"DocumentMetadata\":\n        data = json.loads(path.read_text())\n        if isinstance(data, list):\n            data = data[0]\n        return cls(**data)\n</code></pre> <p>Features:</p> <ul> <li>Serialization: JSON format (<code>.json</code> files)</li> <li>Use cases: Individual documents, metadata, configuration objects</li> <li>Validation: Automatic Pydantic validation</li> </ul>"},{"location":"developer-guide/data-contracts/#dataframe-pandera","title":"DataFrame (Pandera)","text":"<p>For bulk data, use Pandera schema models like <code>EmbeddingResult</code>:</p> <pre><code>import pandas as pd\n\nfrom wurzel.steps.data import EmbeddingResult\n\ndf = pd.DataFrame(\n    {\n        \"text\": [\"doc1\"],\n        \"url\": [\"file:///doc1.md\"],\n        \"vector\": [[0.1, 0.2]],\n        \"keywords\": [\"\"],\n        \"embedding_input_text\": [\"doc1\"],\n        \"metadata\": [{}],\n    }\n)\ntyped_df = EmbeddingResult(df)\n</code></pre> <p>Features:</p> <ul> <li>Serialization: Parquet format (<code>.parquet</code> files)</li> <li>Use cases: Large datasets, tabular data, bulk processing</li> <li>Performance: Optimized for high-volume data operations</li> </ul>"},{"location":"developer-guide/data-contracts/#built-in-data-contracts","title":"Built-in Data Contracts","text":""},{"location":"developer-guide/data-contracts/#markdowndatacontract","title":"MarkdownDataContract","text":"<p>Primary contract for document pipelines. Fields: md, url, keywords, metadata.</p> <pre><code>from wurzel.datacontract import MarkdownDataContract\n\ndoc = MarkdownDataContract(\n    md=\"# My Document\\n\\nThis is the content.\",\n    url=\"document.md\",\n    keywords=\"documentation, markdown\",\n    metadata={\"author\": \"John Doe\", \"created\": \"2024-01-01\", \"tags\": [\"markdown\"]},\n)\n_ = doc.md\n_ = doc.url\n_ = doc.metadata\n</code></pre> <p>Fields:</p> <ul> <li><code>md</code>: The actual markdown text</li> <li><code>url</code>: Source identifier (file path, URL, etc.)</li> <li><code>keywords</code>: Optional keywords</li> <li><code>metadata</code>: Flexible dictionary for additional information</li> </ul> <p>Usage: Document ingestion, text processing, content management</p>"},{"location":"developer-guide/data-contracts/#embeddingresult","title":"EmbeddingResult","text":"<p>DataFrame schema for embedding output (one row per chunk). Columns: text, url, vector, keywords, embedding_input_text, metadata.</p> <pre><code>import pandas as pd\n\nfrom wurzel.steps.data import EmbeddingResult\n\nrow = {\n    \"text\": \"Original text content\",\n    \"url\": \"source_document.md\",\n    \"vector\": [0.1, 0.2, 0.3],\n    \"keywords\": \"\",\n    \"embedding_input_text\": \"Original text content\",\n    \"metadata\": {\"processed_at\": \"2024-01-01T10:00:00Z\"},\n}\ndf = pd.DataFrame([row])\nembedding_df = EmbeddingResult(df)\n</code></pre> <p>Fields (per row):</p> <ul> <li><code>text</code>: Text that was embedded</li> <li><code>url</code>: Source of the content</li> <li><code>vector</code>: Vector representation</li> <li><code>keywords</code>, <code>embedding_input_text</code>, <code>metadata</code>: Optional</li> </ul> <p>Usage: Vector databases, similarity search, ML pipelines</p>"},{"location":"developer-guide/data-contracts/#creating-custom-data-contracts","title":"Creating Custom Data Contracts","text":""},{"location":"developer-guide/data-contracts/#simple-custom-contract","title":"Simple Custom Contract","text":"<p>Subclass <code>PydanticModel</code>, add fields and validators:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import field_validator\n\nfrom wurzel.datacontract import PydanticModel\nfrom wurzel.step import NoSettings, TypedStep\n\n\nclass ProductDataContract(PydanticModel):\n    product_id: str\n    name: str\n    price: float\n    description: Optional[str] = None\n    category: str = \"general\"\n    in_stock: bool = True\n    tags: list[str] = []\n    attributes: dict[str, str] = {}\n    created_at: datetime = datetime.now()\n\n    @field_validator(\"price\")\n    @classmethod\n    def validate_price(cls, v: float) -&gt; float:\n        if v &lt; 0:\n            raise ValueError(\"Price must be positive\")\n        return v\n\n    @field_validator(\"product_id\")\n    @classmethod\n    def validate_id_format(cls, v: str) -&gt; str:\n        if not v.startswith(\"PROD_\"):\n            raise ValueError(\"Product ID must start with PROD_\")\n        return v\n\n\nclass ProductProcessingStep(\n    TypedStep[NoSettings, list[ProductDataContract], list[ProductDataContract]]\n):\n    def run(self, inpt: list[ProductDataContract]) -&gt; list[ProductDataContract]:\n        processed = []\n        for product in inpt:\n            if product.price &gt; 100:\n                product.tags.append(\"premium\")\n            processed.append(product)\n        return processed\n</code></pre>"},{"location":"developer-guide/data-contracts/#complex-hierarchical-contract","title":"Complex Hierarchical Contract","text":"<p>Nested Pydantic models and enums:</p> <pre><code>from datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\n\nfrom pydantic import field_validator\n\nfrom wurzel.datacontract import PydanticModel\n\n\nclass DocumentType(str, Enum):\n    ARTICLE = \"article\"\n    MANUAL = \"manual\"\n    FAQ = \"faq\"\n    TUTORIAL = \"tutorial\"\n\n\nclass AuthorInfo(PydanticModel):\n    name: str\n    email: str\n    organization: Optional[str] = None\n\n\nclass DocumentSection(PydanticModel):\n    title: str\n    content: str\n    level: int = 1\n    subsections: list[\"DocumentSection\"] = []\n\n\nclass RichDocumentContract(PydanticModel):\n    title: str\n    document_type: DocumentType\n    language: str = \"en\"\n    sections: list[DocumentSection]\n    author: AuthorInfo\n    created_at: datetime\n    updated_at: Optional[datetime] = None\n    word_count: Optional[int] = None\n    reading_time_minutes: Optional[float] = None\n\n    @field_validator(\"sections\")\n    @classmethod\n    def validate_sections(cls, v: list[DocumentSection]) -&gt; list[DocumentSection]:\n        if not v:\n            raise ValueError(\"Document must have at least one section\")\n        return v\n</code></pre>"},{"location":"developer-guide/data-contracts/#dataframe-based-contracts","title":"DataFrame-based Contracts","text":"<p>Use Pandera schema types in step signatures:</p> <pre><code>from pandera.typing import DataFrame\n\nfrom wurzel.step import NoSettings, TypedStep\nfrom wurzel.steps.data import EmbeddingResult\n\n\nclass EmbeddingPassthroughStep(\n    TypedStep[NoSettings, DataFrame[EmbeddingResult], DataFrame[EmbeddingResult]]\n):\n    def run(self, inpt: DataFrame[EmbeddingResult]) -&gt; DataFrame[EmbeddingResult]:\n        return inpt\n</code></pre>"},{"location":"developer-guide/data-contracts/#data-contract-best-practices","title":"Data Contract Best Practices","text":""},{"location":"developer-guide/data-contracts/#design-guidelines","title":"Design Guidelines","text":"<ol> <li>Keep contracts focused \u2014 one clear data structure per contract</li> <li>Use descriptive names and sensible defaults for optional fields</li> <li>Add validation with Pydantic validators and <code>Field(..., description=...)</code></li> </ol> <pre><code>import re\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import Field, field_validator\n\nfrom wurzel.datacontract import PydanticModel\n\n\nclass WellDesignedContract(PydanticModel):\n    document_id: str = Field(..., description=\"Unique document identifier\")\n    content: str = Field(..., description=\"Processed document content\")\n    language: str = Field(\"en\", description=\"Document language code\")\n    processing_version: str = Field(\"1.0\", description=\"Processing pipeline version\")\n    quality_score: float = Field(0.0, ge=0.0, le=1.0, description=\"Quality score (0-1)\")\n    confidence: float = Field(0.0, ge=0.0, le=1.0, description=\"Confidence (0-1)\")\n    created_at: datetime = Field(default_factory=datetime.now)\n    processed_at: Optional[datetime] = None\n\n    @field_validator(\"document_id\")\n    @classmethod\n    def validate_document_id(cls, v: str) -&gt; str:\n        if not re.match(r\"^DOC_\\d{8}_\\d{6}$\", v):\n            raise ValueError(\"Document ID must match format: DOC_YYYYMMDD_HHMMSS\")\n        return v\n</code></pre>"},{"location":"developer-guide/data-contracts/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Choose the right base type:</li> <li>Use <code>PydanticModel</code> for individual objects</li> <li> <p>Use <code>DataFrame</code> for bulk data processing</p> </li> <li> <p>Minimize serialization overhead:</p> </li> <li>Keep contracts as simple as possible</li> <li> <p>Avoid deeply nested structures when not necessary</p> </li> <li> <p>Optimize for your use case:</p> </li> <li>For analytics: Use DataFrame with efficient column types</li> <li>For individual processing: Use PydanticModel with focused fields</li> </ol>"},{"location":"developer-guide/data-contracts/#migration-and-versioning","title":"Migration and Versioning","text":"<p>Support multiple contract versions with a migration function and a step that accepts both:</p> <pre><code>from typing import Union\n\nfrom wurzel.datacontract import PydanticModel\nfrom wurzel.step import NoSettings, TypedStep\n\n\nclass AuthorInfo(PydanticModel):\n    name: str\n    email: str = \"\"\n    organization: str | None = None\n\n\nclass DocumentContractV1(PydanticModel):\n    title: str\n    content: str\n    author: str\n\n\nclass DocumentContractV2(PydanticModel):\n    title: str\n    content: str\n    author: AuthorInfo\n    version: int = 2\n\n\ndef migrate_document_v1_to_v2(old_doc: DocumentContractV1) -&gt; DocumentContractV2:\n    return DocumentContractV2(\n        title=old_doc.title,\n        content=old_doc.content,\n        author=AuthorInfo(\n            name=old_doc.author, email=\"unknown@example.com\", organization=None\n        ),\n    )\n\n\nclass VersionAwareStep(\n    TypedStep[\n        NoSettings, Union[DocumentContractV1, DocumentContractV2], DocumentContractV2\n    ]\n):\n    def run(\n        self, inpt: Union[DocumentContractV1, DocumentContractV2]\n    ) -&gt; DocumentContractV2:\n        if isinstance(inpt, DocumentContractV1):\n            return migrate_document_v1_to_v2(inpt)\n        return inpt\n</code></pre>"},{"location":"developer-guide/data-contracts/#testing-data-contracts","title":"Testing Data Contracts","text":""},{"location":"developer-guide/data-contracts/#unit-testing-contracts","title":"Unit Testing Contracts","text":"<pre><code>import pytest\n\nfrom wurzel.datacontract import MarkdownDataContract\n\n\ndef test_markdown_contract_creation():\n    doc = MarkdownDataContract(\n        md=\"# Hello\",\n        url=\"doc.md\",\n        keywords=\"test\",\n        metadata={\"key\": \"value\"},\n    )\n    assert doc.md == \"# Hello\"\n    assert doc.url == \"doc.md\"\n    assert doc.metadata == {\"key\": \"value\"}\n\n\ndef test_markdown_contract_validation():\n    with pytest.raises(Exception):\n        MarkdownDataContract(md=123, url=\"u\", keywords=\"k\")  # type: ignore[arg-type]\n</code></pre>"},{"location":"developer-guide/data-contracts/#integration-testing-with-steps","title":"Integration Testing with Steps","text":"<pre><code>from wurzel.datacontract import PydanticModel\nfrom wurzel.step import NoSettings, TypedStep\n\n\nclass ProductDataContract(PydanticModel):\n    product_id: str\n    name: str\n    price: float\n    tags: list[str] = []\n\n\nclass ProductProcessingStep(\n    TypedStep[NoSettings, list[ProductDataContract], list[ProductDataContract]]\n):\n    def run(self, inpt: list[ProductDataContract]) -&gt; list[ProductDataContract]:\n        out = []\n        for p in inpt:\n            if p.price &gt; 100:\n                p.tags.append(\"premium\")\n            out.append(p)\n        return out\n\n\ndef test_step_with_custom_contract():\n    products = [\n        ProductDataContract(product_id=\"PROD_001\", name=\"Cheap Item\", price=5.99),\n        ProductDataContract(product_id=\"PROD_002\", name=\"Expensive Item\", price=150.00),\n    ]\n    step = ProductProcessingStep()\n    result = step.run(products)\n    assert len(result) == 2\n    assert \"premium\" not in result[0].tags\n    assert \"premium\" in result[1].tags\n</code></pre>"},{"location":"developer-guide/data-contracts/#next-steps","title":"Next Steps","text":"<ul> <li>Create Custom Steps - Build steps that use your custom contracts</li> <li>Explore Backend Integration - Deploy pipelines with custom contracts</li> <li>Review Examples - See real-world contract implementations</li> </ul>"},{"location":"developer-guide/data-contracts/#additional-resources","title":"Additional Resources","text":"<ul> <li>Pydantic Documentation - Learn more about Pydantic features</li> <li>Pandas Documentation - DataFrame operations and optimization</li> <li>API Documentation - Complete data contract API reference</li> </ul>"},{"location":"developer-guide/getting-started/","title":"Getting Started","text":"<p>This guide covers setting up your development environment, understanding the development workflow, and running your first tests with Wurzel.</p>"},{"location":"developer-guide/getting-started/#development-setup","title":"Development Setup","text":""},{"location":"developer-guide/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have completed the installation process. If you're setting up for development, you should have:</p> <ul> <li>Python 3.11 or 3.12</li> <li>Wurzel installed with development dependencies</li> <li>Pre-commit hooks configured</li> </ul>"},{"location":"developer-guide/getting-started/#verify-your-installation","title":"Verify Your Installation","text":"<pre><code># Check that Wurzel is properly installed\npython -c \"import wurzel; print('Wurzel installed successfully')\"\n\n# Verify CLI access\nwurzel --help\n</code></pre>"},{"location":"developer-guide/getting-started/#development-workflow","title":"Development Workflow","text":""},{"location":"developer-guide/getting-started/#code-quality-linting","title":"Code Quality &amp; Linting","text":"<p>Wurzel uses pre-commit hooks to enforce consistent code quality and formatting. These hooks run automatically on every commit to ensure code standards.</p>"},{"location":"developer-guide/getting-started/#set-up-pre-commit-hooks","title":"Set Up Pre-commit Hooks","text":"<p>If you used <code>make install</code>, pre-commit hooks are already configured.</p>"},{"location":"developer-guide/getting-started/#run-linting-manually","title":"Run Linting Manually","text":"<p>You can trigger the linting process manually at any time:</p> <pre><code>make lint\n</code></pre> <p>This runs all configured linters and formatters across the codebase.</p>"},{"location":"developer-guide/getting-started/#running-tests","title":"Running Tests","text":"<p>Before submitting changes, ensure all tests pass:</p> <pre><code># Run the complete test suite\nmake test\n</code></pre>"},{"location":"developer-guide/getting-started/#test-structure","title":"Test Structure","text":"<p>Wurzel's test suite includes:</p> <ul> <li>Unit tests: Testing individual components</li> <li>Integration tests: Testing component interactions</li> <li>End-to-end tests: Testing complete pipeline flows</li> </ul>"},{"location":"developer-guide/getting-started/#documentation","title":"Documentation","text":""},{"location":"developer-guide/getting-started/#local-documentation-development","title":"Local Documentation Development","text":"<p>Preview documentation changes locally:</p> <pre><code># Serve documentation locally (auto-reloads on changes)\nmake documentation\n\n# Build documentation without serving\nmkdocs build\n</code></pre> <p>The documentation will be available at <code>http://127.0.0.1:8000/</code></p>"},{"location":"developer-guide/getting-started/#documentation-structure","title":"Documentation Structure","text":"<p>Wurzel uses MkDocs for documentation management:</p> <ul> <li>Source files: Located in <code>docs/</code></li> <li>Configuration: <code>mkdocs.yml</code></li> <li>Auto-generated docs: Built from docstrings</li> </ul>"},{"location":"developer-guide/getting-started/#development-guidelines","title":"Development Guidelines","text":""},{"location":"developer-guide/getting-started/#commit-strategy","title":"Commit Strategy","text":"<p>Wurzel maintains a clean Git history through a structured commit strategy.</p>"},{"location":"developer-guide/getting-started/#commit-message-format","title":"Commit Message Format","text":"<p>Follow this structure for commit messages:</p> <pre><code>&lt;tag&gt;: &lt;short description&gt;\n\n&lt;longer description (optional)&gt;\n</code></pre>"},{"location":"developer-guide/getting-started/#commit-types","title":"Commit Types","text":"<ul> <li>Breaking Changes: For changes that are not backward-compatible</li> <li>Use tag: <code>breaking</code></li> <li>Features: For new features or enhancements that are backward-compatible</li> <li>Use tags: <code>feat</code>, <code>feature</code></li> <li>Fixes and Improvements: For bug fixes, performance improvements, or small patches</li> <li>Use tags: <code>fix</code>, <code>hotfix</code>, <code>perf</code>, <code>patch</code></li> </ul>"},{"location":"developer-guide/getting-started/#allowed-tags","title":"Allowed Tags","text":"<p>Ensure consistency by using these approved tags:</p> <ul> <li><code>feat</code>, <code>feature</code>, <code>fix</code>, <code>hotfix</code>, <code>perf</code>, <code>patch</code></li> <li><code>build</code>, <code>chore</code>, <code>ci</code>, <code>docs</code>, <code>style</code>, <code>refactor</code>, <code>ref</code>, <code>test</code></li> </ul>"},{"location":"developer-guide/getting-started/#examples","title":"Examples","text":"<pre><code># Good commit messages\ngit commit -m \"feat: add semantic text splitter for German documents\"\ngit commit -m \"fix: resolve memory leak in embedding generation\"\ngit commit -m \"docs: update installation guide with Docker instructions\"\n\n# Bad commit messages\ngit commit -m \"updated stuff\"\ngit commit -m \"fixed bug\"\n</code></pre>"},{"location":"developer-guide/getting-started/#merge-strategy","title":"Merge Strategy","text":"<ul> <li>All commits are squashed when merging into the main branch</li> <li>This maintains a clean, readable project history</li> <li>Focus on meaningful commit messages during development</li> </ul>"},{"location":"developer-guide/getting-started/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"developer-guide/getting-started/#adding-a-new-feature","title":"Adding a New Feature","text":"<ol> <li> <p>Create a feature branch: <pre><code>git checkout -b feat/your-feature-name\n</code></pre></p> </li> <li> <p>Implement your feature following the development guides</p> </li> <li> <p>Add tests for your feature:    <pre><code># Add tests in tests/ directory\npython -m pytest tests/test_your_feature.py\n</code></pre></p> </li> <li> <p>Update documentation if needed</p> </li> <li> <p>Run quality checks: <pre><code>make lint\nmake test\n</code></pre></p> </li> <li> <p>Commit using proper format: <pre><code>git commit -m \"feat: add your feature description\"\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/getting-started/#fixing-a-bug","title":"Fixing a Bug","text":"<ol> <li> <p>Create a fix branch: <pre><code>git checkout -b fix/bug-description\n</code></pre></p> </li> <li> <p>Write a failing test that reproduces the bug</p> </li> <li> <p>Implement the fix</p> </li> <li> <p>Verify the test passes: <pre><code>python -m pytest tests/test_bug_fix.py\n</code></pre></p> </li> <li> <p>Run full test suite: <pre><code>make test\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/getting-started/#working-with-dependencies","title":"Working with Dependencies","text":""},{"location":"developer-guide/getting-started/#adding-new-dependencies","title":"Adding New Dependencies","text":"<ol> <li> <p>Add to pyproject.toml in the appropriate section:    <pre><code>dependencies = [\n    \"existing-package&gt;=1.0.0\",\n    \"new-package&gt;=2.0.0\",\n]\n</code></pre></p> </li> <li> <p>For optional dependencies: <pre><code>[project.optional-dependencies]\nyour-extra = [\"optional-package&gt;=1.0.0\"]\n</code></pre></p> </li> <li> <p>Update installation: <pre><code>make install\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/getting-started/#direct-dependencies","title":"Direct Dependencies","text":"<p>For packages that can't be installed via PyPI (like spaCy models):</p> <ol> <li> <p>Add to DIRECT_REQUIREMENTS.txt: <pre><code>https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl\n</code></pre></p> </li> <li> <p>Document in installation guide if user-facing</p> </li> </ol>"},{"location":"developer-guide/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have your development environment set up:</p> <ol> <li>Build Your First Pipeline - Learn core pipeline concepts</li> <li>Create Custom Steps - Build your own processing components</li> <li>Understand Data Contracts - Learn about type-safe data exchange</li> <li>Explore Backends - Understand deployment options</li> </ol>"},{"location":"developer-guide/getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation - Auto-generated API reference</li> <li>Example Pipelines - Real-world implementation examples</li> </ul>"},{"location":"developer-guide/installation/","title":"Installation &amp; Setup","text":"<p>This comprehensive guide covers installing Wurzel and setting up your development environment, including handling special dependencies and various deployment options.</p>"},{"location":"developer-guide/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"developer-guide/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or 3.12</li> <li>pip (Python package installer)</li> <li>Virtual environment (recommended)</li> </ul>"},{"location":"developer-guide/installation/#quick-install","title":"Quick Install","text":"<p>To get started with Wurzel, install the library using pip:</p> <pre><code>pip install wurzel\n</code></pre>"},{"location":"developer-guide/installation/#development-installation","title":"Development Installation","text":"<p>For development work, we recommend using the provided Makefile which handles all dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/telekom/wurzel.git\ncd wurzel\n\n# Install all dependencies including development tools\nmake install\n</code></pre> <p>This installs:</p> <ul> <li>Core Wurzel library</li> <li>All optional dependencies</li> <li>Development tools (linting, testing, documentation)</li> <li>Pre-commit hooks setup</li> </ul>"},{"location":"developer-guide/installation/#manual-development-setup","title":"Manual Development Setup","text":"<p>If you prefer manual installation:</p> <pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode with all extras\npip install -e .[all,lint,test,docs]\n\n# Install direct dependencies (see below)\npip install -r DIRECT_REQUIREMENTS.txt\n\n# Set up pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"developer-guide/installation/#direct-dependencies","title":"Direct Dependencies","text":"<p>Due to PyPI restrictions on direct dependencies, some components require manual installation. This primarily affects the German spaCy model used for semantic text splitting.</p> <p>\u2139\ufe0f Why Direct Dependencies? PyPI has restrictions on packages that include direct dependencies to external URLs for security reasons. The spaCy German model is hosted on GitHub releases rather than PyPI, requiring manual installation.</p>"},{"location":"developer-guide/installation/#manual-installation","title":"Manual Installation","text":"<p>If you plan to use the semantic text splitting functionality (e.g., <code>SemanticSplitter</code>), you'll need to manually install the German spaCy model:</p> <pre><code>pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl\n</code></pre>"},{"location":"developer-guide/installation/#using-direct_requirementstxt","title":"Using DIRECT_REQUIREMENTS.txt","text":"<p>If you're working with the source code, you can install from the provided requirements file:</p> <pre><code>pip install -r DIRECT_REQUIREMENTS.txt\n</code></pre>"},{"location":"developer-guide/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Wurzel supports various optional features through extras. You can install only what you need for your specific use case.</p>"},{"location":"developer-guide/installation/#vector-database-support","title":"Vector Database Support","text":"<pre><code># For Qdrant vector database\npip install wurzel[qdrant]\n\n# For Milvus vector database\npip install wurzel[milvus]\n</code></pre>"},{"location":"developer-guide/installation/#document-processing","title":"Document Processing","text":"<pre><code># For PDF document processing with Docling\npip install wurzel[docling]\n</code></pre>"},{"location":"developer-guide/installation/#backend-support","title":"Backend Support","text":"<pre><code># For Argo Workflows backend\npip install wurzel[argo]\n</code></pre>"},{"location":"developer-guide/installation/#development-tools","title":"Development Tools","text":"<pre><code># For linting and code quality tools\npip install wurzel[lint]\n\n# For testing framework and tools\npip install wurzel[test]\n\n# For documentation generation\npip install wurzel[docs]\n</code></pre>"},{"location":"developer-guide/installation/#install-everything","title":"Install Everything","text":"<pre><code># Install all optional dependencies\npip install wurzel[all]\n\n# Don't forget the direct dependencies!\npip install -r DIRECT_REQUIREMENTS.txt\n</code></pre>"},{"location":"developer-guide/installation/#docker-installation","title":"Docker Installation","text":"<p>The Docker image includes all dependencies automatically and is the easiest way to get started:</p> <pre><code># Pull the latest image\ndocker pull ghcr.io/telekom/wurzel:latest\n\n# Or build locally\ndocker build -t wurzel .\n</code></pre>"},{"location":"developer-guide/installation/#running-with-docker","title":"Running with Docker","text":"<pre><code>docker run \\\n  -e GIT_USER=wurzel-demo \\\n  -e GIT_MAIL=demo@example.com \\\n  -e DVC_DATA_PATH=/usr/app/data \\\n  -e DVC_FILE=/usr/app/dvc.yaml \\\n  -e WURZEL_PIPELINE=pipelinedemo:pipeline \\\n  ghcr.io/telekom/wurzel:latest\n</code></pre>"},{"location":"developer-guide/installation/#environment-configuration","title":"Environment Configuration","text":"<p>When running Wurzel, several environment variables can be configured to customize behavior:</p>"},{"location":"developer-guide/installation/#git-configuration","title":"Git Configuration","text":"<ul> <li><code>GIT_USER</code>: Git username for repository operations (default: <code>wurzel</code>)</li> <li><code>GIT_MAIL</code>: Git email for repository operations (default: <code>wurzel@example.com</code>)</li> </ul>"},{"location":"developer-guide/installation/#dvc-configuration","title":"DVC Configuration","text":"<ul> <li><code>DVC_DATA_PATH</code>: Path where DVC stores data files (default: <code>/usr/app/dvc-data</code>)</li> <li><code>DVC_FILE</code>: Path to the DVC pipeline definition file (default: <code>/usr/app/dvc.yaml</code>)</li> <li><code>DVC_CACHE_HISTORY_NUMBER</code>: Number of cache versions to keep (default: <code>30</code>)</li> </ul>"},{"location":"developer-guide/installation/#pipeline-configuration","title":"Pipeline Configuration","text":"<ul> <li><code>WURZEL_PIPELINE</code>: Specifies which pipeline to execute (e.g., <code>pipelinedemo:pipeline</code>)</li> </ul>"},{"location":"developer-guide/installation/#monitoring-optional","title":"Monitoring (Optional)","text":"<ul> <li><code>PROMETHEUS__GATEWAY</code>: Prometheus pushgateway URL for metrics</li> </ul> <p>For backend-specific configuration, see:</p> <ul> <li>DVC Backend Configuration</li> <li>Argo Backend Configuration</li> </ul>"},{"location":"developer-guide/installation/#verification","title":"Verification","text":""},{"location":"developer-guide/installation/#verify-basic-installation","title":"Verify Basic Installation","text":"<pre><code># Check Wurzel installation\npython -c \"import wurzel; print('Wurzel installed successfully')\"\n\n# Check CLI is available\nwurzel --help\n</code></pre>"},{"location":"developer-guide/installation/#verify-spacy-model","title":"Verify spaCy Model","text":"<pre><code># Test spaCy model loading\npython -c \"import spacy; nlp = spacy.load('de_core_news_sm'); print('spaCy model loaded successfully')\"\n</code></pre>"},{"location":"developer-guide/installation/#run-tests","title":"Run Tests","text":"<pre><code># Run the test suite\nmake test\n\n# Or manually\npython -m pytest\n</code></pre>"},{"location":"developer-guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer-guide/installation/#common-issues","title":"Common Issues","text":""},{"location":"developer-guide/installation/#spacy-model-issues","title":"spaCy Model Issues","text":"<p>If you encounter issues with the spaCy model:</p> <ol> <li>Verify the model is installed:</li> </ol> <pre><code>python -c \"import spacy; nlp = spacy.load('de_core_news_sm'); print('Model loaded successfully')\"\n</code></pre> <ol> <li>Reinstall the model if needed:</li> </ol> <pre><code>pip uninstall de-core-news-sm\npip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl\n</code></pre>"},{"location":"developer-guide/installation/#environment-issues","title":"Environment Issues","text":"<ul> <li>Python Version: Ensure you're using Python 3.11 or 3.12</li> <li>Virtual Environment: Always use a virtual environment to avoid conflicts:</li> </ul> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install wurzel\n</code></pre>"},{"location":"developer-guide/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<ul> <li>Clean Installation: Start with a fresh virtual environment</li> <li>Upgrade pip: Ensure you have the latest pip version:</li> </ul> <pre><code>python -m pip install --upgrade pip\n</code></pre>"},{"location":"developer-guide/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the examples directory for working configurations</li> <li>Review the API documentation</li> <li>Consult the troubleshooting section in backend documentation</li> <li>Open an issue on the GitHub repository</li> </ol>"},{"location":"developer-guide/installation/#next-steps","title":"Next Steps","text":"<p>Once you have Wurzel installed:</p> <ol> <li>Get Started with Development - Set up your development workflow</li> <li>Build Your First Pipeline - Learn the core concepts</li> <li>Explore Backends - Understand deployment options</li> </ol>"},{"location":"developer-guide/runtime_vs_generatetime/","title":"Runtime vs. Generate Time","text":"<p>This document explains the two main phases of a <code>wurzel</code> pipeline: Generate Time and Runtime.</p> graph LR  subgraph \"Compile Time / Generate Time\"     direction TB     A(Python Codee.g., pipeline.py) -- defines --&gt; P(Wurzel Pipeline Object);     P -- wurzel generate --&gt; C(Backend Configuratione.g., Argo Workflow YAML / DVC YAML); end  subgraph \"Runtime\"     direction TB     C -- submitted to --&gt; R(Execution Backende.g., Argo Workflows );     R -- executes --&gt; S1(Step 1);     S1 -- then --&gt; S2(Step 2);     S2 -- then --&gt; Sn(Step N...);      subgraph \"Anatomy of a Single Step Execution\"         direction TB         subgraph S1             S1_Start(Container Start) --&gt; S1_Mounts(Secrets &amp; Volumesare mounted into the container's filesystem);             S1_Mounts --&gt; S1_Inputs(Input artifactsare downloaded/mounted);             S1_Inputs --&gt; S1_Code(Step's code/commandis executed);             S1_Code --&gt; S1_Outputs(Output artifactsare uploaded/saved);             S1_Outputs --&gt; S1_End(Container Stop);         end     end end  classDef compile fill:#E6F3FF,stroke:#007BFF,color:#000; classDef runtime fill:#E8F5E9,stroke:#4CAF50,color:#000; classDef step fill:#FFF3E0,stroke:#FF9800,color:#000;  class A,P,C compile; class R,S1,S2,Sn runtime; class S1_Start,S1_Mounts,S1_Inputs,S1_Code,S1_Outputs,S1_End step;"},{"location":"developer-guide/runtime_vs_generatetime/#key-concepts","title":"Key Concepts","text":""},{"location":"developer-guide/runtime_vs_generatetime/#generate-time-or-compile-time","title":"Generate Time (or Compile Time)","text":"<ul> <li>What it is: This is the phase where your Python pipeline definition is converted into a concrete, executable workflow for a specific backend (like Argo Workflows or DVC).</li> <li>Trigger: You run the <code>wurzel generate</code> command.</li> <li>Input: Your Python script containing a <code>wurzel</code> pipeline definition.</li> <li>Output: A configuration file (e.g., a <code>.yaml</code> file for Argo) that describes every step, their dependencies, the container images to use, and the commands to run.</li> </ul>"},{"location":"developer-guide/runtime_vs_generatetime/#runtime","title":"Runtime","text":"<ul> <li>What it is: This is the phase where the pipeline is actually executed by the backend.</li> <li>Trigger: You submit the generated configuration file to the backend (e.g., using <code>argo submit</code> or <code>dvc repro</code>).</li> <li>Process: The backend reads the workflow and executes the steps in the correct order.</li> <li>Important Aspects during a Step's Runtime:<ul> <li>Containerization: Each step typically runs in its own isolated container.</li> <li>Secret Mounting: Before your code runs, the backend mounts secrets (like API keys or database credentials) and configurations securely into the container's filesystem. Your code can then read them as if they were local files.</li> <li>Data I/O: The step downloads its necessary input artifacts, executes its logic, and uploads its resulting output artifacts.</li> </ul> </li> </ul>"},{"location":"steps/docling/","title":"Docling","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step","title":"<code>docling_step</code>","text":"<p>Note: Known Limitations with EasyOCR (<code>EasyOcrOptions</code>).</p> <ol> <li>Table structure is often lost or misaligned in the OCR output.</li> <li>Spelling inaccuracies are occasionally observed (e.g., \"Verl\u00e4ngern\" \u2192 \"Verl\u00e4ngenng\").</li> <li>URLs are not parsed correctly (e.g., \"www.telekom.de/agb\" \u2192 \"www telekom delagb\").</li> </ol> <p>While investigating EasyOCR issues and testing alternative OCR engines, we observed that some documents produced distorted text with irregular whitespace. This disrupts the natural sentence flow and significantly reduces readability.</p> <p>Example: \"pra kti sche  i nform ati o nen zu  i h rer  fam i l y  card  basi c Li eber   Tel ekom   Kunde, sch\u00f6n,   dass  Si e  si ch  f \u00fcr...\"</p> <p>Despite these limitations, we have decided to proceed with EasyOCR.</p>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step-classes","title":"Classes","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer","title":"<code>CleanMarkdownRenderer</code>","text":"<p>               Bases: <code>HTMLRenderer</code></p> <p>Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up unwanted elements from Markdown input.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>class CleanMarkdownRenderer(HTMLRenderer):\n    \"\"\"Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up\n    unwanted elements from Markdown input.\n    \"\"\"\n\n    @staticmethod\n    def render_html_block(token):\n        \"\"\"Render HTML block tokens and clean up unwanted elements.\n\n        This method removes HTML comments and returns the cleaned HTML content.\n        Remove comments like &lt;!-- image --&gt;\n        \"\"\"\n        soup = BeautifulSoup(token.content, \"html.parser\")\n\n        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n            comment.extract()\n        return soup.decode_contents().strip()\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer-functions","title":"Functions","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer.render_html_block","title":"<code>render_html_block(token)</code>  <code>staticmethod</code>","text":"<p>Render HTML block tokens and clean up unwanted elements.</p> <p>This method removes HTML comments and returns the cleaned HTML content. Remove comments like </p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>@staticmethod\ndef render_html_block(token):\n    \"\"\"Render HTML block tokens and clean up unwanted elements.\n\n    This method removes HTML comments and returns the cleaned HTML content.\n    Remove comments like &lt;!-- image --&gt;\n    \"\"\"\n    soup = BeautifulSoup(token.content, \"html.parser\")\n\n    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n        comment.extract()\n    return soup.decode_contents().strip()\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep","title":"<code>DoclingStep</code>","text":"<p>               Bases: <code>TypedStep[DoclingSettings, None, list[MarkdownDataContract]]</code></p> <p>Step to return local Markdown files with enhanced PDF extraction for German.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>class DoclingStep(TypedStep[DoclingSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Step to return local Markdown files with enhanced PDF extraction for German.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.converter = self.create_converter()\n\n    def create_converter(self) -&gt; DocumentConverter:\n        \"\"\"Create and configure the document converter for PDF and DOCX.\n\n        Returns:\n            DocumentConverter: Configured document converter.\n\n        \"\"\"\n        pipeline_options = PdfPipelineOptions()\n        ocr_options = EasyOcrOptions()\n        pipeline_options.ocr_options = ocr_options\n\n        return DocumentConverter(\n            allowed_formats=self.settings.FORMATS,\n            format_options={\n                InputFormat.PDF: PdfFormatOption(\n                    pipeline_options=pipeline_options,\n                )\n            },\n        )\n\n    @staticmethod\n    def extract_keywords(md_text: str) -&gt; str:\n        \"\"\"Cleans a Markdown string using mistletoe and extracts useful content.\n\n        - Parses and renders the Markdown content into HTML using a custom HTML renderer\n        - Removes unwanted HTML comments and escaped underscores\n        - Extracts the first heading from the content (e.g., `&lt;h1&gt;` to `&lt;h6&gt;`)\n        - Converts the cleaned HTML into plain text\n\n        Args:\n            md_text (str): The raw Markdown input string.\n\n        \"\"\"\n        with MD_RENDER_LOCK, CleanMarkdownRenderer() as renderer:\n            ast = MTDocument(md_text)\n            cleaned = renderer.render(ast).replace(\"\\n\", \"\")\n            soup = BeautifulSoup(cleaned, \"html.parser\")\n            first_heading_tag = soup.find([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n            heading = first_heading_tag.get_text(strip=True) if first_heading_tag else \"\"\n\n        return heading\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        \"\"\"Run the document extraction and conversion process for German PDFs.\n\n        Args:\n            inpt (None): Input parameter (not used).\n\n        Returns:\n            List[MarkdownDataContract]: List of converted Markdown contracts.\n\n        \"\"\"\n        urls = self.settings.URLS\n        contracts = []\n\n        for url in urls:\n            try:\n                converted_contract = self.converter.convert(url)\n                md = converted_contract.document.export_to_markdown(image_placeholder=\"\")\n                keyword = self.extract_keywords(md)\n                contract_instance = {\"md\": md, \"keywords\": \" \".join([self.settings.DEFAULT_KEYWORD, keyword]), \"url\": url}\n                contracts.append(contract_instance)\n\n            except (FileNotFoundError, OSError) as e:\n                log.warning(f\"Failed to verify URL: {url}. Error: {e}\")\n                continue\n\n        return contracts\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep-functions","title":"Functions","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.create_converter","title":"<code>create_converter()</code>","text":"<p>Create and configure the document converter for PDF and DOCX.</p> <p>Returns:</p> Name Type Description <code>DocumentConverter</code> <code>DocumentConverter</code> <p>Configured document converter.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>def create_converter(self) -&gt; DocumentConverter:\n    \"\"\"Create and configure the document converter for PDF and DOCX.\n\n    Returns:\n        DocumentConverter: Configured document converter.\n\n    \"\"\"\n    pipeline_options = PdfPipelineOptions()\n    ocr_options = EasyOcrOptions()\n    pipeline_options.ocr_options = ocr_options\n\n    return DocumentConverter(\n        allowed_formats=self.settings.FORMATS,\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options,\n            )\n        },\n    )\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.extract_keywords","title":"<code>extract_keywords(md_text)</code>  <code>staticmethod</code>","text":"<p>Cleans a Markdown string using mistletoe and extracts useful content.</p> <ul> <li>Parses and renders the Markdown content into HTML using a custom HTML renderer</li> <li>Removes unwanted HTML comments and escaped underscores</li> <li>Extracts the first heading from the content (e.g., <code>&lt;h1&gt;</code> to <code>&lt;h6&gt;</code>)</li> <li>Converts the cleaned HTML into plain text</li> </ul> <p>Parameters:</p> Name Type Description Default <code>md_text</code> <code>str</code> <p>The raw Markdown input string.</p> required Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>@staticmethod\ndef extract_keywords(md_text: str) -&gt; str:\n    \"\"\"Cleans a Markdown string using mistletoe and extracts useful content.\n\n    - Parses and renders the Markdown content into HTML using a custom HTML renderer\n    - Removes unwanted HTML comments and escaped underscores\n    - Extracts the first heading from the content (e.g., `&lt;h1&gt;` to `&lt;h6&gt;`)\n    - Converts the cleaned HTML into plain text\n\n    Args:\n        md_text (str): The raw Markdown input string.\n\n    \"\"\"\n    with MD_RENDER_LOCK, CleanMarkdownRenderer() as renderer:\n        ast = MTDocument(md_text)\n        cleaned = renderer.render(ast).replace(\"\\n\", \"\")\n        soup = BeautifulSoup(cleaned, \"html.parser\")\n        first_heading_tag = soup.find([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n        heading = first_heading_tag.get_text(strip=True) if first_heading_tag else \"\"\n\n    return heading\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.run","title":"<code>run(inpt)</code>","text":"<p>Run the document extraction and conversion process for German PDFs.</p> <p>Parameters:</p> Name Type Description Default <code>inpt</code> <code>None</code> <p>Input parameter (not used).</p> required <p>Returns:</p> Type Description <code>list[MarkdownDataContract]</code> <p>List[MarkdownDataContract]: List of converted Markdown contracts.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n    \"\"\"Run the document extraction and conversion process for German PDFs.\n\n    Args:\n        inpt (None): Input parameter (not used).\n\n    Returns:\n        List[MarkdownDataContract]: List of converted Markdown contracts.\n\n    \"\"\"\n    urls = self.settings.URLS\n    contracts = []\n\n    for url in urls:\n        try:\n            converted_contract = self.converter.convert(url)\n            md = converted_contract.document.export_to_markdown(image_placeholder=\"\")\n            keyword = self.extract_keywords(md)\n            contract_instance = {\"md\": md, \"keywords\": \" \".join([self.settings.DEFAULT_KEYWORD, keyword]), \"url\": url}\n            contracts.append(contract_instance)\n\n        except (FileNotFoundError, OSError) as e:\n            log.warning(f\"Failed to verify URL: {url}. Error: {e}\")\n            continue\n\n    return contracts\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.settings","title":"<code>settings</code>","text":"<p>Specific docling settings.</p>"},{"location":"steps/docling/#wurzel.steps.docling.settings-classes","title":"Classes","text":""},{"location":"steps/docling/#wurzel.steps.docling.settings.DoclingSettings","title":"<code>DoclingSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>DoclingSettings is a configuration class that inherits from the base <code>Settings</code> class. It provides customizable settings for document processing.</p> <p>Attributes:</p> Name Type Description <code>FORCE_FULL_PAGE_OCR</code> <code>bool</code> <p>A flag to enforce full-page OCR processing. Defaults to True.</p> <code>FORMATS</code> <code>list[InputFormat]</code> <p>A list of supported input formats for document processing. Supported formats include: - \"docx\" - \"asciidoc\" - \"pptx\" - \"html\" - \"image\" - \"pdf\" - \"md\" - \"csv\" - \"xlsx\" - \"xml_uspto\" - \"xml_jats\" - \"json_docling\"</p> <code>URLS</code> <code>list[str]</code> <p>A list of URLs for additional configuration or resources. Defaults to an empty list.</p> Source code in <code>wurzel/steps/docling/settings.py</code> <pre><code>class DoclingSettings(Settings):\n    \"\"\"DoclingSettings is a configuration class that inherits from the base `Settings` class.\n    It provides customizable settings for document processing.\n\n    Attributes:\n        FORCE_FULL_PAGE_OCR (bool): A flag to enforce full-page OCR processing. Defaults to True.\n        FORMATS (list[InputFormat]): A list of supported input formats for document processing.\n            Supported formats include:\n            - \"docx\"\n            - \"asciidoc\"\n            - \"pptx\"\n            - \"html\"\n            - \"image\"\n            - \"pdf\"\n            - \"md\"\n            - \"csv\"\n            - \"xlsx\"\n            - \"xml_uspto\"\n            - \"xml_jats\"\n            - \"json_docling\"\n        URLS (list[str]): A list of URLs for additional configuration or resources. Defaults to an empty list.\n\n    \"\"\"\n\n    FORCE_FULL_PAGE_OCR: bool = True\n    FORMATS: list[InputFormat] = [\n        \"docx\",\n        \"asciidoc\",\n        \"pptx\",\n        \"html\",\n        \"image\",\n        \"pdf\",\n        \"md\",\n        \"csv\",\n        \"xlsx\",\n        \"xml_uspto\",\n        \"xml_jats\",\n        \"json_docling\",\n    ]\n    URLS: list[str] = []\n    DEFAULT_KEYWORD: str = \"\"\n</code></pre>"},{"location":"steps/duplication/","title":"Deduplication","text":""},{"location":"steps/duplication/#wurzel.steps.duplication","title":"<code>duplication</code>","text":"<p>consists of DVCSteps to embedd files and save them as for example as csv.</p>"},{"location":"steps/duplication/#wurzel.steps.duplication-classes","title":"Classes","text":""},{"location":"steps/duplication/#wurzel.steps.duplication.DropDuplicationStep","title":"<code>DropDuplicationStep</code>","text":"<p>               Bases: <code>TypedStep[DropStettings, list[MarkdownDataContract], list[MarkdownDataContract]]</code></p> <p>SimpleSplitterStep to split Markdown Documents rundimentory in medium size chunks.</p> Source code in <code>wurzel/steps/duplication.py</code> <pre><code>class DropDuplicationStep(TypedStep[DropStettings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"SimpleSplitterStep to split Markdown Documents rundimentory in medium size chunks.\"\"\"\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Executes the split step by processing input markdown files, generating smaller splitted markdown files,\n        by preserving the headline.\n        \"\"\"\n        if self.settings.DROP_BY_FIELDS == [\"*\"]:\n            self.settings.DROP_BY_FIELDS = None\n        df = pd.DataFrame(i.model_dump() for i in inpt)\n        if not df.duplicated(self.settings.DROP_BY_FIELDS).any():\n            return inpt\n\n        filtered = df.drop_duplicates(self.settings.DROP_BY_FIELDS)\n        log.warning(\n            \"Removed duplicates\",\n            extra={\n                \"percentage\": len(filtered) / len(df),\n                \"before\": len(df),\n                \"after\": len(filtered),\n                \"by\": str(self.settings.DROP_BY_FIELDS),\n            },\n        )\n        dumped = filtered.to_dict(orient=\"records\")\n        return [MarkdownDataContract.model_construct(**f) for f in dumped]\n</code></pre>"},{"location":"steps/duplication/#wurzel.steps.duplication.DropDuplicationStep-functions","title":"Functions","text":""},{"location":"steps/duplication/#wurzel.steps.duplication.DropDuplicationStep.run","title":"<code>run(inpt)</code>","text":"<p>Executes the split step by processing input markdown files, generating smaller splitted markdown files, by preserving the headline.</p> Source code in <code>wurzel/steps/duplication.py</code> <pre><code>def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n    \"\"\"Executes the split step by processing input markdown files, generating smaller splitted markdown files,\n    by preserving the headline.\n    \"\"\"\n    if self.settings.DROP_BY_FIELDS == [\"*\"]:\n        self.settings.DROP_BY_FIELDS = None\n    df = pd.DataFrame(i.model_dump() for i in inpt)\n    if not df.duplicated(self.settings.DROP_BY_FIELDS).any():\n        return inpt\n\n    filtered = df.drop_duplicates(self.settings.DROP_BY_FIELDS)\n    log.warning(\n        \"Removed duplicates\",\n        extra={\n            \"percentage\": len(filtered) / len(df),\n            \"before\": len(df),\n            \"after\": len(filtered),\n            \"by\": str(self.settings.DROP_BY_FIELDS),\n        },\n    )\n    dumped = filtered.to_dict(orient=\"records\")\n    return [MarkdownDataContract.model_construct(**f) for f in dumped]\n</code></pre>"},{"location":"steps/duplication/#wurzel.steps.duplication.DropStettings","title":"<code>DropStettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>specify DROP_BY_FIELDS to field.</p> Source code in <code>wurzel/steps/duplication.py</code> <pre><code>class DropStettings(Settings):\n    \"\"\"specify DROP_BY_FIELDS to field.\"\"\"\n\n    DROP_BY_FIELDS: list[str] = [\"md\"]\n</code></pre>"},{"location":"steps/embedding/","title":"Embedding","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step","title":"<code>step</code>","text":"<p>consists of DVCSteps to embedd files and save them as for example as csv.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step-classes","title":"Classes","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep","title":"<code>BaseEmbeddingStep</code>","text":"<p>               Bases: <code>TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingResult]]</code></p> <p>Parent class for embedding-related steps implementing common methods.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>class BaseEmbeddingStep(TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingResult]]):\n    \"\"\"Parent class for embedding-related steps implementing common methods.\"\"\"\n\n    embedding: HuggingFaceInferenceAPIEmbeddings\n    n_jobs: int\n    markdown: Markdown\n    stopwords: list[str]\n    settings: EmbeddingSettings\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.embedding = self._select_embedding()\n        self.n_jobs = max(1, (os.cpu_count() or 0) - 1)\n        # Inject net output_format into 3rd party library Markdown\n        Markdown.output_formats[\"plain\"] = self.__md_to_plain  # type: ignore[index]\n        self.markdown = Markdown(output_format=\"plain\")  # type: ignore[arg-type]\n        self.markdown.stripTopLevelTags = False\n        self.stopwords = self._load_stopwords()\n\n    def _load_stopwords(self) -&gt; list[str]:\n        \"\"\"Load stopwords from path (stop words are used by `get_simple_context`).\"\"\"\n        path = self.settings.STEPWORDS_PATH\n        with path.open(encoding=\"utf-8\") as f:\n            stopwords = [w.strip() for w in f.readlines() if not w.startswith(\";\")]\n        return stopwords\n\n    def _select_embedding(self) -&gt; HuggingFaceInferenceAPIEmbeddings:\n        \"\"\"Selects the embedding model to be used for generating embeddings.\n\n        Returns:\n        -------\n        Embeddings\n            An instance of the Embeddings class.\n\n        \"\"\"\n        return PrefixedAPIEmbeddings(self.settings.API, self.settings.PREFIX_MAP)\n\n    def log_statistics(self, series: np.ndarray, name: str):\n        \"\"\"Log descriptive statistics for all documents.\n\n        Parameters\n        ----------\n        series : np.ndarray\n            Numerical values representing the documents.\n        name : str\n            The name of the document metric.\n        \"\"\"\n        stats = {\n            \"count\": len(series),\n            \"mean\": None,\n            \"std\": None,\n        }\n\n        if len(series) &gt; 0:\n            stats.update(\n                {\n                    \"mean\": np.mean(series),\n                    \"median\": np.median(series),\n                    \"std\": np.std(series),\n                    \"var\": np.var(series),\n                    \"min\": np.min(series),\n                    \"percentile_5\": np.percentile(series, 5),\n                    \"percentile_25\": np.percentile(series, 25),\n                    \"percentile_75\": np.percentile(series, 75),\n                    \"percentile_95\": np.percentile(series, 95),\n                    \"max\": np.max(series),\n                }\n            )\n\n        log.info(f\"Distribution of {name}: count={stats['count']}; mean={stats['mean']}; std={stats['std']}\", extra=stats)\n\n    def get_embedding_input_from_document(self, doc: MarkdownDataContract) -&gt; str:\n        \"\"\"Clean the document such that it can be used as input to the embedding model.\n\n        Parameters\n        ----------\n        doc : MarkdownDataContract\n            The document containing the page content in Markdown format.\n\n        Returns:\n        -------\n        str\n            Cleaned text that can be used as input to the embedding model.\n\n        \"\"\"\n        plain_text = self.markdown.convert(doc.md)\n        plain_text = self._replace_link(plain_text)\n\n        return plain_text\n\n    def _get_embedding(self, doc: MarkdownDataContract) -&gt; Embedded:\n        \"\"\"Generates an embedding for a given text and context.\n\n        Parameters\n        ----------\n        d : dict\n            A dictionary containing the text and context for which to generate the embedding.\n\n        Returns:\n        -------\n        dict\n            A dictionary containing the original text, its embedding, and the source URL.\n\n        \"\"\"\n        context = self.get_simple_context(doc.keywords)\n        text = self.get_embedding_input_from_document(doc) if self.settings.CLEAN_MD_BEFORE_EMBEDDING else doc.md\n        vector = self.embedding.embed_query(text)\n        return {\n            \"text\": doc.md,\n            \"vector\": vector,\n            \"url\": doc.url,\n            \"keywords\": context,\n            \"embedding_input_text\": text,\n            \"metadata\": doc.metadata or {},\n        }\n\n    def is_stopword(self, word: str) -&gt; bool:\n        \"\"\"Stopword Detection Function.\"\"\"\n        return word.lower() in self.stopwords\n\n    @classmethod\n    def whitespace_word_tokenizer(cls, text: str) -&gt; list[str]:\n        \"\"\"Simple Regex based whitespace word tokenizer.\"\"\"\n        return [x for x in _WHITESPACE_PATTERN.split(text) if x]\n\n    def get_simple_context(self, text):\n        \"\"\"Simple function to create a context from a text.\"\"\"\n        tokens = self.whitespace_word_tokenizer(text)\n        filtered_tokens = [token for token in tokens if not self.is_stopword(token)]\n        return \" \".join(filtered_tokens)\n\n    @classmethod\n    def __md_to_plain(cls, element, stream: Optional[StringIO] = None):\n        \"\"\"Converts a markdown element into plain text.\n\n        Parameters\n        ----------\n        element : Element\n            The markdown element to convert.\n        stream : StringIO, optional\n            The stream to which the plain text is written. If None, a new stream is created.\n\n        Returns:\n        -------\n        str\n            The plain text representation of the markdown element.\n\n        \"\"\"\n        if stream is None:\n            stream = StringIO()\n        if element.text:\n            stream.write(element.text)\n        for sub in element:\n            cls.__md_to_plain(sub, stream)\n        if element.tail:\n            stream.write(element.tail)\n        return stream.getvalue()\n\n    @classmethod\n    def _replace_link(cls, text: str):\n        \"\"\"Replaces URLs in the text with a placeholder.\n\n        Parameters\n        ----------\n        text : str\n            The text in which URLs will be replaced.\n\n        Returns:\n        -------\n        str\n            The text with URLs replaced by 'LINK'.\n\n        \"\"\"\n        # Use precompiled pattern for better performance\n        links = sorted(_URL_PATTERN.findall(text), key=len, reverse=True)\n        for link in links:\n            text = text.replace(link, \"LINK\")\n        return text\n\n    def preprocess_inputs(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Custom preprocessing of inputs (called by `run` before embedding calculation).\"\"\"\n        raise NotImplementedError\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingResult]:\n        \"\"\"Executes the embedding step by processing input markdown files, generating embeddings,\n        and saving them to a CSV file.\n        \"\"\"\n        if len(inpt) == 0:\n            log.info(\"Got empty result in Embedding - Skipping\")\n            return DataFrame[EmbeddingResult]([])\n\n        preprocessed_inputs = self.preprocess_inputs(inpt)\n\n        output_rows = []\n        failed = 0\n        stats = defaultdict(list)\n\n        for input_row in tqdm(preprocessed_inputs, desc=\"Calculate Embeddings\"):\n            try:\n                output_rows.append(self._get_embedding(input_row))\n\n                # collect statistics\n                if input_row.metadata is not None:\n                    stats[\"char length\"].append(input_row.metadata.get(\"char_len\", 0))\n                    stats[\"token length\"].append(input_row.metadata.get(\"token_len\", 0))\n                    stats[\"chunks count\"].append(input_row.metadata.get(\"chunks_count\", 0))\n\n            except EmbeddingAPIException as err:\n                log.warning(\n                    f\"Skipped because EmbeddingAPIException: {err.message}\",\n                    extra={\"markdown\": str(input_row)},\n                )\n                failed += 1\n        if failed:\n            log.warning(f\"{failed}/{len(preprocessed_inputs)} got skipped\")\n        if failed == len(preprocessed_inputs):\n            raise StepFailed(f\"all {len(preprocessed_inputs)} embeddings got skipped\")\n\n        # log statistics\n        for k, v in stats.items():\n            self.log_statistics(series=np.array(v), name=k)\n\n        return DataFrame[EmbeddingResult](output_rows)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep-functions","title":"Functions","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.__md_to_plain","title":"<code>__md_to_plain(element, stream=None)</code>  <code>classmethod</code>","text":"<p>Converts a markdown element into plain text.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.__md_to_plain--parameters","title":"Parameters","text":"<p>element : Element     The markdown element to convert. stream : StringIO, optional     The stream to which the plain text is written. If None, a new stream is created.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.__md_to_plain--returns","title":"Returns:","text":"<p>str     The plain text representation of the markdown element.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>@classmethod\ndef __md_to_plain(cls, element, stream: Optional[StringIO] = None):\n    \"\"\"Converts a markdown element into plain text.\n\n    Parameters\n    ----------\n    element : Element\n        The markdown element to convert.\n    stream : StringIO, optional\n        The stream to which the plain text is written. If None, a new stream is created.\n\n    Returns:\n    -------\n    str\n        The plain text representation of the markdown element.\n\n    \"\"\"\n    if stream is None:\n        stream = StringIO()\n    if element.text:\n        stream.write(element.text)\n    for sub in element:\n        cls.__md_to_plain(sub, stream)\n    if element.tail:\n        stream.write(element.tail)\n    return stream.getvalue()\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.get_embedding_input_from_document","title":"<code>get_embedding_input_from_document(doc)</code>","text":"<p>Clean the document such that it can be used as input to the embedding model.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.get_embedding_input_from_document--parameters","title":"Parameters","text":"<p>doc : MarkdownDataContract     The document containing the page content in Markdown format.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.get_embedding_input_from_document--returns","title":"Returns:","text":"<p>str     Cleaned text that can be used as input to the embedding model.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def get_embedding_input_from_document(self, doc: MarkdownDataContract) -&gt; str:\n    \"\"\"Clean the document such that it can be used as input to the embedding model.\n\n    Parameters\n    ----------\n    doc : MarkdownDataContract\n        The document containing the page content in Markdown format.\n\n    Returns:\n    -------\n    str\n        Cleaned text that can be used as input to the embedding model.\n\n    \"\"\"\n    plain_text = self.markdown.convert(doc.md)\n    plain_text = self._replace_link(plain_text)\n\n    return plain_text\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.get_simple_context","title":"<code>get_simple_context(text)</code>","text":"<p>Simple function to create a context from a text.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def get_simple_context(self, text):\n    \"\"\"Simple function to create a context from a text.\"\"\"\n    tokens = self.whitespace_word_tokenizer(text)\n    filtered_tokens = [token for token in tokens if not self.is_stopword(token)]\n    return \" \".join(filtered_tokens)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.is_stopword","title":"<code>is_stopword(word)</code>","text":"<p>Stopword Detection Function.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def is_stopword(self, word: str) -&gt; bool:\n    \"\"\"Stopword Detection Function.\"\"\"\n    return word.lower() in self.stopwords\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.log_statistics","title":"<code>log_statistics(series, name)</code>","text":"<p>Log descriptive statistics for all documents.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.log_statistics--parameters","title":"Parameters","text":"<p>series : np.ndarray     Numerical values representing the documents. name : str     The name of the document metric.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def log_statistics(self, series: np.ndarray, name: str):\n    \"\"\"Log descriptive statistics for all documents.\n\n    Parameters\n    ----------\n    series : np.ndarray\n        Numerical values representing the documents.\n    name : str\n        The name of the document metric.\n    \"\"\"\n    stats = {\n        \"count\": len(series),\n        \"mean\": None,\n        \"std\": None,\n    }\n\n    if len(series) &gt; 0:\n        stats.update(\n            {\n                \"mean\": np.mean(series),\n                \"median\": np.median(series),\n                \"std\": np.std(series),\n                \"var\": np.var(series),\n                \"min\": np.min(series),\n                \"percentile_5\": np.percentile(series, 5),\n                \"percentile_25\": np.percentile(series, 25),\n                \"percentile_75\": np.percentile(series, 75),\n                \"percentile_95\": np.percentile(series, 95),\n                \"max\": np.max(series),\n            }\n        )\n\n    log.info(f\"Distribution of {name}: count={stats['count']}; mean={stats['mean']}; std={stats['std']}\", extra=stats)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.preprocess_inputs","title":"<code>preprocess_inputs(inpt)</code>","text":"<p>Custom preprocessing of inputs (called by <code>run</code> before embedding calculation).</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def preprocess_inputs(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n    \"\"\"Custom preprocessing of inputs (called by `run` before embedding calculation).\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.run","title":"<code>run(inpt)</code>","text":"<p>Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingResult]:\n    \"\"\"Executes the embedding step by processing input markdown files, generating embeddings,\n    and saving them to a CSV file.\n    \"\"\"\n    if len(inpt) == 0:\n        log.info(\"Got empty result in Embedding - Skipping\")\n        return DataFrame[EmbeddingResult]([])\n\n    preprocessed_inputs = self.preprocess_inputs(inpt)\n\n    output_rows = []\n    failed = 0\n    stats = defaultdict(list)\n\n    for input_row in tqdm(preprocessed_inputs, desc=\"Calculate Embeddings\"):\n        try:\n            output_rows.append(self._get_embedding(input_row))\n\n            # collect statistics\n            if input_row.metadata is not None:\n                stats[\"char length\"].append(input_row.metadata.get(\"char_len\", 0))\n                stats[\"token length\"].append(input_row.metadata.get(\"token_len\", 0))\n                stats[\"chunks count\"].append(input_row.metadata.get(\"chunks_count\", 0))\n\n        except EmbeddingAPIException as err:\n            log.warning(\n                f\"Skipped because EmbeddingAPIException: {err.message}\",\n                extra={\"markdown\": str(input_row)},\n            )\n            failed += 1\n    if failed:\n        log.warning(f\"{failed}/{len(preprocessed_inputs)} got skipped\")\n    if failed == len(preprocessed_inputs):\n        raise StepFailed(f\"all {len(preprocessed_inputs)} embeddings got skipped\")\n\n    # log statistics\n    for k, v in stats.items():\n        self.log_statistics(series=np.array(v), name=k)\n\n    return DataFrame[EmbeddingResult](output_rows)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.BaseEmbeddingStep.whitespace_word_tokenizer","title":"<code>whitespace_word_tokenizer(text)</code>  <code>classmethod</code>","text":"<p>Simple Regex based whitespace word tokenizer.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>@classmethod\ndef whitespace_word_tokenizer(cls, text: str) -&gt; list[str]:\n    \"\"\"Simple Regex based whitespace word tokenizer.\"\"\"\n    return [x for x in _WHITESPACE_PATTERN.split(text) if x]\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.Embedded","title":"<code>Embedded</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>dict definition of a embedded document.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>class Embedded(TypedDict):\n    \"\"\"dict definition of a embedded document.\"\"\"\n\n    text: str\n    url: str\n    vector: list[float]\n    embedding_input_text: str\n    metadata: dict\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep","title":"<code>EmbeddingStep</code>","text":"<p>               Bases: <code>BaseEmbeddingStep</code>, <code>SimpleSplitterStep</code></p> <p>Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult].</p> <p>This step inherits both from BaseEmbeddingStep and SimpleSplitterStep, meaning that inputs are first splitted and then embeddings of the splits are generated.</p> <p>TODO Due to this, a better name of this step would be \"EmbeddingSplitterStep\", but keeping the name to avoid breaking changes.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>class EmbeddingStep(BaseEmbeddingStep, SimpleSplitterStep):\n    \"\"\"Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult].\n\n    This step inherits both from BaseEmbeddingStep and SimpleSplitterStep, meaning that\n    inputs are first splitted and then embeddings of the splits are generated.\n\n    TODO Due to this, a better name of this step would be \"EmbeddingSplitterStep\", but keeping the name to avoid breaking changes.\n    \"\"\"\n\n    def preprocess_inputs(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Split inputs into chunks (called by `run` before embedding calculation).\"\"\"\n        return self._split_markdown(inpt)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep-functions","title":"Functions","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.preprocess_inputs","title":"<code>preprocess_inputs(inpt)</code>","text":"<p>Split inputs into chunks (called by <code>run</code> before embedding calculation).</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def preprocess_inputs(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n    \"\"\"Split inputs into chunks (called by `run` before embedding calculation).\"\"\"\n    return self._split_markdown(inpt)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.TruncatedEmbeddingStep","title":"<code>TruncatedEmbeddingStep</code>","text":"<p>               Bases: <code>BaseEmbeddingStep</code></p> <p>Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult].</p> <p>In contrast to <code>EmbeddingStep</code>, this step does not perform any splitting but instead the steps truncates all inputs such that the max. token count is fulfiled.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>class TruncatedEmbeddingStep(BaseEmbeddingStep):\n    \"\"\"Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult].\n\n    In contrast to `EmbeddingStep`, this step does not perform any splitting but instead the\n    steps truncates all inputs such that the max. token count is fulfiled.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n        self.tokenizer = Tokenizer.from_name(self.settings.TOKENIZER_MODEL)\n\n    def preprocess_inputs(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"No custom processing is performed.\"\"\"\n        return inpt\n\n    def get_embedding_input_from_document(self, doc: MarkdownDataContract) -&gt; str:\n        \"\"\"Clean the document and truncate it to max. token count such that it can be used as input to the embedding model.\n\n        The setting `CLEAN_MD_BEFORE_EMBEDDING` must be enabled.\n\n        Parameters\n        ----------\n        doc : MarkdownDataContract\n            The document containing the page content in Markdown format.\n\n        Returns:\n        -------\n        str\n            Cleaned and truncated text that can be used as input to the embedding model.\n\n        \"\"\"\n        plain_text = super().get_embedding_input_from_document(doc)\n\n        token_ids = self.tokenizer.encode(plain_text)\n\n        if len(token_ids) &gt; self.settings.TOKEN_COUNT_MAX:\n            log.warning(\n                \"Truncating tokens from embedding input text: %i truncated tokens; %i input tokens &gt; %i max tokens\",\n                len(token_ids) - self.settings.TOKEN_COUNT_MAX,\n                len(token_ids),\n                self.settings.TOKEN_COUNT_MAX,\n                extra={\n                    \"text\": plain_text,\n                    \"input_token_count\": len(token_ids),\n                    \"max_token_count\": self.settings.TOKEN_COUNT_MAX,\n                },\n            )\n\n            token_ids = token_ids[: self.settings.TOKEN_COUNT_MAX]\n\n        return self.tokenizer.decode(token_ids)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.TruncatedEmbeddingStep-functions","title":"Functions","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step.TruncatedEmbeddingStep.get_embedding_input_from_document","title":"<code>get_embedding_input_from_document(doc)</code>","text":"<p>Clean the document and truncate it to max. token count such that it can be used as input to the embedding model.</p> <p>The setting <code>CLEAN_MD_BEFORE_EMBEDDING</code> must be enabled.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.TruncatedEmbeddingStep.get_embedding_input_from_document--parameters","title":"Parameters","text":"<p>doc : MarkdownDataContract     The document containing the page content in Markdown format.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.TruncatedEmbeddingStep.get_embedding_input_from_document--returns","title":"Returns:","text":"<p>str     Cleaned and truncated text that can be used as input to the embedding model.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def get_embedding_input_from_document(self, doc: MarkdownDataContract) -&gt; str:\n    \"\"\"Clean the document and truncate it to max. token count such that it can be used as input to the embedding model.\n\n    The setting `CLEAN_MD_BEFORE_EMBEDDING` must be enabled.\n\n    Parameters\n    ----------\n    doc : MarkdownDataContract\n        The document containing the page content in Markdown format.\n\n    Returns:\n    -------\n    str\n        Cleaned and truncated text that can be used as input to the embedding model.\n\n    \"\"\"\n    plain_text = super().get_embedding_input_from_document(doc)\n\n    token_ids = self.tokenizer.encode(plain_text)\n\n    if len(token_ids) &gt; self.settings.TOKEN_COUNT_MAX:\n        log.warning(\n            \"Truncating tokens from embedding input text: %i truncated tokens; %i input tokens &gt; %i max tokens\",\n            len(token_ids) - self.settings.TOKEN_COUNT_MAX,\n            len(token_ids),\n            self.settings.TOKEN_COUNT_MAX,\n            extra={\n                \"text\": plain_text,\n                \"input_token_count\": len(token_ids),\n                \"max_token_count\": self.settings.TOKEN_COUNT_MAX,\n            },\n        )\n\n        token_ids = token_ids[: self.settings.TOKEN_COUNT_MAX]\n\n    return self.tokenizer.decode(token_ids)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.TruncatedEmbeddingStep.preprocess_inputs","title":"<code>preprocess_inputs(inpt)</code>","text":"<p>No custom processing is performed.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def preprocess_inputs(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n    \"\"\"No custom processing is performed.\"\"\"\n    return inpt\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector","title":"<code>step_multivector</code>","text":"<p>consists of DVCSteps to embedd files and save them as for example as csv.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector-classes","title":"Classes","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep","title":"<code>EmbeddingMultiVectorStep</code>","text":"<p>               Bases: <code>EmbeddingStep</code>, <code>TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingMultiVectorResult]]</code></p> <p>Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingMultiVectorResult].</p> Source code in <code>wurzel/steps/embedding/step_multivector.py</code> <pre><code>class EmbeddingMultiVectorStep(\n    EmbeddingStep,\n    TypedStep[\n        EmbeddingSettings,\n        list[MarkdownDataContract],\n        DataFrame[EmbeddingMultiVectorResult],\n    ],\n):\n    \"\"\"Step for consuming list[MarkdownDataContract]\n    and returning DataFrame[EmbeddingMultiVectorResult].\n    \"\"\"\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingMultiVectorResult]:\n        \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects,\n        generating embeddings for each document, and returning the results as a DataFrame.\n\n        Args:\n            inpt (list[MarkdownDataContract]): A list of markdown data contracts to process.\n\n        Returns:\n            DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results.\n\n        Raises:\n            StepFailed: If all input documents fail to generate embeddings.\n\n        Logs:\n            - Warnings for documents skipped due to EmbeddingAPIException.\n            - A summary warning if some or all documents are skipped.\n\n        \"\"\"\n\n        def process_document(doc):\n            try:\n                return self._get_embedding(doc)\n            except EmbeddingAPIException as err:\n                log.warning(\n                    f\"Skipped because EmbeddingAPIException: {err.message}\",\n                    extra={\"markdown\": str(doc)},\n                )\n                return None\n\n        results = Parallel(backend=\"threading\", n_jobs=self.settings.N_JOBS)(delayed(process_document)(doc) for doc in inpt)\n\n        rows = [res for res in results if res is not None]\n        failed = len(results) - len(rows)\n\n        if failed:\n            log.warning(f\"{failed}/{len(inpt)} got skipped\")\n        if failed == len(inpt):\n            raise StepFailed(f\"All {len(inpt)} embeddings got skipped\")\n\n        return DataFrame[EmbeddingMultiVectorResult](DataFrame[EmbeddingMultiVectorResult](rows))\n\n    def _get_embedding(self, doc: MarkdownDataContract) -&gt; _EmbeddedMultiVector:\n        \"\"\"Generates an embedding for a given text and context.\n\n        Parameters\n        ----------\n        d : dict\n            A dictionary containing the text and context for which to generate the embedding.\n\n        Returns:\n        -------\n        dict\n            A dictionary containing the original text, its embedding, and the source URL.\n\n        \"\"\"\n\n        def prepare_plain(document: MarkdownDataContract) -&gt; str:\n            plain_text = self.markdown.convert(document.md)\n            plain_text = self._replace_link(plain_text)\n            return plain_text\n\n        try:\n            splitted_md_rows = self._split_markdown([doc])\n        except SplittException as err:\n            raise EmbeddingAPIException(\"splitting failed\") from err\n        vectors = [self.embedding.embed_query(prepare_plain(split)) for split in splitted_md_rows]\n        if not vectors:\n            raise EmbeddingAPIException(\"Embedding failed for all splits\")\n\n        context = self.get_simple_context(doc.keywords)\n\n        return {\n            \"text\": doc.md,\n            \"vectors\": vectors,\n            \"url\": doc.url,\n            \"keywords\": context,\n            \"splits\": [split.md for split in splitted_md_rows],\n        }\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep-functions","title":"Functions","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep.run","title":"<code>run(inpt)</code>","text":"<p>Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>inpt</code> <code>list[MarkdownDataContract]</code> <p>A list of markdown data contracts to process.</p> required <p>Returns:</p> Type Description <code>DataFrame[EmbeddingMultiVectorResult]</code> <p>DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results.</p> <p>Raises:</p> Type Description <code>StepFailed</code> <p>If all input documents fail to generate embeddings.</p> Logs <ul> <li>Warnings for documents skipped due to EmbeddingAPIException.</li> <li>A summary warning if some or all documents are skipped.</li> </ul> Source code in <code>wurzel/steps/embedding/step_multivector.py</code> <pre><code>def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingMultiVectorResult]:\n    \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects,\n    generating embeddings for each document, and returning the results as a DataFrame.\n\n    Args:\n        inpt (list[MarkdownDataContract]): A list of markdown data contracts to process.\n\n    Returns:\n        DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results.\n\n    Raises:\n        StepFailed: If all input documents fail to generate embeddings.\n\n    Logs:\n        - Warnings for documents skipped due to EmbeddingAPIException.\n        - A summary warning if some or all documents are skipped.\n\n    \"\"\"\n\n    def process_document(doc):\n        try:\n            return self._get_embedding(doc)\n        except EmbeddingAPIException as err:\n            log.warning(\n                f\"Skipped because EmbeddingAPIException: {err.message}\",\n                extra={\"markdown\": str(doc)},\n            )\n            return None\n\n    results = Parallel(backend=\"threading\", n_jobs=self.settings.N_JOBS)(delayed(process_document)(doc) for doc in inpt)\n\n    rows = [res for res in results if res is not None]\n    failed = len(results) - len(rows)\n\n    if failed:\n        log.warning(f\"{failed}/{len(inpt)} got skipped\")\n    if failed == len(inpt):\n        raise StepFailed(f\"All {len(inpt)} embeddings got skipped\")\n\n    return DataFrame[EmbeddingMultiVectorResult](DataFrame[EmbeddingMultiVectorResult](rows))\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.settings","title":"<code>settings</code>","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.settings-classes","title":"Classes","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.settings.EmbeddingSettings","title":"<code>EmbeddingSettings</code>","text":"<p>               Bases: <code>SplitterSettings</code></p> <p>EmbeddingSettings is a configuration class for embedding-related settings.</p> <p>Attributes:</p> Name Type Description <code>API</code> <code>Url</code> <p>The API endpoint for embedding operations.</p> <code>NORMALIZE</code> <code>bool</code> <p>A flag indicating whether to normalize embeddings. Defaults to False.</p> <code>BATCH_SIZE</code> <code>int</code> <p>The batch size for processing embeddings. Must be greater than 0. Defaults to 100.</p> <code>TOKEN_COUNT_MIN</code> <code>int</code> <p>The minimum token count for processing. Must be greater than 0. Defaults to 1.</p> <code>TOKEN_COUNT_MAX</code> <code>int</code> <p>The maximum token count for processing. Must be greater than 1. Defaults to 256.</p> <code>TOKEN_COUNT_BUFFER</code> <code>int</code> <p>The buffer size for token count. Must be greater than 0. Defaults to 32.</p> <code>STEPWORDS_PATH</code> <code>Path</code> <p>The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\".</p> <code>N_JOBS</code> <code>int</code> <p>The number of parallel jobs to use. Must be greater than 0. Defaults to 1.</p> <code>PREFIX_MAP</code> <code>dict[Pattern, str]</code> <p>A mapping of regex patterns to string prefixes. This is validated and transformed using the <code>_wrap_validator_model_mapping</code> method.</p> <code>CLEAN_MD_BEFORE_EMBEDDING</code> <code>bool</code> <p>If true Markdown content is cleaned before sending to embedding model. Defaults to False.</p> <code>TOKENIZER_MODEL</code> <code>str</code> <p>Name of tokenizer model measuring token count (tiktoken, spacy, or huggingface). Defaults to \"gpt-3.5-turbo\".</p> <p>Methods:</p> Name Description <code>_wrap_validator_model_mapping</code> <p>dict[str, str], handler): A static method to wrap and validate the model mapping. It converts string regex keys in the input dictionary to compiled regex patterns and applies a handler function to the result.</p> Source code in <code>wurzel/steps/embedding/settings.py</code> <pre><code>class EmbeddingSettings(SplitterSettings):\n    \"\"\"EmbeddingSettings is a configuration class for embedding-related settings.\n\n    Attributes:\n        API (Url): The API endpoint for embedding operations.\n        NORMALIZE (bool): A flag indicating whether to normalize embeddings. Defaults to False.\n        BATCH_SIZE (int): The batch size for processing embeddings. Must be greater than 0. Defaults to 100.\n        TOKEN_COUNT_MIN (int): The minimum token count for processing. Must be greater than 0. Defaults to 1.\n        TOKEN_COUNT_MAX (int): The maximum token count for processing. Must be greater than 1. Defaults to 256.\n        TOKEN_COUNT_BUFFER (int): The buffer size for token count. Must be greater than 0. Defaults to 32.\n        STEPWORDS_PATH (Path): The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\".\n        N_JOBS (int): The number of parallel jobs to use. Must be greater than 0. Defaults to 1.\n        PREFIX_MAP (dict[re.Pattern, str]): A mapping of regex patterns to string prefixes.\n            This is validated and transformed using the `_wrap_validator_model_mapping` method.\n        CLEAN_MD_BEFORE_EMBEDDING (bool): If true Markdown content is cleaned before sending to embedding model. Defaults to False.\n        TOKENIZER_MODEL (str): Name of tokenizer model measuring token count (tiktoken, spacy, or huggingface). Defaults to \"gpt-3.5-turbo\".\n\n    Methods:\n        _wrap_validator_model_mapping(input_dict: dict[str, str], handler):\n            A static method to wrap and validate the model mapping. It converts string regex keys\n            in the input dictionary to compiled regex patterns and applies a handler function to the result.\n\n    \"\"\"\n\n    @staticmethod\n    def _wrap_validator_model_mapping(input_dict: dict[str, str], handler):\n        new_dict = {}\n        for regex, prefix in input_dict.items():\n            if isinstance(regex, str):\n                new_dict[re.compile(regex)] = prefix\n            else:\n                new_dict.update({regex: prefix})\n        return handler(new_dict)\n\n    API: Url\n    NORMALIZE: bool = False\n    BATCH_SIZE: int = Field(100, gt=0)\n    TOKEN_COUNT_MIN: int = Field(1, gt=0)\n    TOKEN_COUNT_MAX: int = Field(256, gt=1)\n    TOKEN_COUNT_BUFFER: int = Field(32, gt=0)\n    STEPWORDS_PATH: Path = Path(\"data/german_stopwords_full.txt\")\n    N_JOBS: int = Field(1, gt=0)\n    PREFIX_MAP: Annotated[dict[re.Pattern, str], WrapValidator(_wrap_validator_model_mapping)] = Field(\n        default={\"e5-\": \"query: \", \"DPR|dpr\": \"\"}\n    )\n    CLEAN_MD_BEFORE_EMBEDDING: bool = True\n    TOKENIZER_MODEL: str = Field(\"gpt-3.5-turbo\", description=\"The tokenizer model to use for splitting documents.\")\n</code></pre>"},{"location":"steps/manual_markdown/","title":"Manual Markdown","text":""},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown","title":"<code>manual_markdown</code>","text":""},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown-classes","title":"Classes","text":""},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown.ManualMarkdownSettings","title":"<code>ManualMarkdownSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Settings fro ManMdstep.</p> Source code in <code>wurzel/steps/manual_markdown.py</code> <pre><code>class ManualMarkdownSettings(Settings):\n    \"\"\"Settings fro ManMdstep.\"\"\"\n\n    FOLDER_PATH: Path\n</code></pre>"},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown.ManualMarkdownStep","title":"<code>ManualMarkdownStep</code>","text":"<p>               Bases: <code>TypedStep[ManualMarkdownSettings, None, list[MarkdownDataContract]]</code></p> <p>Data Source for md files from a configurable path.</p> Source code in <code>wurzel/steps/manual_markdown.py</code> <pre><code>class ManualMarkdownStep(TypedStep[ManualMarkdownSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Data Source for md files from a configurable path.\"\"\"\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        return [\n            MarkdownDataContract.from_file(fp, url_prefix=self.__class__.__name__ + \"/\") for fp in self.settings.FOLDER_PATH.rglob(\"*.md\")\n        ]\n</code></pre>"},{"location":"steps/milvus/","title":"Milvus","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.step","title":"<code>step</code>","text":"<p>containing the DVCStep sending embedding data into milvus.</p>"},{"location":"steps/milvus/#wurzel.steps.milvus.step-classes","title":"Classes","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.step.MilvusConnectorStep","title":"<code>MilvusConnectorStep</code>","text":"<p>               Bases: <code>TypedStep[MilvusSettings, DataFrame[EmbeddingResult], Result]</code></p> <p>Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.</p> Source code in <code>wurzel/steps/milvus/step.py</code> <pre><code>class MilvusConnectorStep(TypedStep[MilvusSettings, DataFrame[EmbeddingResult], MilvusResult]):  # pragma: no cover\n    \"\"\"Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\"\n\n    milvus_timeout: float = 20.0\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        # milvus stuff passed as environment\n        # because we need to enject them into the DVC step during runtime,\n        # not during DVC pipeline definition time\n        uri = f\"http://{self.settings.HOST}:{self.settings.PORT}\"\n        if not self.settings.PASSWORD or not self.settings.USER:\n            log.warning(\"MILVUS_HOST, MILVUS_USER or MILVUS_PASSWORD for Milvus not provided. Thus running in non-credential Mode\")\n        self.client: MilvusClient = MilvusClient(\n            uri=uri,\n            user=self.settings.USER,\n            password=self.settings.PASSWORD.get_secret_value(),\n            timeout=self.milvus_timeout,\n        )\n        self.collection_index: IndexParams = IndexParams(**self.settings.INDEX_PARAMS)\n        self.collection_history_len = self.settings.COLLECTION_HISTORY_LEN\n\n        self.collection_prefix = self.settings.COLLECTION\n\n    def __del__(self):\n        if getattr(self, \"client\", None):\n            self.client.close()\n\n    def run(self, inpt: DataFrame[EmbeddingResult]) -&gt; MilvusResult:\n        self._insert_embeddings(inpt)\n        try:\n            old = self.__construct_last_collection_name()\n        except NoPreviousCollection:\n            old = \"\"\n        self._retire_collection()\n        return MilvusResult(new=self.__construct_current_collection_name(), old=old)\n\n    def _insert_embeddings(self, data: pd.DataFrame):\n        collection_name = self.__construct_next_collection_name()\n        log.info(f\"Creating milvus collection {collection_name}\")\n        collection_schema = CollectionSchema(\n            fields=[\n                FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=3000),\n                FieldSchema(\n                    name=\"vector\",\n                    dtype=DataType.FLOAT_VECTOR,\n                    dim=len(data[\"vector\"].loc[0]),\n                ),\n                FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=300),\n            ],\n            description=\"Collection for storing Milvus embeddings\",\n        )\n\n        log.info(\"schema created\")\n        self.client.create_collection(collection_name=collection_name, schema=collection_schema)\n        log.info(\"collection created\")\n        log.info(f\"Inserting embedding {len(data)} into collection {collection_name}\")\n        result: dict = self.client.insert(collection_name=collection_name, data=data.to_dict(\"records\"))\n        if result[\"insert_count\"] != len(data):\n            raise StepFailed(\n                f\"Failed to insert df into collection '{collection_name}'.{result['insert_count']}/{len(data)} where successful\"\n            )\n        log.info(f\"Successfully inserted {len(data)} vectors into collection '{collection_name}'\")\n        self.client.create_index(collection_name=collection_name, index_params=self.collection_index)\n        log.info(f\"Successfully craeted index {self.collection_index} into collection '{collection_name}\")\n        self.client.load_collection(collection_name)\n        log.info(f\"Successfully loaded the collection {collection_name}' into collection '{collection_name}'\")\n        try:\n            self.client.release_collection(self.__construct_last_collection_name())\n        except NoPreviousCollection:\n            pass\n        self._update_alias(collection_name)\n\n    def _retire_collection(self):\n        collections_versioned: dict[int, str] = self._get_collection_versions()\n        to_delete = sorted(collections_versioned.keys())[: -self.collection_history_len]\n        if not to_delete:\n            return\n\n        for col_v in to_delete:\n            col = collections_versioned[col_v]\n            log.info(f\"deleting {col} collection caused by retirement\")\n            self.client.drop_collection(col, timeout=self.milvus_timeout)\n\n    def _update_alias(self, collection_name):\n        try:\n            self.client.create_alias(\n                collection_name=collection_name,\n                alias=self.collection_prefix,\n                timeout=self.milvus_timeout,\n            )\n        except MilvusException:\n            self.client.alter_alias(\n                collection_name=collection_name,\n                alias=self.collection_prefix,\n                timeout=self.milvus_timeout,\n            )\n\n    def __construct_next_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections:\n            return f\"{self.collection_prefix}_v1\"\n        previous_version = max(previous_collections.keys())\n        log.info(f\"Found version v{previous_version}\")\n        return f\"{self.collection_prefix}_v{previous_version + 1}\"\n\n    def __construct_last_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections or len(previous_collections) &lt;= 1:\n            raise NoPreviousCollection(f\"Milvus does not contain a previous collection for {self.collection_prefix}\")\n        previous_version = sorted(previous_collections.keys())[-2]\n        log.info(f\"Found previous version v{previous_version}\")\n        return f\"{self.collection_prefix}_v{previous_version}\"\n\n    def __construct_current_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections or len(previous_collections) &lt; 1:\n            raise NoPreviousCollection(f\"Milvus does not contain a previous collection for {self.collection_prefix}\")\n        previous_version = sorted(previous_collections.keys())[-1]\n        log.info(f\"Found previous version v{previous_version}\")\n        return f\"{self.collection_prefix}_v{previous_version}\"\n\n    def _get_collection_versions(self) -&gt; dict[int, str]:\n        previous_collections = self.client.list_collections(timeout=self.milvus_timeout)\n        versioned_collections = {\n            int(previous.split(\"_v\")[-1]): previous for previous in previous_collections if self.collection_prefix in previous\n        }\n        return versioned_collections\n</code></pre>"},{"location":"steps/milvus/#wurzel.steps.milvus.settings","title":"<code>settings</code>","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.settings-classes","title":"Classes","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings","title":"<code>MilvusSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>MilvusSettings is a configuration class for managing settings related to MilvusDB.</p> <p>Attributes:</p> Name Type Description <code>HOST</code> <code>str</code> <p>The hostname or IP address of the Milvus server. Defaults to \"localhost\".</p> <code>PORT</code> <code>int</code> <p>The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530.</p> <code>COLLECTION</code> <code>str</code> <p>The name of the collection in MilvusDB.</p> <code>COLLECTION_HISTORY_LEN</code> <code>int</code> <p>The length of the collection history. Defaults to 10.</p> <code>SEARCH_PARAMS</code> <code>dict</code> <p>Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}.</p> <code>INDEX_PARAMS</code> <code>dict</code> <p>Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\",                     \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}.</p> <code>USER</code> <code>str</code> <p>The username for authentication with MilvusDB.</p> <code>PASSWORD</code> <code>SecretStr</code> <p>The password for authentication with MilvusDB.</p> <code>SECURED</code> <code>bool</code> <p>Indicates whether the connection to MilvusDB is secured. Defaults to False.</p> <p>Methods:</p> Name Description <code>parse_json</code> <p>Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.</p> Source code in <code>wurzel/steps/milvus/settings.py</code> <pre><code>class MilvusSettings(Settings):\n    \"\"\"MilvusSettings is a configuration class for managing settings related to MilvusDB.\n\n    Attributes:\n        HOST (str): The hostname or IP address of the Milvus server. Defaults to \"localhost\".\n        PORT (int): The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530.\n        COLLECTION (str): The name of the collection in MilvusDB.\n        COLLECTION_HISTORY_LEN (int): The length of the collection history. Defaults to 10.\n        SEARCH_PARAMS (dict): Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}.\n        INDEX_PARAMS (dict): Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\",\n                                \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}.\n        USER (str): The username for authentication with MilvusDB.\n        PASSWORD (SecretStr): The password for authentication with MilvusDB.\n        SECURED (bool): Indicates whether the connection to MilvusDB is secured. Defaults to False.\n\n    Methods:\n        parse_json(cls, v): Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.\n\n    \"\"\"\n\n    HOST: str = \"localhost\"\n    PORT: int = Field(19530, gt=0, le=65535)\n    COLLECTION: str\n    COLLECTION_HISTORY_LEN: int = 10\n    SEARCH_PARAMS: dict = {\"metric_type\": \"IP\", \"params\": {}}\n    INDEX_PARAMS: dict = {\n        \"index_type\": \"FLAT\",\n        \"field_name\": \"vector\",\n        \"metric_type\": \"IP\",\n        \"params\": {},\n    }\n    USER: str\n    PASSWORD: SecretStr\n    SECURED: bool = False\n\n    @field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n    @classmethod\n    # pylint: disable-next=R0801\n    def parse_json(cls, v):\n        \"\"\"Validation for json.\"\"\"\n        if isinstance(v, str):\n            return json.loads(v)\n        return v\n</code></pre>"},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings-functions","title":"Functions","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings.parse_json","title":"<code>parse_json(v)</code>  <code>classmethod</code>","text":"<p>Validation for json.</p> Source code in <code>wurzel/steps/milvus/settings.py</code> <pre><code>@field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n@classmethod\n# pylint: disable-next=R0801\ndef parse_json(cls, v):\n    \"\"\"Validation for json.\"\"\"\n    if isinstance(v, str):\n        return json.loads(v)\n    return v\n</code></pre>"},{"location":"steps/qdrant/","title":"Qdrant","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step","title":"<code>step</code>","text":"<p>containing the DVCStep sending embedding data into Qdrant.</p>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step-classes","title":"Classes","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep","title":"<code>QdrantConnectorStep</code>","text":"<p>               Bases: <code>TypedStep[QdrantSettings, DataFrame[EmbeddingResult], DataFrame[QdrantResult]]</code></p> <p>Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.</p> Source code in <code>wurzel/steps/qdrant/step.py</code> <pre><code>class QdrantConnectorStep(TypedStep[QdrantSettings, DataFrame[EmbeddingResult], DataFrame[QdrantResult]]):\n    \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\"\n\n    _timeout: int = 20\n    s: QdrantSettings\n    client: QdrantClient\n    collection_name: str\n    result_class = QdrantResult\n    vector_key = \"vector\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        # Qdrant stuff passed as environment\n        # because we need to enject them into the DVC step during runtime,\n        # not during DVC pipeline definition time\n        # uri = \":memory:\"\n        log.info(f\"connecting to {self.settings.URI}\")\n        if not self.settings.APIKEY:\n            log.warning(\"QDRANT__APIKEY for Qdrant not provided. Thus running in non-credential Mode\")\n        self.client = QdrantClient(\n            location=self.settings.URI,\n            api_key=self.settings.APIKEY.get_secret_value(),\n            timeout=self._timeout,\n        )\n        self.collection_name = self.__construct_next_collection_name()\n        self.id_iter = self.__id_gen()\n\n    def __del__(self):\n        if getattr(self, \"client\", None):\n            self.client.close()\n\n    def finalize(self) -&gt; None:\n        self._create_indices()\n        self._update_alias()\n        self._retire_collections()\n        return super().finalize()\n\n    def __id_gen(self):\n        i = 0\n        while True:\n            i += 1\n            yield i\n\n    def run(self, inpt: DataFrame[EmbeddingResult]) -&gt; DataFrame[QdrantResult]:\n        if not self.client.collection_exists(self.collection_name):\n            self._create_collection(len(inpt[\"vector\"].loc[0]))\n        df_result = self._insert_embeddings(inpt)\n        return df_result\n\n    def _create_collection(self, size: int):\n        log.debug(f\"Creating Qdrant collection {self.collection_name}\")\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(size=size, distance=self.settings.DISTANCE),\n            replication_factor=self.settings.REPLICATION_FACTOR,\n        )\n\n    def _get_entry_payload(self, row: dict[str, object]) -&gt; dict[str, object]:\n        \"\"\"Create the payload for the entry.\"\"\"\n        payload = {\n            \"url\": row[\"url\"],\n            \"text\": row[\"text\"],\n            **self.get_available_hashes(row[\"text\"]),\n            \"keywords\": row[\"keywords\"],\n            \"history\": str(step_history.get()),\n            \"metadata\": row.get(\"metadata\", None),\n        }\n\n        # Add to payload if `embedding_input_text` is used\n        # (due to difference between `EmbeddingMultiVectorStep` and `EmbeddingStep`)\n        if \"embedding_input_text\" in row:\n            payload[\"embedding_input_text\"] = row[\"embedding_input_text\"]\n\n        return payload\n\n    def _create_point(self, row: dict) -&gt; models.PointStruct:\n        \"\"\"Creates a Qdrant PointStruct object from a given row dictionary.\n\n        Args:\n            row (dict): A dictionary representing a data entry, expected to contain at least the vector data under `self.vector_key`.\n\n        Returns:\n            models.PointStruct: An instance of PointStruct with a unique id, vector, and payload extracted from the row.\n\n        Raises:\n            KeyError: If the required vector key is not present in the row.\n\n        \"\"\"\n        payload = self._get_entry_payload(row)\n\n        return models.PointStruct(\n            id=next(self.id_iter),  # type: ignore[arg-type]\n            vector=row[self.vector_key],\n            payload=payload,\n        )\n\n    def _upsert_points(self, points: list[models.PointStruct]):\n        \"\"\"Inserts a list of points into the Qdrant collection in batches.\n\n        Args:\n            points (list[models.PointStruct]): The list of point structures to upsert into the collection.\n\n        Raises:\n            StepFailed: If any batch fails to be inserted into the collection.\n\n        Logs:\n            Logs a message for each successfully inserted batch, including the collection name and number of points.\n\n        \"\"\"\n        for point_chunk in _batch(points, self.settings.BATCH_SIZE):\n            operation_info = self.client.upsert(\n                collection_name=self.collection_name,\n                wait=True,\n                points=point_chunk,\n            )\n            if operation_info.status != \"completed\":\n                raise StepFailed(f\"Failed to insert df chunk into collection '{self.collection_name}' {operation_info}\")\n            log.info(\n                \"Successfully inserted vector_chunk\",\n                extra={\"collection\": self.collection_name, \"count\": len(point_chunk)},\n            )\n\n    def _build_result_dataframe(self, points: list[models.PointStruct]):\n        \"\"\"Constructs a DataFrame from a list of PointStruct objects.\n\n        Each PointStruct's payload is unpacked into the resulting dictionary, along with its vector, collection name, and ID.\n        The resulting list of dictionaries is used to create a DataFrame of the specified result_class.\n\n        Args:\n            points (list[models.PointStruct]): A list of PointStruct objects containing payload, vector, and id information.\n\n        \"\"\"\n        result_data = [\n            {\n                **entry.payload,\n                self.vector_key: entry.vector,\n                \"collection\": self.collection_name,\n                \"id\": entry.id,\n            }\n            for entry in points\n        ]\n        return DataFrame[self.result_class](result_data)\n\n    def _insert_embeddings(self, data: DataFrame[EmbeddingResult]):\n        log.info(\"Inserting embeddings\", extra={\"count\": len(data), \"collection\": self.collection_name})\n\n        points = [self._create_point(row) for _, row in data.iterrows()]\n\n        self._upsert_points(points)\n\n        return self._build_result_dataframe(points)\n\n    def _create_indices(self):\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"keywords\",\n            field_schema=models.TextIndexParams(\n                type=models.TextIndexType.TEXT,\n                tokenizer=models.TokenizerType.WHITESPACE,\n            ),\n        )\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"url\",\n            field_schema=models.TextIndexParams(\n                type=models.TextIndexType.TEXT,\n                tokenizer=models.TokenizerType.PREFIX,\n                min_token_len=3,\n            ),\n        )\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"text\",\n            field_schema=models.TextIndexParams(\n                type=models.TextIndexType.TEXT,\n                tokenizer=models.TokenizerType.MULTILINGUAL,\n            ),\n        )\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"history\",\n            field_schema=models.TextIndexParams(type=models.TextIndexType.TEXT, tokenizer=models.TokenizerType.WORD),\n        )\n\n    def _retire_collections(self):\n        collections_versioned: dict[int, str] = self._get_collection_versions()\n        to_delete = list(collections_versioned.keys())[: -self.settings.COLLECTION_HISTORY_LEN]\n        if not to_delete:\n            return\n\n        for col_v in to_delete:\n            col = collections_versioned[col_v]\n            log.info(f\"deleting {col} collection caused by retirement\")\n            self.client.delete_collection(col)\n\n    def _update_alias(self):\n        success = self.client.update_collection_aliases(\n            change_aliases_operations=[\n                models.CreateAliasOperation(\n                    create_alias=models.CreateAlias(\n                        collection_name=self.collection_name,\n                        alias_name=self.settings.COLLECTION,\n                    )\n                )\n            ]\n        )\n        if not success:\n            raise CustomQdrantException(\"Alias Update failed\")\n\n    def __construct_next_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections:\n            return f\"{self.settings.COLLECTION}_v1\"\n        previous_version = max(previous_collections.keys())\n        log.info(f\"Found version v{previous_version}\")\n        return f\"{self.settings.COLLECTION}_v{previous_version + 1}\"\n\n    def _get_collection_versions(self) -&gt; dict[int, str]:\n        previous_collections = self.client.get_collections().collections\n        versioned_collections = {\n            int(previous.name.split(\"_v\")[-1]): previous.name\n            for previous in previous_collections\n            if f\"{self.settings.COLLECTION}_v\" in previous.name\n        }\n        return dict(sorted(versioned_collections.items()))\n\n    @staticmethod\n    def get_available_hashes(text: str, encoding: str = \"utf-8\") -&gt; dict:\n        \"\"\"Compute `n` hashes for a given input text based.\n        The number `n` depends on the optionally installed python libs.\n        For now only TLSH (Trend Micro Locality Sensitive Hash) is supported\n        ## TLSH\n        Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons.\n\n        Args:\n            text (str): Input text\n            encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\".\n\n        Returns:\n            dict[str, str]: keys: `text_&lt;algo&gt;_hash` hash as string ! Dict might be empty!\n\n        \"\"\"\n        hashes = {}\n        encoded_text = text.encode(encoding)\n        if HAS_TLSH:\n            # pylint: disable=no-name-in-module, import-outside-toplevel\n            from tlsh import hash as tlsh_hash\n\n            hashes[\"text_tlsh_hash\"] = tlsh_hash(encoded_text)\n        hashes[\"text_sha256_hash\"] = sha256(encoded_text).hexdigest()\n        return hashes\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep-functions","title":"Functions","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep.get_available_hashes","title":"<code>get_available_hashes(text, encoding='utf-8')</code>  <code>staticmethod</code>","text":"<p>Compute <code>n</code> hashes for a given input text based. The number <code>n</code> depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported</p>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep.get_available_hashes--tlsh","title":"TLSH","text":"<p>Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <code>encoding</code> <code>str</code> <p>Input text will encoded to bytes using this encoding. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, str]: keys: <code>text_&lt;algo&gt;_hash</code> hash as string ! Dict might be empty!</p> Source code in <code>wurzel/steps/qdrant/step.py</code> <pre><code>@staticmethod\ndef get_available_hashes(text: str, encoding: str = \"utf-8\") -&gt; dict:\n    \"\"\"Compute `n` hashes for a given input text based.\n    The number `n` depends on the optionally installed python libs.\n    For now only TLSH (Trend Micro Locality Sensitive Hash) is supported\n    ## TLSH\n    Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons.\n\n    Args:\n        text (str): Input text\n        encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\".\n\n    Returns:\n        dict[str, str]: keys: `text_&lt;algo&gt;_hash` hash as string ! Dict might be empty!\n\n    \"\"\"\n    hashes = {}\n    encoded_text = text.encode(encoding)\n    if HAS_TLSH:\n        # pylint: disable=no-name-in-module, import-outside-toplevel\n        from tlsh import hash as tlsh_hash\n\n        hashes[\"text_tlsh_hash\"] = tlsh_hash(encoded_text)\n    hashes[\"text_sha256_hash\"] = sha256(encoded_text).hexdigest()\n    return hashes\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector","title":"<code>step_multi_vector</code>","text":"<p>containing the DVCStep sending embedding data into Qdrant.</p>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector-classes","title":"Classes","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector.QdrantConnectorMultiVectorStep","title":"<code>QdrantConnectorMultiVectorStep</code>","text":"<p>               Bases: <code>QdrantConnectorStep</code>, <code>TypedStep[QdrantSettings, DataFrame[EmbeddingMultiVectorResult], DataFrame[QdrantMultiVectorResult]]</code></p> <p>Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.</p> Source code in <code>wurzel/steps/qdrant/step_multi_vector.py</code> <pre><code>class QdrantConnectorMultiVectorStep(\n    QdrantConnectorStep,\n    TypedStep[\n        QdrantSettings,\n        DataFrame[EmbeddingMultiVectorResult],\n        DataFrame[QdrantMultiVectorResult],\n    ],\n):\n    \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\"\n\n    vector_key = \"vectors\"\n    result_class = QdrantMultiVectorResult\n\n    def _create_collection(self, size: int):\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(\n                size=size,\n                distance=self.settings.DISTANCE,\n                multivector_config=models.MultiVectorConfig(comparator=models.MultiVectorComparator.MAX_SIM),\n            ),\n            replication_factor=self.settings.REPLICATION_FACTOR,\n        )\n\n    def run(self, inpt: DataFrame[EmbeddingMultiVectorResult]) -&gt; DataFrame[QdrantMultiVectorResult]:\n        log.debug(f\"Creating Qdrant collection {self.collection_name}\")\n        if not self.client.collection_exists(self.collection_name):\n            self._create_collection(len(inpt[\"vectors\"].loc[0][0]))\n        df_result = self._insert_embeddings(inpt)\n        return df_result\n\n    def _get_entry_payload(self, row: dict[str, object]) -&gt; dict[str, object]:\n        \"\"\"Create the payload for the entry.\"\"\"\n        payload = super()._get_entry_payload(row)\n        payload[\"splits\"] = row[\"splits\"]\n        return payload\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings","title":"<code>settings</code>","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings-classes","title":"Classes","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings","title":"<code>QdrantSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>QdrantSettings is a configuration class for managing settings related to the Qdrant database.</p> <p>Attributes:</p> Name Type Description <code>DISTANCE</code> <code>Distance</code> <p>The distance metric to be used, default is Distance.DOT.</p> <code>URI</code> <code>str</code> <p>The URI for the Qdrant database, default is \"http://localhost:6333\".</p> <code>COLLECTION</code> <code>str</code> <p>The name of the collection in the Qdrant database.</p> <code>COLLECTION_HISTORY_LEN</code> <code>int</code> <p>The length of the collection history, default is 10.</p> <code>SEARCH_PARAMS</code> <code>dict</code> <p>Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}.</p> <code>INDEX_PARAMS</code> <code>dict</code> <p>Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\".</p> <code>APIKEY</code> <code>SecretStr</code> <p>The API key for authentication, default is an empty SecretStr.</p> <code>REPLICATION_FACTOR</code> <code>int</code> <p>The replication factor for the database, default is 3, must be greater than 0.</p> <code>BATCH_SIZE</code> <code>int</code> <p>The batch size for operations, default is 1024, must be greater than 0.</p> <p>Methods:</p> Name Description <code>parse_json</code> <p>Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.</p> Source code in <code>wurzel/steps/qdrant/settings.py</code> <pre><code>class QdrantSettings(Settings):\n    \"\"\"QdrantSettings is a configuration class for managing settings related to the Qdrant database.\n\n    Attributes:\n        DISTANCE (Distance): The distance metric to be used, default is Distance.DOT.\n        URI (str): The URI for the Qdrant database, default is \"http://localhost:6333\".\n        COLLECTION (str): The name of the collection in the Qdrant database.\n        COLLECTION_HISTORY_LEN (int): The length of the collection history, default is 10.\n        SEARCH_PARAMS (dict): Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}.\n        INDEX_PARAMS (dict): Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\".\n        APIKEY (SecretStr): The API key for authentication, default is an empty SecretStr.\n        REPLICATION_FACTOR (int): The replication factor for the database, default is 3, must be greater than 0.\n        BATCH_SIZE (int): The batch size for operations, default is 1024, must be greater than 0.\n\n    Methods:\n        parse_json(v):\n            Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.\n    \"\"\"\n\n    DISTANCE: Distance = Distance.DOT\n    URI: str = \"http://localhost:6333\"\n    COLLECTION: str\n    COLLECTION_HISTORY_LEN: int = 10\n    SEARCH_PARAMS: dict = {\"metric_type\": \"IP\", \"params\": {}}\n    INDEX_PARAMS: dict = {\n        \"index_type\": \"FLAT\",\n        \"field_name\": \"vector\",\n        \"distance\": \"Dot\",\n        \"params\": {},\n    }\n    APIKEY: SecretStr = SecretStr(\"\")\n    REPLICATION_FACTOR: int = Field(default=3, gt=0)\n    BATCH_SIZE: int = Field(default=1024, gt=0)\n\n    @field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n    @classmethod\n    def parse_json(cls, v):\n        \"\"\"Validation for json.\"\"\"\n        if isinstance(v, str):\n            return json.loads(v)\n        return v\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings-functions","title":"Functions","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings.parse_json","title":"<code>parse_json(v)</code>  <code>classmethod</code>","text":"<p>Validation for json.</p> Source code in <code>wurzel/steps/qdrant/settings.py</code> <pre><code>@field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n@classmethod\ndef parse_json(cls, v):\n    \"\"\"Validation for json.\"\"\"\n    if isinstance(v, str):\n        return json.loads(v)\n    return v\n</code></pre>"},{"location":"steps/scraperapi/","title":"ScraperAPI","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step","title":"<code>step</code>","text":"<p>interacts with the scraperAPI service and converts the retrieved Documents to Markdown.</p>"},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step-classes","title":"Classes","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step.ScraperAPIStep","title":"<code>ScraperAPIStep</code>","text":"<p>               Bases: <code>TypedStep[ScraperAPISettings, list[UrlItem], list[MarkdownDataContract]]</code></p> <p>ScraperAPIStep uses the ScraperAPI service to srape the html by the given url through list[UrlItem]. this html gets filtered and transformed to MarkdownDataContract.</p> Source code in <code>wurzel/steps/scraperapi/step.py</code> <pre><code>class ScraperAPIStep(TypedStep[ScraperAPISettings, list[UrlItem], list[MarkdownDataContract]]):\n    \"\"\"ScraperAPIStep uses the ScraperAPI service to srape the html by the given url through list[UrlItem].\n    this html gets filtered and transformed to MarkdownDataContract.\n    \"\"\"\n\n    def run(self, inpt: list[UrlItem]) -&gt; list[MarkdownDataContract]:\n        def fetch_and_process(url_item: UrlItem, recursion_depth=0):\n            session = requests.Session()\n            retries = Retry(\n                total=self.settings.RETRY, backoff_factor=0.1, raise_on_status=False, status_forcelist=[403, 500, 502, 503, 504]\n            )\n            session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n            payload = {\n                \"api_key\": self.settings.TOKEN.get_secret_value(),\n                \"url\": url_item.url,\n                \"device_type\": self.settings.DEVICE_TYPE,\n                \"follow_redirect\": str(self.settings.FOLLOW_REDIRECT).lower(),\n                \"wait_for_selector\": self.settings.WAIT_FOR_SELECTOR,\n                \"country_code\": self.settings.COUNTRY_CODE,\n                \"render\": str(self.settings.RENDER).lower(),\n                \"premium\": str(self.settings.PREMIUM).lower(),\n                \"ultra_premium\": str(self.settings.ULTRA_PREMIUM).lower(),\n                \"screenshot\": str(self.settings.SCREENSHOT).lower(),\n                \"max_cost\": str(self.settings.MAX_COST),\n            }\n            try:\n                r = None  # for short error handling\n                r = session.get(self.settings.API, params=payload, timeout=self.settings.TIMEOUT)\n                r.raise_for_status()\n            except requests.exceptions.ReadTimeout:\n                log.warning(\n                    \"Crawling failed due to timeout\",\n                    extra={\"url\": url_item.url},\n                )\n                return None\n            except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError):\n                log.warning(\n                    \"Crawling failed\",\n                    extra={\"url\": url_item.url, \"status\": r.status_code if r else None, \"retries\": self.settings.RETRY},\n                )\n                return None\n\n            try:\n                md = to_markdown(self._filter_body(r.text), self.settings.HTML2MD_SETTINGS)\n            except (KeyError, IndexError):\n                if recursion_depth &gt; self.settings.RETRY:\n                    log.warning(\"xpath retry failed\", extra={\"filter\": self.settings.XPATH, \"url\": url_item.url})\n                    return None\n                log.warning(\n                    \"website does not have the searched xpath, retrying\", extra={\"filter\": self.settings.XPATH, \"url\": url_item.url}\n                )\n                return fetch_and_process(url_item, recursion_depth=recursion_depth + 1)\n\n            progress_bar.update(1)\n            return MarkdownDataContract(md=md, url=url_item.url, keywords=url_item.title)\n\n        with tqdm(total=len(inpt), desc=\"Processing URLs\") as progress_bar:\n            results = Parallel(n_jobs=self.settings.CONCURRENCY_NUM, backend=\"threading\")(delayed(fetch_and_process)(item) for item in inpt)\n\n        filtered_results = [res for res in results if res]\n        if not filtered_results:\n            raise StepFailed(\"no results from scraperAPI\")\n\n        return filtered_results\n\n    def __init__(self) -&gt; None:\n        logging.getLogger(\"urllib3\").setLevel(\"ERROR\")\n        super().__init__()\n\n    def finalize(self) -&gt; None:\n        logging.getLogger(\"urllib3\").setLevel(\"WARNING\")\n\n        return super().finalize()\n\n    def _filter_body(self, html: str) -&gt; str:\n        tree: lxml.html = lxml.html.fromstring(html)\n        tree = tree.xpath(self.settings.XPATH)[0]\n        return html2str(tree)\n</code></pre>"},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step-functions","title":"Functions","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.settings","title":"<code>settings</code>","text":"<p>interacts with the scraperAPI service and converts the retrieved Documents to Markdown.</p>"},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.settings-classes","title":"Classes","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.settings.ScraperAPISettings","title":"<code>ScraperAPISettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Settings of ScraperAPIStep. Mainly the list of https://docs.scraperapi.com/python/credits-and-requests.</p> Source code in <code>wurzel/steps/scraperapi/settings.py</code> <pre><code>class ScraperAPISettings(Settings):\n    \"\"\"Settings of ScraperAPIStep. Mainly the list of https://docs.scraperapi.com/python/credits-and-requests.\"\"\"\n\n    API: str = \"https://api.scraperapi.com/\"\n    RETRY: int = Field(ge=0, default=5)\n    TOKEN: SecretStr\n    TIMEOUT: int = 61.0\n    XPATH: str = \"//main\"\n    CONCURRENCY_NUM: int = Field(gt=0, default=1)\n    DEVICE_TYPE: str = \"desktop\"\n    FOLLOW_REDIRECT: bool = True\n    WAIT_FOR_SELECTOR: str = \"#cookies-notification-accept-cookie\"\n    COUNTRY_CODE: str = \"en\"\n    RENDER: bool = True\n    PREMIUM: bool = False\n    ULTRA_PREMIUM: bool = False\n    SCREENSHOT: bool = False\n    MAX_COST: int = Field(gt=0, default=30)\n    HTML2MD_SETTINGS: MarkdownConverterSettings = Field(\n        default_factory=MarkdownConverterSettings, description=\"Settings for the Markdown converter.\"\n    )\n</code></pre>"},{"location":"steps/sftp/","title":"SFTP Manual Markdown","text":""},{"location":"steps/sftp/#wurzel.steps.sftp","title":"<code>sftp</code>","text":""},{"location":"steps/sftp/#wurzel.steps.sftp-classes","title":"Classes","text":""},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownSettings","title":"<code>SFTPManualMarkdownSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Settings for SFTP Manual Markdown Step.</p> <p>These settings configure the SFTP connection and file retrieval parameters. All settings can be provided via environment variables with the prefix SFTPMANUALMARKDOWNSTEP__ (e.g., SFTPMANUALMARKDOWNSTEP__HOST=sftp.example.com)</p> Source code in <code>wurzel/steps/sftp/sftp_manual_markdown.py</code> <pre><code>class SFTPManualMarkdownSettings(Settings):\n    \"\"\"Settings for SFTP Manual Markdown Step.\n\n    These settings configure the SFTP connection and file retrieval parameters.\n    All settings can be provided via environment variables with the prefix\n    SFTPMANUALMARKDOWNSTEP__ (e.g., SFTPMANUALMARKDOWNSTEP__HOST=sftp.example.com)\n    \"\"\"\n\n    HOST: str = Field(..., description=\"SFTP server hostname or IP address\")\n    PORT: int = Field(22, description=\"SFTP server port\")\n    USERNAME: str = Field(..., description=\"SFTP username\")\n    PASSWORD: SecretStr = SecretStr(\"\")\n    PRIVATE_KEY_PATH: Optional[Path] = Field(None, description=\"Path to SSH private key file\")\n    PRIVATE_KEY_PASSPHRASE: SecretStr = SecretStr(\"\")\n    REMOTE_PATH: str = Field(..., description=\"Remote path on SFTP server to search for .md files\")\n    RECURSIVE: bool = Field(True, description=\"Whether to search recursively for .md files\")\n    TIMEOUT: float = Field(30.0, description=\"Connection timeout in seconds\")\n</code></pre>"},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownStep","title":"<code>SFTPManualMarkdownStep</code>","text":"<p>               Bases: <code>TypedStep[SFTPManualMarkdownSettings, None, list[MarkdownDataContract]]</code></p> <p>Data Source for Markdown files from an SFTP server.</p> <p>This step connects to an SFTP server using Paramiko and retrieves all Markdown (.md) files from the specified remote path. It works similarly to ManualMarkdownStep but loads files from a remote SFTP server instead of the local filesystem.</p> <p>Features: - Supports password and key-based authentication - Recursive directory traversal - Automatic connection management - Preserves file metadata</p> <p>Example usage: <pre><code>from wurzel.steps.sftp import SFTPManualMarkdownStep, SFTPManualMarkdownSettings\n\nsettings = SFTPManualMarkdownSettings(HOST=\"sftp.example.com\", USERNAME=\"user\", PASSWORD=\"password\", REMOTE_PATH=\"/documents\")\nstep = SFTPManualMarkdownStep(settings=settings)\nmarkdown_docs = step.run(None)\n</code></pre></p> Source code in <code>wurzel/steps/sftp/sftp_manual_markdown.py</code> <pre><code>class SFTPManualMarkdownStep(TypedStep[SFTPManualMarkdownSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Data Source for Markdown files from an SFTP server.\n\n    This step connects to an SFTP server using Paramiko and retrieves all Markdown (.md) files\n    from the specified remote path. It works similarly to ManualMarkdownStep but loads files\n    from a remote SFTP server instead of the local filesystem.\n\n    Features:\n    - Supports password and key-based authentication\n    - Recursive directory traversal\n    - Automatic connection management\n    - Preserves file metadata\n\n    Example usage:\n    ```python\n    from wurzel.steps.sftp import SFTPManualMarkdownStep, SFTPManualMarkdownSettings\n\n    settings = SFTPManualMarkdownSettings(HOST=\"sftp.example.com\", USERNAME=\"user\", PASSWORD=\"password\", REMOTE_PATH=\"/documents\")\n    step = SFTPManualMarkdownStep(settings=settings)\n    markdown_docs = step.run(None)\n    ```\n    \"\"\"\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        \"\"\"Execute the step to retrieve Markdown files from SFTP server.\n\n        Args:\n            inpt: None (this is a leaf step)\n\n        Returns:\n            list[MarkdownDataContract]: List of loaded Markdown documents\n\n        Raises:\n            paramiko.SSHException: If connection or authentication fails\n            IOError: If file operations fail\n        \"\"\"\n        logger.info(\n            f\"Connecting to SFTP server {self.settings.HOST}:{self.settings.PORT}\",\n            extra={\"host\": self.settings.HOST, \"port\": self.settings.PORT, \"remote_path\": self.settings.REMOTE_PATH},\n        )\n\n        # Establish SFTP connection\n        transport = None\n        sftp = None\n        try:\n            # Create SSH transport\n            transport = paramiko.Transport((self.settings.HOST, self.settings.PORT))\n            transport.connect(\n                username=self.settings.USERNAME,\n                password=self.settings.PASSWORD.get_secret_value() if self.settings.PASSWORD.get_secret_value() != \"\" else None,\n                pkey=self._load_private_key() if self.settings.PRIVATE_KEY_PATH else None,\n            )\n\n            # Create SFTP client\n            sftp = paramiko.SFTPClient.from_transport(transport)\n\n            if sftp is None:\n                raise OSError(\"Failed to create SFTP client\")\n\n            # Find all .md files\n            md_files = self._find_markdown_files(sftp, self.settings.REMOTE_PATH)\n\n            logger.info(f\"Found {len(md_files)} Markdown files on SFTP server\", extra={\"file_count\": len(md_files)})\n\n            # Load each file into MarkdownDataContract\n            results: list[MarkdownDataContract] = []\n            for remote_file in md_files:\n                try:\n                    contract = self._load_markdown_from_sftp(sftp, remote_file)\n                    if contract:\n                        results.append(contract)\n                except (OSError, paramiko.SSHException) as e:\n                    logger.error(f\"Failed to load file {remote_file}: {e}\", extra={\"remote_file\": remote_file, \"error\": str(e)})\n\n            logger.info(\n                f\"Successfully loaded {len(results)} Markdown files from SFTP\",\n                extra={\"loaded_count\": len(results), \"total_found\": len(md_files)},\n            )\n            if len(results) == 0:\n                raise StepFailed(\"No Markdown files found or failed to load any\")\n\n            return results\n\n        finally:\n            # Clean up connections\n            if sftp:\n                sftp.close()\n            if transport:\n                transport.close()\n\n    def _load_private_key(self) -&gt; paramiko.PKey:\n        \"\"\"Load SSH private key from file.\n\n        Returns:\n            paramiko.PKey: Loaded private key\n\n        Raises:\n            paramiko.SSHException: If key cannot be loaded\n        \"\"\"\n        key_path = self.settings.PRIVATE_KEY_PATH\n        passphrase = (\n            self.settings.PRIVATE_KEY_PASSPHRASE.get_secret_value()\n            if self.settings.PRIVATE_KEY_PASSPHRASE.get_secret_value() != \"\"\n            else None\n        )\n\n        # Try different key types\n        for key_class in (paramiko.RSAKey, paramiko.Ed25519Key, paramiko.ECDSAKey):\n            try:\n                return key_class.from_private_key_file(str(key_path), password=passphrase)\n            except paramiko.SSHException:\n                continue\n\n        raise paramiko.SSHException(f\"Could not load private key from {key_path}\")\n\n    def _find_markdown_files(self, sftp: paramiko.SFTPClient, remote_path: str) -&gt; list[str]:\n        \"\"\"Recursively find all .md files in the remote path.\n\n        Args:\n            sftp: Active SFTP client connection\n            remote_path: Path to search for .md files\n\n        Returns:\n            list[str]: List of remote file paths\n        \"\"\"\n        md_files = []\n\n        try:\n            # List directory contents\n            for entry in sftp.listdir_attr(remote_path):\n                full_path = str(PurePosixPath(remote_path) / entry.filename)\n\n                # Check if it's a directory\n                if self._is_directory(entry):\n                    if self.settings.RECURSIVE:\n                        # Recursively search subdirectories\n                        md_files.extend(self._find_markdown_files(sftp, full_path))\n                elif entry.filename.endswith(\".md\"):\n                    # It's a markdown file\n                    md_files.append(full_path)\n\n        except OSError as e:\n            logger.warning(f\"Could not access directory {remote_path}: {e}\", extra={\"remote_path\": remote_path, \"error\": str(e)})\n\n        return md_files\n\n    def _is_directory(self, attr: paramiko.SFTPAttributes) -&gt; bool:\n        \"\"\"Check if an SFTP file attribute represents a directory.\n\n        Args:\n            attr: SFTP file attributes\n\n        Returns:\n            bool: True if it's a directory\n        \"\"\"\n        if attr.st_mode is None:\n            return False\n        return stat.S_ISDIR(attr.st_mode)\n\n    def _load_markdown_from_sftp(self, sftp: paramiko.SFTPClient, remote_file: str) -&gt; Optional[MarkdownDataContract]:\n        \"\"\"Load a Markdown file from SFTP and convert to MarkdownDataContract.\n\n        Args:\n            sftp: Active SFTP client connection\n            remote_file: Remote file path\n\n        Returns:\n            Optional[MarkdownDataContract]: Loaded contract or None if failed\n        \"\"\"\n        try:\n            # Download file to temporary location and use MarkdownDataContract.from_file\n            with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".md\", delete=False, encoding=\"utf-8\") as tmp_file:\n                # Read content from SFTP\n                with sftp.open(remote_file, \"r\") as remote:\n                    content = remote.read().decode(\"utf-8\")\n\n                # Write to temp file\n                tmp_file.write(content)\n                tmp_path = Path(tmp_file.name)\n\n            try:\n                # Use MarkdownDataContract.from_file to handle metadata parsing\n                url_prefix = f\"{self.__class__.__name__}/{remote_file}\"\n                contract = MarkdownDataContract.from_file(tmp_path, url_prefix=\"\")\n\n                # Override URL if not set in metadata to use remote file path\n                if contract.url == str(tmp_path.absolute()):\n                    contract.url = url_prefix\n\n                return contract\n            finally:\n                # Clean up temporary file\n                tmp_path.unlink(missing_ok=True)\n\n        except (OSError, paramiko.SSHException) as e:\n            raise StepFailed(f\"Failed to load markdown file {remote_file}: {e}\") from e\n</code></pre>"},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownStep-functions","title":"Functions","text":""},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownStep.run","title":"<code>run(inpt)</code>","text":"<p>Execute the step to retrieve Markdown files from SFTP server.</p> <p>Parameters:</p> Name Type Description Default <code>inpt</code> <code>None</code> <p>None (this is a leaf step)</p> required <p>Returns:</p> Type Description <code>list[MarkdownDataContract]</code> <p>list[MarkdownDataContract]: List of loaded Markdown documents</p> <p>Raises:</p> Type Description <code>SSHException</code> <p>If connection or authentication fails</p> <code>IOError</code> <p>If file operations fail</p> Source code in <code>wurzel/steps/sftp/sftp_manual_markdown.py</code> <pre><code>def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n    \"\"\"Execute the step to retrieve Markdown files from SFTP server.\n\n    Args:\n        inpt: None (this is a leaf step)\n\n    Returns:\n        list[MarkdownDataContract]: List of loaded Markdown documents\n\n    Raises:\n        paramiko.SSHException: If connection or authentication fails\n        IOError: If file operations fail\n    \"\"\"\n    logger.info(\n        f\"Connecting to SFTP server {self.settings.HOST}:{self.settings.PORT}\",\n        extra={\"host\": self.settings.HOST, \"port\": self.settings.PORT, \"remote_path\": self.settings.REMOTE_PATH},\n    )\n\n    # Establish SFTP connection\n    transport = None\n    sftp = None\n    try:\n        # Create SSH transport\n        transport = paramiko.Transport((self.settings.HOST, self.settings.PORT))\n        transport.connect(\n            username=self.settings.USERNAME,\n            password=self.settings.PASSWORD.get_secret_value() if self.settings.PASSWORD.get_secret_value() != \"\" else None,\n            pkey=self._load_private_key() if self.settings.PRIVATE_KEY_PATH else None,\n        )\n\n        # Create SFTP client\n        sftp = paramiko.SFTPClient.from_transport(transport)\n\n        if sftp is None:\n            raise OSError(\"Failed to create SFTP client\")\n\n        # Find all .md files\n        md_files = self._find_markdown_files(sftp, self.settings.REMOTE_PATH)\n\n        logger.info(f\"Found {len(md_files)} Markdown files on SFTP server\", extra={\"file_count\": len(md_files)})\n\n        # Load each file into MarkdownDataContract\n        results: list[MarkdownDataContract] = []\n        for remote_file in md_files:\n            try:\n                contract = self._load_markdown_from_sftp(sftp, remote_file)\n                if contract:\n                    results.append(contract)\n            except (OSError, paramiko.SSHException) as e:\n                logger.error(f\"Failed to load file {remote_file}: {e}\", extra={\"remote_file\": remote_file, \"error\": str(e)})\n\n        logger.info(\n            f\"Successfully loaded {len(results)} Markdown files from SFTP\",\n            extra={\"loaded_count\": len(results), \"total_found\": len(md_files)},\n        )\n        if len(results) == 0:\n            raise StepFailed(\"No Markdown files found or failed to load any\")\n\n        return results\n\n    finally:\n        # Clean up connections\n        if sftp:\n            sftp.close()\n        if transport:\n            transport.close()\n</code></pre>"},{"location":"steps/splitter/","title":"Splitter","text":"<p>The splitter step (also known as chunking) takes a long Markdown document (<code>*.md</code>) as the input and returns smaller splits (or chunks) that can easier processed by an embedding model or language model. The splitter keeps the length of the output chunks below a defined threshold (token limit) and tries to split without breaking the document context, e.g., split only at the end of a sentence and not within a sentence.</p>"},{"location":"steps/splitter/#semantic-splitter","title":"Semantic Splitter","text":"<p>Semantic document elements (e.g., headings) are repeated.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter","title":"<code>SemanticSplitter</code>","text":"<p>Splitter implementation for splitting Markdown documents into chunks of a given maximum token count.</p> <p>To preserve context, the splitter repeats headers (headlines, table headers, ...).</p> <p>The splitter tries to only split at:</p> <ul> <li>Boundaries of Markdown elements.</li> <li>Between sentences (using a sentence splitting model).</li> </ul> <p>As the last resort, splits can be made a abritrary string offsets.</p> <p>The splitter store metadata in the output chunks:</p> <ul> <li>chunks_count (number of chunks in the source document)</li> <li>chunk_index (index of current chunk)</li> <li>token_len (number of tokens in the current chunk)</li> <li>char_len (number of characters in the current chunk)</li> </ul>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter.__init__","title":"<code>__init__(token_limit=256, token_limit_buffer=32, token_limit_min=64, sentence_splitter_model='de_core_news_sm', repeat_table_header_row=True, tokenizer_model='gpt-3.5-turbo')</code>","text":"<p>Initializes the SemanticSplitter class with specified token limits and a sentence splitter model.</p> <p>Parameters:</p> Name Type Description Default <code>token_limit</code> <code>int</code> <p>The maximum number of tokens allowed. Defaults to 256.</p> <code>256</code> <code>token_limit_buffer</code> <code>int</code> <p>The buffer size for token limit to allow flexibility. Defaults to 32.</p> <code>32</code> <code>token_limit_min</code> <code>int</code> <p>The minimum number of tokens required. Defaults to 64.</p> <code>64</code> <code>sentence_splitter_model</code> <code>str</code> <p>The name of the sentence splitter model. Defaults to \"de_core_news_sm\".</p> <code>'de_core_news_sm'</code> <code>repeat_table_header_row</code> <code>bool</code> <p>If a table is splitted, the header is repeated. Defaults to True.</p> <code>True</code> <code>tokenizer_model</code> <code>str</code> <p>The name of the tokenizer model to use for encoding. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the specified sentence splitter cannot be loaded.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter.split_markdown_document","title":"<code>split_markdown_document(doc)</code>","text":"<p>Split a Markdown Document into Snippets.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter.text_sentences","title":"<code>text_sentences(text)</code>","text":"<p>Split a text into sentences using a sentence splitter model.</p> <p>This does not use a Regex based approach on purpose as they break with punctuation very easily see: https://stackoverflow.com/a/61254146</p>"},{"location":"steps/splitter/#table-splitter","title":"Table Splitter","text":"<p>For Markdown tables, a custom logic is implemented that preserves the table structure by repeating the header row if a split occurs within a table. So subsequent chunks maintain the semantic table information from the header row. By default, tables are never broken in the middle of a row; if a single row exceeds the budget, it is split at column boundaries instead and full-header is repeated.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil","title":"<code>MarkdownTableSplitterUtil</code>  <code>dataclass</code>","text":"<p>A class to split markdown tables into token-bounded chunks.</p> <p>This class encapsulates the logic for splitting large markdown tables while preserving table structure. Tables are never broken in the middle of a row; if a single row exceeds the max length, it is split at column boundaries instead and the full header is repeated.</p> <p>Example: <pre><code>&gt;&gt;&gt; from wurzel.utils.tokenizers import Tokenizer\n&gt;&gt;&gt; tokenizer = Tokenizer.from_name(\"cl100k_base\")\n&gt;&gt;&gt; splitter = MarkdownTableSplitterUtil(token_limit=8000, tokenizer=tokenizer)\n&gt;&gt;&gt; chunks = splitter.split(markdown_text)\n&gt;&gt;&gt; len(chunks)\n3\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>token_limit</code> <code>int</code> <p>Maximum tokens per chunk (model tokens, not characters).</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer used for counting tokens.</p> required <code>repeat_header_row</code> <code>bool</code> <p>If True, repeat the header row in each chunk. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>chunks</code> <code>list[str]</code> <p>Completed chunks of markdown.</p> <code>buf</code> <code>list[str]</code> <p>Current buffer of lines.</p> <code>buf_tok</code> <code>int</code> <p>Current token count in buffer.</p> <code>min_safety_token_limit</code> <code>int</code> <p>A minimum of 10 tokens is a safety threshold to ensure the splitter can always fit at least a minimal table structure in a chunk.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration after initialization.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil.split","title":"<code>split(md)</code>","text":"<p>Split a markdown document into token-bounded chunks while respecting tables.</p> <p>Parameters:</p> Name Type Description Default <code>md</code> <code>str</code> <p>str Markdown document.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[int]]</code> <p>tuple[list[str], list[int]]: Tuple of chunks whose token counts are &lt;= token_limit and token count of chunks.</p>"},{"location":"steps/splitter/#sentence-splitter","title":"Sentence Splitter","text":"<p>The semantic splitter avoids splitting within sentences and to achieve this it relies on a sentence splitter. The sentence splitter takes longer text as input and splits the text into individual sentences. There are different implementations available.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter","title":"<code>RegexSentenceSplitter</code>","text":"<p>               Bases: <code>SentenceSplitter</code></p> <p>A sentence splitter based on regular expressions.</p> <p>NOTE: Using the regex splitter is not recommended since it based on very simple heuristics.</p> <p>Heuristics: - Split after sentence-ending punctuation (. ! ? \u2026) and any closing quotes/brackets. - Only split if the next non-space token looks like a sentence start   (capital letter or digit, optionally after an opening quote/paren). - Merge back false positives caused by common abbreviations, initials,   dotted acronyms (e.g., U.S.), decimals (e.g., 3.14), ordinals (No. 5), and ellipses.</p> <p>Notes: - Tweak <code>self.abbreviations</code> for your domain/corpus. - For chatty/poetic text where sentences may start lowercase, relax   <code>self._split_re</code>'s lookahead (see comment in init).</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a regex sentence splitter (compile regex, set abbreviations).</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter.get_sentences","title":"<code>get_sentences(text)</code>","text":"<p>Split text into sentences.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter","title":"<code>SpacySentenceSplitter</code>","text":"<p>               Bases: <code>SentenceSplitter</code></p> <p>Adapter for Spacy sentence splitter.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter.__init__","title":"<code>__init__(nlp)</code>","text":"<p>Initialize a SpacySentenceSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>nlp</code> <code>Language</code> <p>A Spacy model from spacy.load().</p> required"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter.get_sentences","title":"<code>get_sentences(text)</code>","text":"<p>Split text into sentences.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter","title":"<code>SaTSentenceSplitter</code>","text":"<p>               Bases: <code>SentenceSplitter</code></p> <p>Adapter for wtpsplit's SaT sentence splitter.</p> <p>SaT (Segment any Text) is a state-of-the-art sentence splitter. Depending on the selected model you may want to use a GPU for faster inference.</p> <p>Available models and benchmark results:  https://github.com/segment-any-text/wtpsplit</p> <p>Example usage: <pre><code>splitter = SentenceSplitter.from_name(\"sat-3l\")\nsplitter.get_sentences(\"This is a test This is another test.\")\n# returns [\"This is a test \", \"This is another test.\"]\n</code></pre></p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter.__init__","title":"<code>__init__(model_name_or_model)</code>","text":"<p>Initialize a SaTSentenceSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_model</code> <code>str</code> <p>A string or Path (Hugging Face ID or local directory path)</p> required"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter.get_sentences","title":"<code>get_sentences(text)</code>","text":"<p>Split text into sentences.</p>"}]}