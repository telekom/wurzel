{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Wurzel","text":"<p>Wurzel is an open-source Python library built to address advanced Extract, Transform, Load (ETL) needs for Retrieval-Augmented Generation (RAG) systems. It is designed to streamline ETL processes while offering essential features like multi-tenancy, cloud-native deployment support, and job scheduling.</p> <p>The repository includes initial implementations for widely-used frameworks in the RAG ecosystem, such as Qdrant, Milvus, and Hugging Face, providing users with a strong starting point for building scalable and efficient RAG pipelines.</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Advanced ETL Pipelines: Tailored for the specific needs of RAG systems.</li> <li>Multi-Tenancy: Easily manage multiple tenants or projects within a single system.</li> <li>Cloud-Native Deployment: Designed for seamless integration with Kubernetes, Docker, and other cloud platforms.</li> <li>Scheduling Capabilities: Schedule and manage ETL tasks using built-in or external tools.</li> <li>Framework Integrations: Pre-built support for popular tools like Qdrant, Milvus, and Hugging Face.</li> <li>Type Security: By leveraging capabilities of pydantic and pandera we ensure type security</li> </ul>"},{"location":"cli/","title":"Cli","text":""},{"location":"cli/#wurzel-cli-reference","title":"\ud83d\udda5\ufe0f Wurzel CLI Reference","text":"<p>The Wurzel CLI provides a powerful command-line interface for managing and executing ETL pipelines for RAG systems.</p>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<pre><code># Install wurzel\npip install wurzel\n\n# Run a step\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep --inputs ./data --output ./out\n\n# Inspect a step\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate a pipeline\nwurzel generate wurzel.steps.manual_markdown.ManualMarkdownStep\n</code></pre>"},{"location":"cli/#cli-commands-reference","title":"CLI Commands Reference","text":"<p>The following documentation is automatically generated from the Wurzel CLI code:</p>"},{"location":"cli/#wurzel","title":"wurzel","text":"<p>Global settings, main.</p>"},{"location":"cli/#usage","title":"Usage","text":"<p><code>wurzel [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"cli/#arguments","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options","title":"Options","text":"Name Description Required Default <code>--log-level TEXT</code> [default: INFO] No - <code>--install-completion</code> Install completion for the current shell. No - <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation. No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#commands","title":"Commands","text":"Name Description <code>run</code> Run a step <code>inspect</code> Display information about a step <code>generate</code> generate a pipeline"},{"location":"cli/#sub-commands","title":"Sub Commands","text":""},{"location":"cli/#wurzel-run","title":"<code>wurzel run</code>","text":"<p>Run a step</p>"},{"location":"cli/#usage_1","title":"Usage","text":"<p><code>wurzel run [OPTIONS] STEP</code></p>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Name Description Required <code>STEP</code> module path to step Yes"},{"location":"cli/#options_1","title":"Options","text":"Name Description Required Default <code>-o, --output DIRECTORY</code> Folder with outputs  [default: -2025-10-31T08:58:28.324] No - <code>-i, --inputs DIRECTORY</code> input folders No - <code>-e, --executor TEXT</code> executor to use  [default: BaseStepExecutor] No - <code>--encapsulate-env / --no-encapsulate-env</code> [default: encapsulate-env] No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#wurzel-inspect","title":"<code>wurzel inspect</code>","text":"<p>Display information about a step</p>"},{"location":"cli/#usage_2","title":"Usage","text":"<p><code>wurzel inspect [OPTIONS] STEP</code></p>"},{"location":"cli/#arguments_2","title":"Arguments","text":"Name Description Required <code>STEP</code> module path to step Yes"},{"location":"cli/#options_2","title":"Options","text":"Name Description Required Default <code>--gen-env / --no-gen-env</code> [default: no-gen-env] No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#wurzel-generate","title":"<code>wurzel generate</code>","text":"<p>generate a pipeline</p>"},{"location":"cli/#usage_3","title":"Usage","text":"<p><code>wurzel generate [OPTIONS] PIPELINE</code></p>"},{"location":"cli/#arguments_3","title":"Arguments","text":"Name Description Required <code>PIPELINE</code> module path to step or pipeline(which is a chained step) Yes"},{"location":"cli/#options_3","title":"Options","text":"Name Description Required Default <code>-b, --backend TEXT</code> backend to use  [default: DvcBackend] No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#usage-examples","title":"Usage Examples","text":""},{"location":"cli/#running-steps","title":"Running Steps","text":"<pre><code># Basic usage\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep \\\n    --inputs ./markdown-files \\\n    --output ./processed-output\n\n# With custom executor\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep \\\n    --inputs ./markdown-files \\\n    --output ./processed-output \\\n    --executor PrometheusStepExecutor\n\n# Multiple input folders\nwurzel run wurzel.steps.splitter.SimpleSplitterStep \\\n    --inputs ./docs \\\n    --inputs ./markdown \\\n    --inputs ./pdfs \\\n    --output ./split-output\n</code></pre>"},{"location":"cli/#inspecting-steps","title":"Inspecting Steps","text":"<pre><code># Basic inspection\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate environment file\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep --gen-env\n</code></pre>"},{"location":"cli/#generating-pipelines","title":"Generating Pipelines","text":"<pre><code># Generate DVC pipeline (default)\nwurzel generate wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate Argo pipeline\nwurzel generate wurzel.steps.manual_markdown.ManualMarkdownStep --backend ArgoBackend\n</code></pre>"},{"location":"cli/#step-auto-discovery","title":"Step Auto-Discovery","text":"<p>The CLI supports intelligent auto-completion for step names using TAB completion:</p> <pre><code>wurzel run &lt;TAB&gt;                    # Shows all available steps\nwurzel run wurzel.steps.&lt;TAB&gt;       # Shows wurzel built-in steps\nwurzel run mysteps.&lt;TAB&gt;            # Shows your custom steps\n</code></pre> <p>The auto-completion discovers:</p> <ol> <li>Built-in Wurzel steps - Available in the <code>wurzel.steps.*</code> namespace</li> <li>User-defined steps - TypedStep classes in your current project</li> </ol>"},{"location":"cli/#performance-optimizations","title":"Performance Optimizations","text":"<p>The CLI auto-completion is optimized for speed:</p> <ul> <li>\u2705 Fast scanning - Only scans relevant directories</li> <li>\u2705 Smart exclusions - Skips <code>.venv</code>, <code>tests</code>, <code>docs</code>, <code>__pycache__</code>, etc.</li> <li>\u2705 AST parsing - Analyzes code without importing modules</li> <li>\u2705 Depth limiting - Prevents deep directory traversal</li> </ul>"},{"location":"backends/","title":"Backend Architecture in Wurzel","text":""},{"location":"backends/#what-are-backends","title":"What are Backends?","text":"<p>Backends in Wurzel are powerful abstractions that transform your pipeline definitions into executable configurations for different orchestration platforms. Think of them as translators that take your high-level pipeline logic and convert it into the specific format required by your target execution environment.</p>"},{"location":"backends/#why-backends-are-great","title":"Why Backends are Great","text":""},{"location":"backends/#write-once-deploy-anywhere","title":"\ud83d\ude80 Write Once, Deploy Anywhere","text":"<p>Define your data pipeline logic once using Wurzel's intuitive API, then deploy it to multiple platforms without rewriting code. Whether you need local development with DVC, cloud-native execution with Argo Workflows, or future platforms like GitHub Actions - your pipeline logic remains the same.</p>"},{"location":"backends/#platform-specific-optimization","title":"\ud83d\udd27 Platform-Specific Optimization","text":"<p>Each backend is specifically designed to leverage the unique capabilities of its target platform:</p> <ul> <li>DVC Backend: Optimizes for data versioning, experiment tracking, and reproducible ML workflows</li> <li>Argo Backend: Leverages Kubernetes-native features like horizontal scaling, resource management, and cloud-native scheduling</li> </ul>"},{"location":"backends/#environment-aware-configuration","title":"\ud83c\udfaf Environment-Aware Configuration","text":"<p>Backends automatically handle environment-specific concerns:</p> <ul> <li>Container orchestration and resource allocation</li> <li>Storage and artifact management</li> <li>Scheduling and triggering mechanisms</li> <li>Security and access control integration</li> </ul>"},{"location":"backends/#scalability-without-complexity","title":"\ud83d\udcc8 Scalability Without Complexity","text":"<p>Start with simple local execution and seamlessly scale to enterprise-grade orchestration platforms. Backends abstract away the complexity of different deployment targets while preserving the power and flexibility of each platform.</p>"},{"location":"backends/#how-backends-work","title":"How Backends Work","text":"<ol> <li>Pipeline Definition: You define your pipeline using Wurzel's step classes and the <code>WZ</code> utility</li> <li>Backend Selection: Choose the appropriate backend for your target environment</li> <li>Code Generation: The backend generates platform-specific configuration files</li> <li>Execution: Deploy and run using the native tools of your chosen platform</li> </ol>"},{"location":"backends/#available-backends","title":"Available Backends","text":"<ul> <li>DVC Backend: For data versioning and ML experiment tracking</li> <li>Argo Workflows Backend: For Kubernetes-native pipeline orchestration</li> </ul>"},{"location":"backends/#future-backends","title":"Future Backends","text":"<p>Wurzel's extensible architecture supports adding new backends for:</p> <ul> <li>GitLab CI/CD: For generating <code>.gitlab-ci.yml</code> pipelines</li> <li>GitHub Actions: To produce <code>workflow.yml</code> for GitHub-native automation</li> <li>Apache Airflow: For DAG-based orchestration and scheduling</li> <li>LocalBackend: Execute steps locally without an external orchestrator</li> <li>Kubernetes CronJobs: Direct Kubernetes-native <code>CronJob</code> manifests</li> </ul>"},{"location":"backends/argoworkflows/","title":"Argo Workflows Backend","text":"<p>The Argo Workflows Backend transforms your Wurzel pipeline into Kubernetes-native CronWorkflow YAML configurations, enabling cloud-native, scalable pipeline orchestration with advanced scheduling capabilities.</p>"},{"location":"backends/argoworkflows/#overview","title":"Overview","text":"<p>Argo Workflows is a powerful, Kubernetes-native workflow engine that excels at container orchestration and parallel execution. The Argo Backend generates <code>CronWorkflow</code> YAML files that leverage Kubernetes' native scheduling and resource management capabilities.</p>"},{"location":"backends/argoworkflows/#key-features","title":"Key Features","text":"<ul> <li>Cloud-Native Orchestration: Run pipelines natively on Kubernetes clusters</li> <li>Horizontal Scaling: Automatically scale pipeline steps based on resource requirements</li> <li>Advanced Scheduling: Cron-based scheduling with fine-grained control</li> <li>Resource Management: Leverage Kubernetes resource limits and requests</li> <li>Artifact Management: Integrated S3-compatible artifact storage</li> <li>Service Integration: Seamless integration with Kubernetes services and secrets</li> </ul>"},{"location":"backends/argoworkflows/#usage","title":"Usage","text":""},{"location":"backends/argoworkflows/#installation","title":"Installation","text":"<p>Install Wurzel with Argo support:</p> <pre><code>pip install wurzel[argo]\n</code></pre>"},{"location":"backends/argoworkflows/#cli-usage","title":"CLI Usage","text":"<p>Generate an Argo Workflows CronWorkflow configuration:</p> <pre><code># Generate cronworkflow.yaml using Argo backend\nwurzel generate --backend ArgoBackend --output cronworkflow.yaml examples.pipeline.pipelinedemo:pipeline\n</code></pre>"},{"location":"backends/argoworkflows/#environment-configuration","title":"Environment Configuration","text":"<p>Configure the Argo backend using environment variables:</p> <pre><code>export ARGOWORKFLOWBACKEND__IMAGE=ghcr.io/telekom/wurzel\nexport ARGOWORKFLOWBACKEND__SCHEDULE=\"0 4 * * *\"\nexport ARGOWORKFLOWBACKEND__DATA_DIR=/usr/app\nexport ARGOWORKFLOWBACKEND__ENCAPSULATE_ENV=true\nexport ARGOWORKFLOWBACKEND__S3_ARTIFACT_TEMPLATE__BUCKET=wurzel-bucket\nexport ARGOWORKFLOWBACKEND__S3_ARTIFACT_TEMPLATE__ENDPOINT=s3.amazonaws.com\nexport ARGOWORKFLOWBACKEND__SERVICE_ACCOUNT_NAME=wurzel-service-account\nexport ARGOWORKFLOWBACKEND__SECRET_NAME=wurzel-secret\nexport ARGOWORKFLOWBACKEND__CONFIG_MAP=wurzel-config\nexport ARGOWORKFLOWBACKEND__PIPELINE_NAME=my-wurzel-pipeline\n</code></pre> <p>Available configuration options:</p> <ul> <li><code>IMAGE</code>: Container image to use for pipeline execution</li> <li><code>SCHEDULE</code>: Cron schedule for automatic pipeline execution</li> <li><code>DATA_DIR</code>: Directory path within the container for data files</li> <li><code>ENCAPSULATE_ENV</code>: Whether to encapsulate environment variables</li> <li><code>S3_ARTIFACT_TEMPLATE__BUCKET</code>: S3 bucket for artifact storage</li> <li><code>S3_ARTIFACT_TEMPLATE__ENDPOINT</code>: S3 endpoint URL</li> <li><code>SERVICE_ACCOUNT_NAME</code>: Kubernetes service account for pipeline execution</li> <li><code>SECRET_NAME</code>: Kubernetes secret containing credentials</li> <li><code>CONFIG_MAP</code>: Kubernetes ConfigMap for configuration</li> <li><code>PIPELINE_NAME</code>: Name for the generated CronWorkflow</li> </ul>"},{"location":"backends/argoworkflows/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the Argo backend directly in Python:</p> <pre><code>from wurzel.backend.argo import ArgoBackend\nfrom wurzel.steps.embedding import EmbeddingStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.qdrant.step import QdrantConnectorStep\nfrom wurzel.utils import WZ\n\n# Define your pipeline\nsource = WZ(ManualMarkdownStep)\nembedding = WZ(EmbeddingStep)\nstep = WZ(QdrantConnectorStep)\n\nsource &gt;&gt; embedding &gt;&gt; step\npipeline = step\n\n# Generate Argo Workflows configuration\nargo_yaml = ArgoBackend().generate_yaml(pipeline)\nprint(argo_yaml)\n</code></pre>"},{"location":"backends/argoworkflows/#deploying-argo-workflows","title":"Deploying Argo Workflows","text":"<p>Once you've generated your CronWorkflow YAML, deploy it to your Kubernetes cluster:</p> <pre><code># Apply the CronWorkflow to your cluster\nkubectl apply -f cronworkflow.yaml\n\n# Monitor workflow executions\nargo list\n\n# Check workflow logs\nargo logs &lt;workflow-name&gt;\n\n# Get workflow status\nargo get &lt;workflow-name&gt;\n</code></pre>"},{"location":"backends/argoworkflows/#benefits-for-cloud-native-pipelines","title":"Benefits for Cloud-Native Pipelines","text":""},{"location":"backends/argoworkflows/#kubernetes-native-execution","title":"Kubernetes-Native Execution","text":"<p>Leverage the full power of Kubernetes for container orchestration, resource management, and fault tolerance.</p>"},{"location":"backends/argoworkflows/#scalable-processing","title":"Scalable Processing","text":"<p>Automatically scale pipeline steps based on workload requirements, with support for parallel execution across multiple nodes.</p>"},{"location":"backends/argoworkflows/#enterprise-security","title":"Enterprise Security","text":"<p>Integrate with Kubernetes RBAC, service accounts, and network policies for enterprise-grade security.</p>"},{"location":"backends/argoworkflows/#cost-optimization","title":"Cost Optimization","text":"<p>Take advantage of Kubernetes features like node auto-scaling and spot instances to optimize infrastructure costs.</p>"},{"location":"backends/argoworkflows/#observability","title":"Observability","text":"<p>Built-in integration with Kubernetes monitoring tools and Argo's web UI for comprehensive pipeline observability.</p>"},{"location":"backends/argoworkflows/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster with Argo Workflows installed</li> <li>kubectl configured to access your cluster</li> <li>Appropriate RBAC permissions for workflow execution</li> <li>S3-compatible storage for artifacts (optional but recommended)</li> </ul>"},{"location":"backends/argoworkflows/#learn-more","title":"Learn More","text":"<ul> <li>Argo Workflows Documentation</li> <li>Kubernetes Documentation</li> <li>Back to Backend Overview</li> </ul>"},{"location":"backends/dvc/","title":"DVC Backend","text":"<p>The DVC Backend transforms your Wurzel pipeline into Data Version Control (DVC) configuration files, enabling reproducible machine learning workflows with built-in data versioning and experiment tracking.</p>"},{"location":"backends/dvc/#overview","title":"Overview","text":"<p>DVC (Data Version Control) is a powerful tool for ML experiment management that works seamlessly with Git. The DVC Backend generates <code>dvc.yaml</code> files that define your pipeline stages, dependencies, and outputs in a format that DVC can execute and track.</p>"},{"location":"backends/dvc/#key-features","title":"Key Features","text":"<ul> <li>Data Versioning: Automatically track changes to datasets and model artifacts</li> <li>Reproducible Pipelines: Generate deterministic pipeline definitions</li> <li>Experiment Tracking: Compare different pipeline runs and their results</li> <li>Git Integration: Version control your pipeline configurations alongside your code</li> <li>Caching: Intelligent caching of intermediate results to speed up development</li> </ul>"},{"location":"backends/dvc/#usage","title":"Usage","text":""},{"location":"backends/dvc/#cli-usage","title":"CLI Usage","text":"<p>Generate a DVC pipeline configuration:</p> <pre><code># Install Wurzel\npip install wurzel\n\n# Generate dvc.yaml (default backend)\nwurzel generate examples.pipeline.pipelinedemo:pipeline\n\n# Explicitly specify DVC backend\nwurzel generate --backend DvcBackend --output dvc.yaml examples.pipeline.pipelinedemo:pipeline\n</code></pre>"},{"location":"backends/dvc/#environment-configuration","title":"Environment Configuration","text":"<p>Configure the DVC backend using environment variables:</p> <pre><code>export DVCBACKEND__DATA_DIR=./data\nexport DVCBACKEND__ENCAPSULATE_ENV=true\n</code></pre> <p>Available configuration options:</p> <ul> <li><code>DVCBACKEND__DATA_DIR</code>: Directory for data files (default: <code>./data</code>)</li> <li><code>DVCBACKEND__ENCAPSULATE_ENV</code>: Whether to encapsulate environment variables (default: <code>false</code>)</li> </ul>"},{"location":"backends/dvc/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the DVC backend directly in Python:</p> <pre><code>from wurzel.backend.dvc import DvcBackend\nfrom wurzel.steps.embedding import EmbeddingStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.steps.qdrant.step import QdrantConnectorStep\nfrom wurzel.utils import WZ\n\n# Define your pipeline\nsource = WZ(ManualMarkdownStep)\nembedding = WZ(EmbeddingStep)\nstep = WZ(QdrantConnectorStep)\n\nsource &gt;&gt; embedding &gt;&gt; step\npipeline = step\n\n# Generate DVC configuration\ndvc_yaml = DvcBackend().generate_yaml(pipeline)\nprint(dvc_yaml)\n</code></pre>"},{"location":"backends/dvc/#running-dvc-pipelines","title":"Running DVC Pipelines","text":"<p>Once you've generated your <code>dvc.yaml</code> file, you can execute the pipeline using DVC:</p> <pre><code># Run the entire pipeline\ndvc repro\n\n# Run specific stages\ndvc repro &lt;stage_name&gt;\n\n# Show pipeline status\ndvc status\n\n# Compare experiments\ndvc plots show\n</code></pre>"},{"location":"backends/dvc/#benefits-for-ml-workflows","title":"Benefits for ML Workflows","text":""},{"location":"backends/dvc/#data-lineage","title":"Data Lineage","text":"<p>Track the complete history of your data transformations, making it easy to understand how your final model was created.</p>"},{"location":"backends/dvc/#experiment-reproducibility","title":"Experiment Reproducibility","text":"<p>Every pipeline run is completely reproducible, with DVC tracking all inputs, parameters, and outputs.</p>"},{"location":"backends/dvc/#collaborative-development","title":"Collaborative Development","text":"<p>Share pipeline definitions through Git while DVC handles the heavy lifting of data and model versioning.</p>"},{"location":"backends/dvc/#performance-optimization","title":"Performance Optimization","text":"<p>DVC's intelligent caching means you only recompute what's changed, dramatically speeding up iterative development.</p>"},{"location":"backends/dvc/#learn-more","title":"Learn More","text":"<ul> <li>DVC Documentation</li> <li>Back to Backend Overview</li> </ul>"},{"location":"datacontract/common/","title":"Data contracts","text":"<p>Data contracts are the primarily inputs and outputs of pipeline steps, e.g., Markdown documents.</p>"},{"location":"datacontract/common/#markdowndatacontract","title":"MarkdownDataContract","text":""},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract","title":"<code>MarkdownDataContract</code>","text":"<p>               Bases: <code>PydanticModel</code></p> <p>A data contract of the input of the EmbeddingStep representing a document in Markdown format.</p> <p>The document consists have the Markdown body (document content) and additional metadata (keywords, url). The metadata is optional.</p> <p>Example 1 (with metadata): <pre><code>---\nkeywords: \"bread,butter\"\nurl: \"some/file/path.md\"\n---\n# Some title\n\nWith some more text.\n\n## And\n\n- Other\n- [Markdown content](#some-link)\n</code></pre></p> <p>Example 2 (without metadata): <pre><code># Another title\n\nAnother text.\n</code></pre></p>"},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract-functions","title":"Functions","text":""},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract.from_dict_w_function","title":"<code>from_dict_w_function(doc, func)</code>  <code>classmethod</code>","text":"<p>Create a MarkdownDataContract from a dict and apply a custom func to test.</p>"},{"location":"datacontract/common/#wurzel.datacontract.common.MarkdownDataContract.from_file","title":"<code>from_file(path, url_prefix='')</code>  <code>classmethod</code>","text":"<p>Load MdContract from .md file and parse YAML metadata from header.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to a Markdown file.</p> required <code>url_prefix</code> <code>str</code> <p>Prefix to add to the URL if it is not specified in the metadata.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>MarkdownDataContract</code> <code>Self</code> <p>The file that was loaded</p> <p>Raises:</p> Type Description <code>YAMLError</code> <p>If the YAML metadata cannot be parsed.</p> <code>ValueError</code> <p>If the YAML metadata is not a dictionary.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>Welcome to the Wurzel Developer Guide! This comprehensive guide covers everything you need to know to get started with Wurzel, from installation to building your own pipeline steps.</p>"},{"location":"developer-guide/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>New to Wurzel? Follow this recommended path:</p> <ol> <li>Installation &amp; Setup - Get Wurzel running on your system</li> <li>Getting Started - Learn the basics and development workflow</li> <li>Building Pipelines - Create your first pipeline</li> <li>Creating Custom Steps - Build your own processing components</li> <li>Data Contracts - Understand type-safe data exchange</li> </ol>"},{"location":"developer-guide/#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":""},{"location":"developer-guide/#installation-environment","title":"Installation &amp; Environment","text":"<ul> <li>Installation &amp; Setup - Complete installation guide with Docker, dependencies, and troubleshooting</li> </ul>"},{"location":"developer-guide/#development-workflow","title":"Development Workflow","text":"<ul> <li>Getting Started - Development environment setup, testing, and code quality tools</li> </ul>"},{"location":"developer-guide/#core-concepts","title":"Core Concepts","text":"<ul> <li>Building Pipelines - How to define and structure pipelines</li> <li>Creating Custom Steps - Build your own data sources and processing steps</li> <li>Data Contracts - Type-safe data exchange between pipeline steps</li> </ul>"},{"location":"developer-guide/#common-tasks","title":"\ud83c\udfaf Common Tasks","text":"Task Documentation Install Wurzel Installation Guide Set up development environment Getting Started Create a pipeline Building Pipelines Build a custom step Creating Steps Add a data source Creating Steps Run tests Getting Started Generate documentation Getting Started"},{"location":"developer-guide/#external-resources","title":"\ud83d\udd17 External Resources","text":"<ul> <li>Backend Documentation - Detailed guides for DVC and Argo Workflows backends</li> <li>Wurzel Project Documentation - Auto-generated API documentation</li> </ul>"},{"location":"developer-guide/#need-help","title":"\ud83d\udca1 Need Help?","text":"<ul> <li>Check the troubleshooting section for common issues</li> <li>Review the examples for real-world usage patterns</li> <li>Consult the AI documentation for reference</li> </ul>"},{"location":"developer-guide/building-pipelines/","title":"Building Pipelines","text":"<p>Learn how to define and structure data processing pipelines in Wurzel using the intuitive chaining syntax and modular step architecture.</p>"},{"location":"developer-guide/building-pipelines/#what-is-a-wurzel-pipeline","title":"What is a Wurzel Pipeline?","text":"<p>A pipeline in Wurzel is a chain of processing steps that are connected and executed in sequence. Each step processes the output of the previous one, enabling modular, reusable, and optimally scheduled workflows.</p>"},{"location":"developer-guide/building-pipelines/#key-concepts","title":"Key Concepts","text":"<ul> <li>TypedStep: Individual processing units with defined input/output contracts</li> <li>Pipeline Chaining: Steps are connected using the <code>&gt;&gt;</code> operator</li> <li>Automatic Dependency Resolution: Wurzel determines execution order automatically</li> </ul>"},{"location":"developer-guide/building-pipelines/#basic-pipeline-structure","title":"Basic Pipeline Structure","text":""},{"location":"developer-guide/building-pipelines/#the-wz-utility","title":"The WZ Utility","text":"<p>The <code>WZ()</code> utility function is your primary tool for instantiating steps:</p> <pre><code>from wurzel.utils import WZ\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\n\n# Create a step instance\nmarkdown_step = WZ(ManualMarkdownStep)\n</code></pre>"},{"location":"developer-guide/building-pipelines/#chaining-steps","title":"Chaining Steps","text":"<p>Connect steps using the <code>&gt;&gt;</code> operator to define data flow:</p> <pre><code>from wurzel.steps import EmbeddingStep, QdrantConnectorStep\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\n# Define individual steps\nsource = WZ(ManualMarkdownStep)\nembedding = WZ(EmbeddingStep)\nstorage = WZ(QdrantConnectorStep)\n\n# Chain them together\nsource &gt;&gt; embedding &gt;&gt; storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#defining-a-complete-pipeline","title":"Defining a Complete Pipeline","text":""},{"location":"developer-guide/building-pipelines/#basic-example","title":"Basic Example","text":"<p>Here's a complete pipeline that processes markdown documents, generates embeddings, and stores them in a vector database:</p> <pre><code>from wurzel.steps import (\n    EmbeddingStep,\n    QdrantConnectorStep,\n)\nfrom wurzel.utils import WZ\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.step import TypedStep\n\ndef pipeline() -&gt; TypedStep:\n    \"\"\"Defines a Wurzel pipeline that embeds manual markdown and stores it in Qdrant.\"\"\"\n\n    # Step 1: Load markdown input manually\n    md = WZ(ManualMarkdownStep)\n\n    # Step 2: Generate embeddings from markdown content\n    embed = WZ(EmbeddingStep)\n\n    # Step 3: Store embeddings in a Qdrant vector database\n    db = WZ(QdrantConnectorStep)\n\n    # Chain the steps\n    md &gt;&gt; embed &gt;&gt; db\n\n    # Return the final step in the chain\n    return db\n</code></pre>"},{"location":"developer-guide/building-pipelines/#execution-order","title":"Execution Order","text":"<p>Even though the function returns only the last step (<code>db</code>), Wurzel automatically resolves and runs all upstream dependencies in the correct order:</p> <ol> <li>ManualMarkdownStep runs first to provide data</li> <li>EmbeddingStep transforms that data into vectors</li> <li>QdrantConnectorStep persists the result</li> </ol>"},{"location":"developer-guide/building-pipelines/#advanced-pipeline-patterns","title":"Advanced Pipeline Patterns","text":""},{"location":"developer-guide/building-pipelines/#branching-pipelines","title":"Branching Pipelines","text":"<p>You can create branches in your pipeline where one step feeds into multiple downstream steps:</p> <pre><code>def branching_pipeline() -&gt; TypedStep:\n    \"\"\"Pipeline with branching data flow.\"\"\"\n\n    # Source step\n    source = WZ(ManualMarkdownStep)\n\n    # Processing steps\n    embedding = WZ(EmbeddingStep)\n    preprocessor = WZ(TextPreprocessorStep)\n\n    # Branch: source feeds into both embedding and preprocessor\n    source &gt;&gt; embedding\n    source &gt;&gt; preprocessor\n\n    # Converge: both feed into final storage\n    vector_db = WZ(QdrantConnectorStep)\n    processed_storage = WZ(ProcessedTextStorageStep)\n\n    embedding &gt;&gt; vector_db\n    preprocessor &gt;&gt; processed_storage\n\n    # Return one of the final steps (or create a step that depends on both)\n    return vector_db\n</code></pre>"},{"location":"developer-guide/building-pipelines/#multi-input-steps","title":"Multi-Input Steps","text":"<p>Some steps can accept input from multiple upstream steps:</p> <pre><code>def multi_input_pipeline() -&gt; TypedStep:\n    \"\"\"Pipeline where a step receives multiple inputs.\"\"\"\n\n    text_source = WZ(TextSourceStep)\n    image_source = WZ(ImageSourceStep)\n\n    # Multi-modal step that accepts both text and images\n    multimodal_processor = WZ(MultiModalProcessorStep)\n\n    # Both sources feed into the processor\n    text_source &gt;&gt; multimodal_processor\n    image_source &gt;&gt; multimodal_processor\n\n    storage = WZ(MultiModalStorageStep)\n    multimodal_processor &gt;&gt; storage\n\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#conditional-processing","title":"Conditional Processing","text":"<p>Create pipelines with conditional logic using step parameters:</p> <pre><code>def conditional_pipeline(use_advanced_processing: bool = False) -&gt; TypedStep:\n    \"\"\"Pipeline with conditional processing paths.\"\"\"\n\n    source = WZ(ManualMarkdownStep)\n\n    if use_advanced_processing:\n        processor = WZ(AdvancedEmbeddingStep)\n    else:\n        processor = WZ(BasicEmbeddingStep)\n\n    storage = WZ(QdrantConnectorStep)\n\n    source &gt;&gt; processor &gt;&gt; storage\n\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"developer-guide/building-pipelines/#step-settings","title":"Step Settings","text":"<p>Each step can be configured through environment variables or settings classes. See Creating Custom Steps for details.</p>"},{"location":"developer-guide/building-pipelines/#pipeline-level-configuration","title":"Pipeline-Level Configuration","text":"<p>Configure entire pipelines through environment variables:</p> <pre><code>import os\nfrom wurzel.steps import EmbeddingStep\nfrom wurzel.utils import WZ\n\ndef configurable_pipeline() -&gt; TypedStep:\n    \"\"\"Pipeline that adapts based on environment configuration.\"\"\"\n\n    source = WZ(ManualMarkdownStep)\n\n    # Configure embedding step based on environment\n    embedding_model = os.getenv('EMBEDDING_MODEL', 'default')\n    if embedding_model == 'advanced':\n        embedding = WZ(AdvancedEmbeddingStep)\n    else:\n        embedding = WZ(EmbeddingStep)\n\n    storage = WZ(QdrantConnectorStep)\n\n    source &gt;&gt; embedding &gt;&gt; storage\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#testing-pipelines","title":"Testing Pipelines","text":""},{"location":"developer-guide/building-pipelines/#unit-testing-individual-steps","title":"Unit Testing Individual Steps","text":"<p>Test steps in isolation:</p> <pre><code>import pytest\nfrom wurzel.steps.manual_markdown import ManualMarkdownStep\nfrom wurzel.utils import WZ\n\ndef test_markdown_step():\n    \"\"\"Test the markdown step in isolation.\"\"\"\n    step = WZ(ManualMarkdownStep)\n    result = step.run(None)\n\n    assert result is not None\n    assert len(result) &gt; 0\n</code></pre>"},{"location":"developer-guide/building-pipelines/#integration-testing","title":"Integration Testing","text":"<p>Test complete pipeline flows:</p> <pre><code>def test_complete_pipeline():\n    \"\"\"Test the entire pipeline execution.\"\"\"\n    pipeline_result = pipeline()\n\n    # Execute the pipeline (this would typically be done by a backend)\n    # result = execute_pipeline(pipeline_result)\n\n    # Assert pipeline structure\n    assert pipeline_result is not None\n    # Add more specific assertions based on your pipeline\n</code></pre>"},{"location":"developer-guide/building-pipelines/#pipeline-optimization","title":"Pipeline Optimization","text":""},{"location":"developer-guide/building-pipelines/#parallel-execution","title":"Parallel Execution","text":"<p>Wurzel can automatically parallelize steps that don't depend on each other:</p> <pre><code>def parallel_pipeline() -&gt; TypedStep:\n    \"\"\"Pipeline optimized for parallel execution.\"\"\"\n\n    source = WZ(ManualMarkdownStep)\n\n    # These can run in parallel since they're independent\n    embedding_a = WZ(EmbeddingStepA)\n    embedding_b = WZ(EmbeddingStepB)\n\n    source &gt;&gt; embedding_a\n    source &gt;&gt; embedding_b\n\n    # This step waits for both embeddings\n    combiner = WZ(EmbeddingCombinerStep)\n    embedding_a &gt;&gt; combiner\n    embedding_b &gt;&gt; combiner\n\n    storage = WZ(QdrantConnectorStep)\n    combiner &gt;&gt; storage\n\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#caching-and-persistence","title":"Caching and Persistence","text":"<p>Steps automatically cache their outputs based on input changes. This is handled transparently by the backend execution engines.</p>"},{"location":"developer-guide/building-pipelines/#best-practices","title":"Best Practices","text":""},{"location":"developer-guide/building-pipelines/#pipeline-design","title":"Pipeline Design","text":"<ol> <li>Keep steps focused: Each step should have a single, clear responsibility</li> <li>Use meaningful names: Choose descriptive names for your pipeline functions and step variables</li> <li>Document data flow: Use comments to explain complex pipeline logic</li> <li>Handle errors gracefully: Implement proper error handling in custom steps</li> </ol>"},{"location":"developer-guide/building-pipelines/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Minimize data copying: Use efficient data structures and avoid unnecessary transformations</li> <li>Batch processing: Design steps to handle multiple items efficiently</li> <li>Resource management: Be mindful of memory usage in data-intensive steps</li> </ol>"},{"location":"developer-guide/building-pipelines/#code-organization","title":"Code Organization","text":"<pre><code># Good: Organized and readable\ndef document_processing_pipeline() -&gt; TypedStep:\n    \"\"\"\n    Processes documents through embedding and storage.\n\n    Pipeline flow:\n    1. Load markdown documents\n    2. Generate embeddings\n    3. Store in vector database\n    \"\"\"\n    # Data ingestion\n    documents = WZ(ManualMarkdownStep)\n\n    # Processing\n    embeddings = WZ(EmbeddingStep)\n\n    # Storage\n    storage = WZ(QdrantConnectorStep)\n\n    # Pipeline definition\n    documents &gt;&gt; embeddings &gt;&gt; storage\n\n    return storage\n</code></pre>"},{"location":"developer-guide/building-pipelines/#common-patterns","title":"Common Patterns","text":""},{"location":"developer-guide/building-pipelines/#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>def etl_pipeline() -&gt; TypedStep:\n    \"\"\"Extract, Transform, Load pipeline.\"\"\"\n\n    # Extract\n    extractor = WZ(DataExtractionStep)\n\n    # Transform\n    transformer = WZ(DataTransformationStep)\n    validator = WZ(DataValidationStep)\n\n    # Load\n    loader = WZ(DataLoadingStep)\n\n    # Chain\n    extractor &gt;&gt; transformer &gt;&gt; validator &gt;&gt; loader\n\n    return loader\n</code></pre>"},{"location":"developer-guide/building-pipelines/#ml-pipeline","title":"ML Pipeline","text":"<pre><code>def ml_pipeline() -&gt; TypedStep:\n    \"\"\"Machine learning pipeline with training and inference.\"\"\"\n\n    # Data preparation\n    data_loader = WZ(MLDataLoaderStep)\n    preprocessor = WZ(DataPreprocessorStep)\n\n    # Model training\n    trainer = WZ(ModelTrainingStep)\n\n    # Model evaluation\n    evaluator = WZ(ModelEvaluationStep)\n\n    # Pipeline\n    data_loader &gt;&gt; preprocessor &gt;&gt; trainer &gt;&gt; evaluator\n\n    return evaluator\n</code></pre>"},{"location":"developer-guide/building-pipelines/#next-steps","title":"Next Steps","text":"<ul> <li>Create Custom Steps - Learn to build your own processing components</li> <li>Understand Data Contracts - Deep dive into type-safe data exchange</li> <li>Explore Backends - Deploy your pipelines to different platforms</li> </ul>"},{"location":"developer-guide/building-pipelines/#additional-resources","title":"Additional Resources","text":"<ul> <li>Step Examples - Real-world step implementations</li> <li>API Documentation - Complete API reference</li> <li>Backend Guides - Platform-specific deployment instructions</li> </ul>"},{"location":"developer-guide/cli/","title":"Cli Reference","text":""},{"location":"developer-guide/cli/#wurzel-cli-reference","title":"\ud83d\udda5\ufe0f Wurzel CLI Reference","text":"<p>The Wurzel CLI provides a powerful command-line interface for managing and executing ETL pipelines for RAG systems.</p>"},{"location":"developer-guide/cli/#quick-start","title":"Quick Start","text":"<pre><code># Install wurzel\npip install wurzel\n\n# Run a step\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep --inputs ./data --output ./out\n\n# Inspect a step\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate a pipeline\nwurzel generate wurzel.steps.manual_markdown.ManualMarkdownStep\n</code></pre>"},{"location":"developer-guide/cli/#cli-commands-reference","title":"CLI Commands Reference","text":"<p>The following documentation is automatically generated from the Wurzel CLI code:</p>"},{"location":"developer-guide/cli/#wurzel","title":"wurzel","text":"<p>Global settings, main.</p>"},{"location":"developer-guide/cli/#usage","title":"Usage","text":"<p><code>wurzel [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"developer-guide/cli/#arguments","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"developer-guide/cli/#options","title":"Options","text":"Name Description Required Default <code>--log-level TEXT</code> [default: INFO] No - <code>--install-completion</code> Install completion for the current shell. No - <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation. No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#commands","title":"Commands","text":"Name Description <code>run</code> Run a step <code>inspect</code> Display information about a step <code>generate</code> generate a pipeline"},{"location":"developer-guide/cli/#sub-commands","title":"Sub Commands","text":""},{"location":"developer-guide/cli/#wurzel-run","title":"<code>wurzel run</code>","text":"<p>Run a step</p>"},{"location":"developer-guide/cli/#usage_1","title":"Usage","text":"<p><code>wurzel run [OPTIONS] STEP</code></p>"},{"location":"developer-guide/cli/#arguments_1","title":"Arguments","text":"Name Description Required <code>STEP</code> module path to step Yes"},{"location":"developer-guide/cli/#options_1","title":"Options","text":"Name Description Required Default <code>-o, --output DIRECTORY</code> Folder with outputs  [default: -2025-10-31T08:58:28.968] No - <code>-i, --inputs DIRECTORY</code> input folders No - <code>-e, --executor TEXT</code> executor to use  [default: BaseStepExecutor] No - <code>--encapsulate-env / --no-encapsulate-env</code> [default: encapsulate-env] No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#wurzel-inspect","title":"<code>wurzel inspect</code>","text":"<p>Display information about a step</p>"},{"location":"developer-guide/cli/#usage_2","title":"Usage","text":"<p><code>wurzel inspect [OPTIONS] STEP</code></p>"},{"location":"developer-guide/cli/#arguments_2","title":"Arguments","text":"Name Description Required <code>STEP</code> module path to step Yes"},{"location":"developer-guide/cli/#options_2","title":"Options","text":"Name Description Required Default <code>--gen-env / --no-gen-env</code> [default: no-gen-env] No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#wurzel-generate","title":"<code>wurzel generate</code>","text":"<p>generate a pipeline</p>"},{"location":"developer-guide/cli/#usage_3","title":"Usage","text":"<p><code>wurzel generate [OPTIONS] PIPELINE</code></p>"},{"location":"developer-guide/cli/#arguments_3","title":"Arguments","text":"Name Description Required <code>PIPELINE</code> module path to step or pipeline(which is a chained step) Yes"},{"location":"developer-guide/cli/#options_3","title":"Options","text":"Name Description Required Default <code>-b, --backend TEXT</code> backend to use  [default: DvcBackend] No - <code>--help</code> Show this message and exit. No -"},{"location":"developer-guide/cli/#usage-examples","title":"Usage Examples","text":""},{"location":"developer-guide/cli/#running-steps","title":"Running Steps","text":"<pre><code># Basic usage\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep \\\n    --inputs ./markdown-files \\\n    --output ./processed-output\n\n# With custom executor\nwurzel run wurzel.steps.manual_markdown.ManualMarkdownStep \\\n    --inputs ./markdown-files \\\n    --output ./processed-output \\\n    --executor PrometheusStepExecutor\n\n# Multiple input folders\nwurzel run wurzel.steps.splitter.SimpleSplitterStep \\\n    --inputs ./docs \\\n    --inputs ./markdown \\\n    --inputs ./pdfs \\\n    --output ./split-output\n</code></pre>"},{"location":"developer-guide/cli/#inspecting-steps","title":"Inspecting Steps","text":"<pre><code># Basic inspection\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate environment file\nwurzel inspect wurzel.steps.manual_markdown.ManualMarkdownStep --gen-env\n</code></pre>"},{"location":"developer-guide/cli/#generating-pipelines","title":"Generating Pipelines","text":"<pre><code># Generate DVC pipeline (default)\nwurzel generate wurzel.steps.manual_markdown.ManualMarkdownStep\n\n# Generate Argo pipeline\nwurzel generate wurzel.steps.manual_markdown.ManualMarkdownStep --backend ArgoBackend\n</code></pre>"},{"location":"developer-guide/cli/#step-auto-discovery","title":"Step Auto-Discovery","text":"<p>The CLI supports intelligent auto-completion for step names using TAB completion:</p> <pre><code>wurzel run &lt;TAB&gt;                    # Shows all available steps\nwurzel run wurzel.steps.&lt;TAB&gt;       # Shows wurzel built-in steps\nwurzel run mysteps.&lt;TAB&gt;            # Shows your custom steps\n</code></pre> <p>The auto-completion discovers:</p> <ol> <li>Built-in Wurzel steps - Available in the <code>wurzel.steps.*</code> namespace</li> <li>User-defined steps - TypedStep classes in your current project</li> </ol>"},{"location":"developer-guide/cli/#performance-optimizations","title":"Performance Optimizations","text":"<p>The CLI auto-completion is optimized for speed:</p> <ul> <li>\u2705 Fast scanning - Only scans relevant directories</li> <li>\u2705 Smart exclusions - Skips <code>.venv</code>, <code>tests</code>, <code>docs</code>, <code>__pycache__</code>, etc.</li> <li>\u2705 AST parsing - Analyzes code without importing modules</li> <li>\u2705 Depth limiting - Prevents deep directory traversal</li> </ul>"},{"location":"developer-guide/creating-steps/","title":"Creating Custom Steps","text":"<p>Learn how to build your own data processing steps in Wurzel, from simple data sources to complex transformation components.</p>"},{"location":"developer-guide/creating-steps/#step-types-overview","title":"Step Types Overview","text":"<p>Wurzel provides two main types of steps:</p> <ul> <li>Data Source Steps (WurzelTips): Entry points that ingest data from external sources</li> <li>Processing Steps (WurzelSteps): Transform data from upstream steps</li> </ul> <p>Both types follow the same <code>TypedStep</code> interface but serve different roles in your pipeline.</p>"},{"location":"developer-guide/creating-steps/#step-architecture","title":"Step Architecture","text":""},{"location":"developer-guide/creating-steps/#the-typedstep-interface","title":"The TypedStep Interface","text":"<p>All steps inherit from <code>TypedStep</code>, which provides:</p> <pre><code>class TypedStep[TSettings, TInput, TOutput]:\n    \"\"\"\n    Base class for all pipeline steps.\n\n    Type parameters:\n    - TSettings: Configuration schema (Pydantic BaseModel)\n    - TInput: Input data type (or None for data sources)\n    - TOutput: Output data type\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the step (setup logic goes here).\"\"\"\n        pass\n\n    def run(self, inpt: TInput) -&gt; TOutput:\n        \"\"\"Process input data and return output.\"\"\"\n        raise NotImplementedError\n\n    def finalize(self) -&gt; None:\n        \"\"\"Cleanup logic called after pipeline execution.\"\"\"\n        pass\n</code></pre>"},{"location":"developer-guide/creating-steps/#step-lifecycle","title":"Step Lifecycle","text":"<ol> <li>Initialization (<code>__init__</code>): Setup connections, create resources</li> <li>Execution (<code>run</code>): Process data (may be called multiple times)</li> <li>Finalization (<code>finalize</code>): Cleanup resources, close connections</li> </ol> <p>\u26a0\ufe0f Important: The <code>run</code> method may be executed multiple times for different upstream dependencies. Put setup logic in <code>__init__</code>, not <code>run</code>.</p>"},{"location":"developer-guide/creating-steps/#creating-data-source-steps","title":"Creating Data Source Steps","text":"<p>Data source steps introduce data into your pipeline. They have <code>None</code> as their input type since they don't depend on previous steps.</p>"},{"location":"developer-guide/creating-steps/#basic-data-source","title":"Basic Data Source","text":"<pre><code>from typing import Any\nfrom wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.meta.settings import Settings\n\nclass MySettings(Settings):\n    \"\"\"Configuration for MyDatasourceStep.\"\"\"\n    data_path: str = \"./data\"\n    file_pattern: str = \"*.md\"\n\nclass MyDatasourceStep(TypedStep[MySettings, None, list[MarkdownDataContract]]):\n    \"\"\"Data source step for loading Markdown files from a configurable path.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the data source.\"\"\"\n        self.settings = MySettings()\n        # Setup logic here (validate paths, check permissions, etc.)\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        \"\"\"Load and return markdown documents.\"\"\"\n        import glob\n        import os\n\n        pattern = os.path.join(self.settings.data_path, self.settings.file_pattern)\n        files = glob.glob(pattern)\n\n        documents = []\n        for file_path in files:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n\n            doc = MarkdownDataContract(\n                content=content,\n                source=file_path,\n                metadata={\"file_path\": file_path}\n            )\n            documents.append(doc)\n\n        return documents\n</code></pre>"},{"location":"developer-guide/creating-steps/#advanced-data-source-with-database","title":"Advanced Data Source with Database","text":"<pre><code>import sqlite3\nfrom wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.meta.settings import Settings\n\nclass DatabaseSettings(Settings):\n    \"\"\"Database connection settings.\"\"\"\n    db_path: str = \"data.db\"\n    table_name: str = \"documents\"\n    query: str = \"SELECT content, source, metadata FROM documents\"\n\nclass DatabaseSourceStep(TypedStep[DatabaseSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Load documents from a SQLite database.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize database connection.\"\"\"\n        self.settings = DatabaseSettings()\n        self.connection = sqlite3.connect(self.settings.db_path)\n        self.connection.row_factory = sqlite3.Row  # Enable column access by name\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        \"\"\"Query database and return documents.\"\"\"\n        cursor = self.connection.cursor()\n        cursor.execute(self.settings.query)\n\n        documents = []\n        for row in cursor.fetchall():\n            doc = MarkdownDataContract(\n                content=row['content'],\n                source=row['source'],\n                metadata=eval(row['metadata']) if row['metadata'] else {}\n            )\n            documents.append(doc)\n\n        return documents\n\n    def finalize(self) -&gt; None:\n        \"\"\"Close database connection.\"\"\"\n        if self.connection:\n            self.connection.close()\n</code></pre>"},{"location":"developer-guide/creating-steps/#creating-processing-steps","title":"Creating Processing Steps","text":"<p>Processing steps transform data from upstream steps. They can filter, validate, transform, or enrich data.</p>"},{"location":"developer-guide/creating-steps/#filter-step","title":"Filter Step","text":"<pre><code>from wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.meta.settings import Settings\n\nclass FilterSettings(Settings):\n    \"\"\"Settings for document filtering.\"\"\"\n    min_length: int = 100\n    max_length: int = 10000\n    required_keywords: list[str] = []\n\nclass DocumentFilterStep(TypedStep[FilterSettings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"Filter documents based on length and keyword criteria.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the filter.\"\"\"\n        self.settings = FilterSettings()\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Filter documents based on criteria.\"\"\"\n        filtered_docs = []\n\n        for doc in inpt:\n            # Length filter\n            content_length = len(doc.content)\n            if content_length &lt; self.settings.min_length or content_length &gt; self.settings.max_length:\n                continue\n\n            # Keyword filter\n            if self.settings.required_keywords:\n                content_lower = doc.content.lower()\n                if not all(keyword.lower() in content_lower for keyword in self.settings.required_keywords):\n                    continue\n\n            filtered_docs.append(doc)\n\n        return filtered_docs\n</code></pre>"},{"location":"developer-guide/creating-steps/#transformation-step","title":"Transformation Step","text":"<pre><code>import pandas as pd\nfrom wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract, EmbeddingResult\nfrom wurzel.meta.settings import Settings\n\nclass EmbeddingSettings(Settings):\n    \"\"\"Settings for embedding generation.\"\"\"\n    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    batch_size: int = 32\n\nclass CustomEmbeddingStep(TypedStep[EmbeddingSettings, list[MarkdownDataContract], pd.DataFrame[EmbeddingResult]]):\n    \"\"\"Transform documents into embeddings stored in a DataFrame.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the embedding model.\"\"\"\n        self.settings = EmbeddingSettings()\n\n        # Import and initialize model\n        from sentence_transformers import SentenceTransformer\n        self.model = SentenceTransformer(self.settings.model_name)\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; pd.DataFrame[EmbeddingResult]:\n        \"\"\"Generate embeddings for input documents.\"\"\"\n\n        # Extract texts for embedding\n        texts = [doc.content for doc in inpt]\n\n        # Generate embeddings in batches\n        embeddings = self.model.encode(\n            texts,\n            batch_size=self.settings.batch_size,\n            show_progress_bar=True\n        )\n\n        # Create result objects\n        results = []\n        for i, (doc, embedding) in enumerate(zip(inpt, embeddings)):\n            result = EmbeddingResult(\n                id=f\"doc_{i}\",\n                content=doc.content,\n                source=doc.source,\n                metadata=doc.metadata,\n                embedding=embedding.tolist(),\n                embedding_model=self.settings.model_name\n            )\n            results.append(result)\n\n        # Convert to DataFrame\n        return pd.DataFrame(results)\n</code></pre>"},{"location":"developer-guide/creating-steps/#validation-step","title":"Validation Step","text":"<pre><code>from wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.meta.settings import Settings\n\nclass ValidationSettings(Settings):\n    \"\"\"Settings for document validation.\"\"\"\n    check_encoding: bool = True\n    check_structure: bool = True\n    max_errors: int = 5\n\nclass DocumentValidationStep(TypedStep[ValidationSettings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"Validate documents and filter out invalid ones.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize validator.\"\"\"\n        self.settings = ValidationSettings()\n        self.error_count = 0\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Validate and filter documents.\"\"\"\n        valid_docs = []\n\n        for doc in inpt:\n            if self._validate_document(doc):\n                valid_docs.append(doc)\n            else:\n                self.error_count += 1\n                if self.error_count &gt;= self.settings.max_errors:\n                    raise RuntimeError(f\"Too many validation errors: {self.error_count}\")\n\n        return valid_docs\n\n    def _validate_document(self, doc: MarkdownDataContract) -&gt; bool:\n        \"\"\"Validate a single document.\"\"\"\n\n        # Check encoding\n        if self.settings.check_encoding:\n            try:\n                doc.content.encode('utf-8')\n            except UnicodeEncodeError:\n                return False\n\n        # Check structure\n        if self.settings.check_structure:\n            if not doc.content.strip():\n                return False\n\n            if len(doc.content) &lt; 10:  # Minimum content length\n                return False\n\n        return True\n</code></pre>"},{"location":"developer-guide/creating-steps/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"developer-guide/creating-steps/#multi-input-steps","title":"Multi-Input Steps","text":"<p>Some steps need to combine data from multiple sources:</p> <pre><code>from typing import Union\nfrom wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.meta.settings import Settings\n\nclass MergerSettings(Settings):\n    \"\"\"Settings for document merging.\"\"\"\n    merge_strategy: str = \"concatenate\"  # or \"interleave\"\n\nclass DocumentMergerStep(TypedStep[MergerSettings, Union[list[MarkdownDataContract], list[MarkdownDataContract]], list[MarkdownDataContract]]):\n    \"\"\"Merge documents from multiple sources.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize merger.\"\"\"\n        self.settings = MergerSettings()\n        self.collected_inputs = []\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Collect inputs and merge when all are available.\"\"\"\n        self.collected_inputs.append(inpt)\n\n        # In a real implementation, you'd need logic to determine\n        # when all inputs have been received\n        if len(self.collected_inputs) &gt;= 2:  # Expecting 2 sources\n            return self._merge_documents()\n\n        return []  # Return empty until all inputs received\n\n    def _merge_documents(self) -&gt; list[MarkdownDataContract]:\n        \"\"\"Merge collected documents.\"\"\"\n        if self.settings.merge_strategy == \"concatenate\":\n            # Simply concatenate all documents\n            all_docs = []\n            for doc_list in self.collected_inputs:\n                all_docs.extend(doc_list)\n            return all_docs\n\n        elif self.settings.merge_strategy == \"interleave\":\n            # Interleave documents from different sources\n            # Implementation depends on your specific needs\n            pass\n\n        return []\n</code></pre>"},{"location":"developer-guide/creating-steps/#stateful-processing","title":"Stateful Processing","text":"<p>For steps that need to maintain state across executions:</p> <pre><code>from collections import defaultdict\nfrom wurzel.step import TypedStep\nfrom wurzel.datacontract import MarkdownDataContract\nfrom wurzel.meta.settings import Settings\n\nclass DeduplicationSettings(Settings):\n    \"\"\"Settings for deduplication.\"\"\"\n    similarity_threshold: float = 0.9\n    hash_algorithm: str = \"md5\"\n\nclass DeduplicationStep(TypedStep[DeduplicationSettings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"Remove duplicate documents based on content similarity.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize deduplication.\"\"\"\n        self.settings = DeduplicationSettings()\n        self.seen_hashes = set()\n        self.document_index = defaultdict(list)\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Remove duplicates from input documents.\"\"\"\n        import hashlib\n\n        unique_docs = []\n\n        for doc in inpt:\n            # Generate content hash\n            if self.settings.hash_algorithm == \"md5\":\n                content_hash = hashlib.md5(doc.content.encode()).hexdigest()\n            else:\n                content_hash = hashlib.sha256(doc.content.encode()).hexdigest()\n\n            # Check if we've seen this content before\n            if content_hash not in self.seen_hashes:\n                self.seen_hashes.add(content_hash)\n                unique_docs.append(doc)\n                self.document_index[content_hash].append(doc.source)\n\n        return unique_docs\n\n    def finalize(self) -&gt; None:\n        \"\"\"Log deduplication statistics.\"\"\"\n        total_seen = len(self.seen_hashes)\n        duplicates = sum(len(sources) - 1 for sources in self.document_index.values())\n        print(f\"Deduplication complete: {total_seen} unique documents, {duplicates} duplicates removed\")\n</code></pre>"},{"location":"developer-guide/creating-steps/#step-settings-and-configuration","title":"Step Settings and Configuration","text":""},{"location":"developer-guide/creating-steps/#environment-variable-integration","title":"Environment Variable Integration","text":"<p>Wurzel automatically maps environment variables to step settings:</p> <pre><code>class APISettings(Settings):\n    \"\"\"Settings for API-based data source.\"\"\"\n    api_key: str  # Maps to API_KEY environment variable\n    base_url: str = \"https://api.example.com\"  # Default value\n    timeout: int = 30\n    max_retries: int = 3\n\n# Environment variables:\n# API_KEY=your_secret_key\n# BASE_URL=https://custom.api.com  (optional override)\n# TIMEOUT=60  (optional override)\n</code></pre>"},{"location":"developer-guide/creating-steps/#nested-configuration","title":"Nested Configuration","text":"<p>For complex configuration structures:</p> <pre><code>class DatabaseConfig(Settings):\n    \"\"\"Database connection configuration.\"\"\"\n    host: str = \"localhost\"\n    port: int = 5432\n    database: str = \"wurzel\"\n    username: str = \"user\"\n    password: str = \"password\"\n\nclass ProcessingConfig(Settings):\n    \"\"\"Processing configuration.\"\"\"\n    batch_size: int = 100\n    parallel_workers: int = 4\n\nclass ComplexStepSettings(Settings):\n    \"\"\"Complex step configuration.\"\"\"\n    database: DatabaseConfig = DatabaseConfig()\n    processing: ProcessingConfig = ProcessingConfig()\n    debug_mode: bool = False\n\n# Environment variables:\n# DATABASE__HOST=production-db.example.com\n# DATABASE__PORT=5433\n# PROCESSING__BATCH_SIZE=200\n# DEBUG_MODE=true\n</code></pre>"},{"location":"developer-guide/creating-steps/#testing-custom-steps","title":"Testing Custom Steps","text":""},{"location":"developer-guide/creating-steps/#unit-testing","title":"Unit Testing","text":"<pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom your_module import MyDatasourceStep, MySettings\n\ndef test_datasource_step():\n    \"\"\"Test the data source step.\"\"\"\n\n    # Create test settings\n    settings = MySettings(data_path=\"./test_data\", file_pattern=\"*.md\")\n\n    # Mock file system\n    with patch('glob.glob') as mock_glob, \\\n         patch('builtins.open', create=True) as mock_open:\n\n        mock_glob.return_value = [\"test1.md\", \"test2.md\"]\n        mock_open.return_value.__enter__.return_value.read.return_value = \"# Test Content\"\n\n        # Test step execution\n        step = MyDatasourceStep()\n        step.settings = settings\n        result = step.run(None)\n\n        assert len(result) == 2\n        assert result[0].content == \"# Test Content\"\n\ndef test_filter_step():\n    \"\"\"Test the document filter step.\"\"\"\n    from your_module import DocumentFilterStep, FilterSettings\n    from wurzel.datacontract import MarkdownDataContract\n\n    # Create test data\n    docs = [\n        MarkdownDataContract(content=\"Short\", source=\"test1\"),\n        MarkdownDataContract(content=\"This is a longer document with enough content\", source=\"test2\"),\n        MarkdownDataContract(content=\"A\" * 200, source=\"test3\"),  # Long enough\n    ]\n\n    # Test filtering\n    step = DocumentFilterStep()\n    step.settings = FilterSettings(min_length=50)\n    result = step.run(docs)\n\n    assert len(result) == 2  # Short document filtered out\n    assert result[0].content == \"This is a longer document with enough content\"\n</code></pre>"},{"location":"developer-guide/creating-steps/#integration-testing","title":"Integration Testing","text":"<pre><code>def test_pipeline_with_custom_steps():\n    \"\"\"Test a complete pipeline using custom steps.\"\"\"\n    from wurzel.utils import WZ\n\n    # Create pipeline\n    source = WZ(MyDatasourceStep)\n    filter_step = WZ(DocumentFilterStep)\n    embedding = WZ(CustomEmbeddingStep)\n\n    source &gt;&gt; filter_step &gt;&gt; embedding\n\n    # Test pipeline structure\n    assert filter_step.inputs == [source]\n    assert embedding.inputs == [filter_step]\n</code></pre>"},{"location":"developer-guide/creating-steps/#best-practices","title":"Best Practices","text":""},{"location":"developer-guide/creating-steps/#error-handling","title":"Error Handling","text":"<pre><code>class RobustProcessingStep(TypedStep[Settings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"Example of robust error handling in steps.\"\"\"\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Process documents with error handling.\"\"\"\n        processed_docs = []\n        errors = []\n\n        for i, doc in enumerate(inpt):\n            try:\n                processed_doc = self._process_document(doc)\n                processed_docs.append(processed_doc)\n            except Exception as e:\n                error_msg = f\"Failed to process document {i}: {str(e)}\"\n                errors.append(error_msg)\n                # Log error but continue processing\n                print(f\"Warning: {error_msg}\")\n\n        if errors and len(errors) &gt; len(inpt) * 0.5:  # More than 50% failed\n            raise RuntimeError(f\"Too many processing errors: {len(errors)}/{len(inpt)}\")\n\n        return processed_docs\n</code></pre>"},{"location":"developer-guide/creating-steps/#resource-management","title":"Resource Management","text":"<pre><code>class ResourceManagedStep(TypedStep[Settings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"Example of proper resource management.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with resource management.\"\"\"\n        self.connection = None\n        self.temp_files = []\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Process with proper resource handling.\"\"\"\n        try:\n            # Your processing logic here\n            return self._process_documents(inpt)\n        except Exception:\n            # Clean up on error\n            self._cleanup_resources()\n            raise\n\n    def finalize(self) -&gt; None:\n        \"\"\"Ensure cleanup happens.\"\"\"\n        self._cleanup_resources()\n\n    def _cleanup_resources(self):\n        \"\"\"Clean up allocated resources.\"\"\"\n        if self.connection:\n            self.connection.close()\n            self.connection = None\n\n        for temp_file in self.temp_files:\n            try:\n                import os\n                os.unlink(temp_file)\n            except OSError:\n                pass\n        self.temp_files.clear()\n</code></pre>"},{"location":"developer-guide/creating-steps/#next-steps","title":"Next Steps","text":"<ul> <li>Understand Data Contracts - Learn about type-safe data exchange</li> <li>Explore Backend Integration - Deploy your custom steps</li> </ul>"},{"location":"developer-guide/creating-steps/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation - Complete TypedStep API reference</li> <li>Built-in Steps - Examples of existing step implementations</li> <li>Testing Guidelines - Best practices for testing steps</li> </ul>"},{"location":"developer-guide/data-contracts/","title":"Data Contracts","text":"<p>Understand Wurzel's type-safe data exchange system that ensures data integrity and enables seamless communication between pipeline steps.</p>"},{"location":"developer-guide/data-contracts/#overview","title":"Overview","text":"<p>Wurzel implements a type-safe pipeline system where data flows between processing steps through strictly defined Data Contracts. These contracts ensure data integrity, enable automatic validation, and provide clear interfaces between pipeline components.</p>"},{"location":"developer-guide/data-contracts/#key-benefits","title":"Key Benefits","text":"<ul> <li>Type Safety: Compile-time and runtime validation</li> <li>Modularity: Interchangeable steps with clear interfaces</li> <li>Persistence: Automatic serialization between steps</li> <li>Scalability: Efficient DataFrame-based bulk processing</li> </ul>"},{"location":"developer-guide/data-contracts/#data-contract-fundamentals","title":"Data Contract Fundamentals","text":""},{"location":"developer-guide/data-contracts/#the-datamodel-interface","title":"The DataModel Interface","text":"<p>All data contracts in Wurzel implement the abstract <code>DataModel</code> interface:</p> <pre><code>from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, TypeVar\n\nT = TypeVar('T', bound='DataModel')\n\nclass DataModel(ABC):\n    \"\"\"\n    Abstract base class for all data models in Wurzel.\n\n    Provides serialization and deserialization capabilities\n    for data exchange between pipeline steps.\n    \"\"\"\n\n    @abstractmethod\n    def save_to_path(self, path: Path) -&gt; None:\n        \"\"\"Save the data model to the specified path.\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def load_from_path(cls: type[T], path: Path) -&gt; T:\n        \"\"\"Load the data model from the specified path.\"\"\"\n        pass\n</code></pre> <p>This interface ensures that all data types can be:</p> <ul> <li>Persisted to disk between pipeline steps</li> <li>Loaded automatically by the next step</li> <li>Validated for type correctness</li> </ul>"},{"location":"developer-guide/data-contracts/#supported-base-data-types","title":"Supported Base Data Types","text":"<p>Wurzel provides two concrete implementations of the <code>DataModel</code> interface:</p>"},{"location":"developer-guide/data-contracts/#pydanticmodel","title":"PydanticModel","text":"<p>For structured objects and metadata:</p> <pre><code>from pydantic import BaseModel\nfrom wurzel.datacontract import PydanticModel\n\nclass DocumentMetadata(PydanticModel):\n    \"\"\"Example of a Pydantic-based data contract.\"\"\"\n    title: str\n    author: str\n    created_date: str\n    tags: list[str] = []\n\n    # Inherited methods for serialization\n    def save_to_path(self, path: Path) -&gt; None:\n        \"\"\"Save as JSON file.\"\"\"\n        with open(path, 'w') as f:\n            f.write(self.model_dump_json())\n\n    @classmethod\n    def load_from_path(cls, path: Path) -&gt; 'DocumentMetadata':\n        \"\"\"Load from JSON file.\"\"\"\n        with open(path, 'r') as f:\n            data = json.load(f)\n        return cls(**data)\n</code></pre> <p>Features:</p> <ul> <li>Serialization: JSON format (<code>.json</code> files)</li> <li>Use cases: Individual documents, metadata, configuration objects</li> <li>Validation: Automatic Pydantic validation</li> </ul>"},{"location":"developer-guide/data-contracts/#dataframe","title":"DataFrame","text":"<p>For bulk data processing:</p> <pre><code>import pandas as pd\nfrom wurzel.datacontract import DataFrame\n\n# Type-safe DataFrame with specific row type\nEmbeddingDataFrame = DataFrame[EmbeddingResult]\n\nclass EmbeddingResult(PydanticModel):\n    \"\"\"Data contract for embedding results.\"\"\"\n    id: str\n    content: str\n    source: str\n    metadata: dict[str, Any]\n    embedding: list[float]\n    embedding_model: str\n</code></pre> <p>Features:</p> <ul> <li>Serialization: Parquet format (<code>.parquet</code> files)</li> <li>Use cases: Large datasets, tabular data, bulk processing</li> <li>Performance: Optimized for high-volume data operations</li> </ul>"},{"location":"developer-guide/data-contracts/#built-in-data-contracts","title":"Built-in Data Contracts","text":""},{"location":"developer-guide/data-contracts/#markdowndatacontract","title":"MarkdownDataContract","text":"<p>The primary contract for document processing pipelines:</p> <pre><code>from wurzel.datacontract import MarkdownDataContract\n\n# Create a markdown document\ndoc = MarkdownDataContract(\n    content=\"# My Document\\n\\nThis is the content.\",\n    source=\"document.md\",\n    metadata={\n        \"author\": \"John Doe\",\n        \"created\": \"2024-01-01\",\n        \"tags\": [\"documentation\", \"markdown\"]\n    }\n)\n\n# Access fields\nprint(doc.content)    # The markdown content\nprint(doc.source)     # Source identifier/path\nprint(doc.metadata)   # Additional metadata dictionary\n</code></pre> <p>Fields:</p> <ul> <li><code>content</code>: The actual markdown text</li> <li><code>source</code>: Source identifier (file path, URL, etc.)</li> <li><code>metadata</code>: Flexible dictionary for additional information</li> </ul> <p>Usage: Document ingestion, text processing, content management</p>"},{"location":"developer-guide/data-contracts/#embeddingresult","title":"EmbeddingResult","text":"<p>For vector embedding data:</p> <pre><code>from wurzel.datacontract import EmbeddingResult\n\n# Create an embedding result\nembedding = EmbeddingResult(\n    id=\"doc_001\",\n    content=\"Original text content\",\n    source=\"source_document.md\",\n    metadata={\"processed_at\": \"2024-01-01T10:00:00Z\"},\n    embedding=[0.1, 0.2, 0.3, ...],  # Vector representation\n    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n</code></pre> <p>Fields:</p> <ul> <li><code>id</code>: Unique identifier for the embedding</li> <li><code>content</code>: Original text that was embedded</li> <li><code>source</code>: Source of the original content</li> <li><code>metadata</code>: Additional context information</li> <li><code>embedding</code>: The actual vector representation</li> <li><code>embedding_model</code>: Model used to generate the embedding</li> </ul> <p>Usage: Vector databases, similarity search, ML pipelines</p>"},{"location":"developer-guide/data-contracts/#creating-custom-data-contracts","title":"Creating Custom Data Contracts","text":""},{"location":"developer-guide/data-contracts/#simple-custom-contract","title":"Simple Custom Contract","text":"<pre><code>from wurzel.datacontract import PydanticModel\nfrom typing import Optional\nfrom datetime import datetime\n\nclass ProductDataContract(PydanticModel):\n    \"\"\"Data contract for product information.\"\"\"\n\n    # Required fields\n    product_id: str\n    name: str\n    price: float\n\n    # Optional fields with defaults\n    description: Optional[str] = None\n    category: str = \"general\"\n    in_stock: bool = True\n\n    # Complex fields\n    tags: list[str] = []\n    attributes: dict[str, str] = {}\n\n    # Computed fields\n    created_at: datetime = datetime.now()\n\n    # Validation\n    @validator('price')\n    def validate_price(cls, v):\n        if v &lt; 0:\n            raise ValueError('Price must be positive')\n        return v\n\n    @validator('product_id')\n    def validate_id_format(cls, v):\n        if not v.startswith('PROD_'):\n            raise ValueError('Product ID must start with PROD_')\n        return v\n\n# Usage in a step\nclass ProductProcessingStep(TypedStep[Settings, list[ProductDataContract], list[ProductDataContract]]):\n    \"\"\"Process product data with type safety.\"\"\"\n\n    def run(self, inpt: list[ProductDataContract]) -&gt; list[ProductDataContract]:\n        processed_products = []\n\n        for product in inpt:\n            # Type-safe access to fields\n            if product.price &gt; 100:\n                product.tags.append(\"premium\")\n\n            # Validation happens automatically\n            processed_products.append(product)\n\n        return processed_products\n</code></pre>"},{"location":"developer-guide/data-contracts/#complex-hierarchical-contract","title":"Complex Hierarchical Contract","text":"<pre><code>from wurzel.datacontract import PydanticModel\nfrom typing import Union, Literal\nfrom enum import Enum\n\nclass DocumentType(str, Enum):\n    \"\"\"Enumeration of document types.\"\"\"\n    ARTICLE = \"article\"\n    MANUAL = \"manual\"\n    FAQ = \"faq\"\n    TUTORIAL = \"tutorial\"\n\nclass AuthorInfo(PydanticModel):\n    \"\"\"Nested contract for author information.\"\"\"\n    name: str\n    email: str\n    organization: Optional[str] = None\n\nclass DocumentSection(PydanticModel):\n    \"\"\"Contract for document sections.\"\"\"\n    title: str\n    content: str\n    level: int = 1  # Heading level\n    subsections: list['DocumentSection'] = []  # Self-referential\n\nclass RichDocumentContract(PydanticModel):\n    \"\"\"Complex document contract with hierarchical structure.\"\"\"\n\n    # Basic information\n    title: str\n    document_type: DocumentType\n    language: str = \"en\"\n\n    # Content structure\n    sections: list[DocumentSection]\n\n    # Metadata\n    author: AuthorInfo\n    created_at: datetime\n    updated_at: Optional[datetime] = None\n\n    # Processing information\n    word_count: Optional[int] = None\n    reading_time_minutes: Optional[float] = None\n\n    # Computed properties\n    @property\n    def total_sections(self) -&gt; int:\n        \"\"\"Count total sections including subsections.\"\"\"\n        def count_sections(sections: list[DocumentSection]) -&gt; int:\n            count = len(sections)\n            for section in sections:\n                count += count_sections(section.subsections)\n            return count\n        return count_sections(self.sections)\n\n    # Custom validation\n    @validator('sections')\n    def validate_sections(cls, v):\n        if not v:\n            raise ValueError('Document must have at least one section')\n        return v\n</code></pre>"},{"location":"developer-guide/data-contracts/#dataframe-based-contracts","title":"DataFrame-based Contracts","text":"<p>For high-volume data processing:</p> <pre><code>import pandas as pd\nfrom wurzel.datacontract import DataFrame, PydanticModel\n\nclass LogEntry(PydanticModel):\n    \"\"\"Single log entry contract.\"\"\"\n    timestamp: datetime\n    level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"]\n    message: str\n    source: str\n    user_id: Optional[str] = None\n    session_id: Optional[str] = None\n    metadata: dict[str, Any] = {}\n\n# Type-safe DataFrame for bulk log processing\nLogDataFrame = DataFrame[LogEntry]\n\nclass LogProcessingStep(TypedStep[Settings, LogDataFrame, LogDataFrame]):\n    \"\"\"Process log entries in bulk.\"\"\"\n\n    def run(self, inpt: LogDataFrame) -&gt; LogDataFrame:\n        \"\"\"Filter and enrich log entries.\"\"\"\n\n        # Convert to pandas DataFrame for processing\n        df = inpt.to_pandas()\n\n        # Bulk operations\n        df = df[df['level'].isin(['WARNING', 'ERROR'])]  # Filter\n        df['processed_at'] = datetime.now()  # Add column\n\n        # Convert back to type-safe DataFrame\n        processed_entries = []\n        for _, row in df.iterrows():\n            entry = LogEntry(\n                timestamp=row['timestamp'],\n                level=row['level'],\n                message=row['message'],\n                source=row['source'],\n                user_id=row.get('user_id'),\n                session_id=row.get('session_id'),\n                metadata=row.get('metadata', {})\n            )\n            processed_entries.append(entry)\n\n        return DataFrame(processed_entries)\n</code></pre>"},{"location":"developer-guide/data-contracts/#data-contract-best-practices","title":"Data Contract Best Practices","text":""},{"location":"developer-guide/data-contracts/#design-guidelines","title":"Design Guidelines","text":"<ol> <li>Keep contracts focused: Each contract should represent a single, well-defined data structure</li> <li>Use descriptive names: Make field names and contract names self-documenting</li> <li>Provide defaults: Use sensible defaults for optional fields</li> <li>Add validation: Use Pydantic validators for data integrity</li> <li>Document everything: Add docstrings and field descriptions</li> </ol> <pre><code>class WellDesignedContract(PydanticModel):\n    \"\"\"\n    Example of a well-designed data contract.\n\n    This contract represents a processed document with\n    quality metrics and processing metadata.\n    \"\"\"\n\n    # Core data (required)\n    document_id: str = Field(..., description=\"Unique document identifier\")\n    content: str = Field(..., description=\"Processed document content\")\n\n    # Metadata (optional with defaults)\n    language: str = Field(\"en\", description=\"Document language code\")\n    processing_version: str = Field(\"1.0\", description=\"Processing pipeline version\")\n\n    # Quality metrics\n    quality_score: float = Field(0.0, ge=0.0, le=1.0, description=\"Document quality score (0-1)\")\n    confidence: float = Field(0.0, ge=0.0, le=1.0, description=\"Processing confidence (0-1)\")\n\n    # Timestamps\n    created_at: datetime = Field(default_factory=datetime.now)\n    processed_at: Optional[datetime] = None\n\n    @validator('document_id')\n    def validate_document_id(cls, v):\n        \"\"\"Ensure document ID follows expected format.\"\"\"\n        if not re.match(r'^DOC_\\d{8}_\\d{6}$', v):\n            raise ValueError('Document ID must match format: DOC_YYYYMMDD_HHMMSS')\n        return v\n</code></pre>"},{"location":"developer-guide/data-contracts/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Choose the right base type:</li> <li>Use <code>PydanticModel</code> for individual objects</li> <li> <p>Use <code>DataFrame</code> for bulk data processing</p> </li> <li> <p>Minimize serialization overhead:</p> </li> <li>Keep contracts as simple as possible</li> <li> <p>Avoid deeply nested structures when not necessary</p> </li> <li> <p>Optimize for your use case:</p> </li> <li>For analytics: Use DataFrame with efficient column types</li> <li>For individual processing: Use PydanticModel with focused fields</li> </ol>"},{"location":"developer-guide/data-contracts/#migration-and-versioning","title":"Migration and Versioning","text":"<p>Handle contract evolution gracefully:</p> <pre><code>from typing import Union\n\nclass DocumentContractV1(PydanticModel):\n    \"\"\"Original document contract.\"\"\"\n    title: str\n    content: str\n    author: str\n\nclass DocumentContractV2(PydanticModel):\n    \"\"\"Enhanced document contract with structured metadata.\"\"\"\n    title: str\n    content: str\n    author: AuthorInfo  # Now a structured object\n    version: int = 2\n\n# Migration function\ndef migrate_document_v1_to_v2(old_doc: DocumentContractV1) -&gt; DocumentContractV2:\n    \"\"\"Migrate from V1 to V2 contract.\"\"\"\n    return DocumentContractV2(\n        title=old_doc.title,\n        content=old_doc.content,\n        author=AuthorInfo(\n            name=old_doc.author,\n            email=\"unknown@example.com\",  # Default for missing data\n            organization=None\n        )\n    )\n\n# Version-aware step\nclass VersionAwareStep(TypedStep[Settings, Union[DocumentContractV1, DocumentContractV2], DocumentContractV2]):\n    \"\"\"Step that handles multiple contract versions.\"\"\"\n\n    def run(self, inpt: Union[DocumentContractV1, DocumentContractV2]) -&gt; DocumentContractV2:\n        if isinstance(inpt, DocumentContractV1):\n            return migrate_document_v1_to_v2(inpt)\n        return inpt\n</code></pre>"},{"location":"developer-guide/data-contracts/#testing-data-contracts","title":"Testing Data Contracts","text":""},{"location":"developer-guide/data-contracts/#unit-testing-contracts","title":"Unit Testing Contracts","text":"<pre><code>import pytest\nfrom datetime import datetime\nfrom your_module import ProductDataContract\n\ndef test_product_contract_creation():\n    \"\"\"Test basic contract creation and validation.\"\"\"\n    product = ProductDataContract(\n        product_id=\"PROD_001\",\n        name=\"Test Product\",\n        price=29.99\n    )\n\n    assert product.product_id == \"PROD_001\"\n    assert product.name == \"Test Product\"\n    assert product.price == 29.99\n    assert product.in_stock is True  # Default value\n\ndef test_product_contract_validation():\n    \"\"\"Test contract validation rules.\"\"\"\n\n    # Test invalid price\n    with pytest.raises(ValueError, match=\"Price must be positive\"):\n        ProductDataContract(\n            product_id=\"PROD_001\",\n            name=\"Test Product\",\n            price=-10.0\n        )\n\n    # Test invalid ID format\n    with pytest.raises(ValueError, match=\"Product ID must start with PROD_\"):\n        ProductDataContract(\n            product_id=\"INVALID_001\",\n            name=\"Test Product\",\n            price=29.99\n        )\n\ndef test_product_contract_serialization():\n    \"\"\"Test contract serialization and deserialization.\"\"\"\n    import tempfile\n    from pathlib import Path\n\n    # Create test product\n    original = ProductDataContract(\n        product_id=\"PROD_001\",\n        name=\"Test Product\",\n        price=29.99,\n        tags=[\"electronics\", \"gadget\"]\n    )\n\n    # Test serialization\n    with tempfile.TemporaryDirectory() as temp_dir:\n        file_path = Path(temp_dir) / \"product.json\"\n        original.save_to_path(file_path)\n\n        # Test deserialization\n        loaded = ProductDataContract.load_from_path(file_path)\n\n        assert loaded.product_id == original.product_id\n        assert loaded.name == original.name\n        assert loaded.price == original.price\n        assert loaded.tags == original.tags\n</code></pre>"},{"location":"developer-guide/data-contracts/#integration-testing-with-steps","title":"Integration Testing with Steps","text":"<pre><code>def test_step_with_custom_contract():\n    \"\"\"Test step using custom data contract.\"\"\"\n    from your_module import ProductProcessingStep, ProductDataContract\n\n    # Create test data\n    products = [\n        ProductDataContract(product_id=\"PROD_001\", name=\"Cheap Item\", price=5.99),\n        ProductDataContract(product_id=\"PROD_002\", name=\"Expensive Item\", price=150.00),\n    ]\n\n    # Test step execution\n    step = ProductProcessingStep()\n    result = step.run(products)\n\n    # Verify processing logic\n    assert len(result) == 2\n    assert \"premium\" not in result[0].tags  # Cheap item\n    assert \"premium\" in result[1].tags      # Expensive item\n</code></pre>"},{"location":"developer-guide/data-contracts/#next-steps","title":"Next Steps","text":"<ul> <li>Create Custom Steps - Build steps that use your custom contracts</li> <li>Explore Backend Integration - Deploy pipelines with custom contracts</li> <li>Review Examples - See real-world contract implementations</li> </ul>"},{"location":"developer-guide/data-contracts/#additional-resources","title":"Additional Resources","text":"<ul> <li>Pydantic Documentation - Learn more about Pydantic features</li> <li>Pandas Documentation - DataFrame operations and optimization</li> <li>API Documentation - Complete data contract API reference</li> </ul>"},{"location":"developer-guide/getting-started/","title":"Getting Started","text":"<p>This guide covers setting up your development environment, understanding the development workflow, and running your first tests with Wurzel.</p>"},{"location":"developer-guide/getting-started/#development-setup","title":"Development Setup","text":""},{"location":"developer-guide/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have completed the installation process. If you're setting up for development, you should have:</p> <ul> <li>Python 3.11 or 3.12</li> <li>Wurzel installed with development dependencies</li> <li>Pre-commit hooks configured</li> </ul>"},{"location":"developer-guide/getting-started/#verify-your-installation","title":"Verify Your Installation","text":"<pre><code># Check that Wurzel is properly installed\npython -c \"import wurzel; print('Wurzel installed successfully')\"\n\n# Verify CLI access\nwurzel --help\n</code></pre>"},{"location":"developer-guide/getting-started/#development-workflow","title":"Development Workflow","text":""},{"location":"developer-guide/getting-started/#code-quality-linting","title":"Code Quality &amp; Linting","text":"<p>Wurzel uses pre-commit hooks to enforce consistent code quality and formatting. These hooks run automatically on every commit to ensure code standards.</p>"},{"location":"developer-guide/getting-started/#set-up-pre-commit-hooks","title":"Set Up Pre-commit Hooks","text":"<p>If you used <code>make install</code>, pre-commit hooks are already configured.</p>"},{"location":"developer-guide/getting-started/#run-linting-manually","title":"Run Linting Manually","text":"<p>You can trigger the linting process manually at any time:</p> <pre><code>make lint\n</code></pre> <p>This runs all configured linters and formatters across the codebase.</p>"},{"location":"developer-guide/getting-started/#running-tests","title":"Running Tests","text":"<p>Before submitting changes, ensure all tests pass:</p> <pre><code># Run the complete test suite\nmake test\n</code></pre>"},{"location":"developer-guide/getting-started/#test-structure","title":"Test Structure","text":"<p>Wurzel's test suite includes:</p> <ul> <li>Unit tests: Testing individual components</li> <li>Integration tests: Testing component interactions</li> <li>End-to-end tests: Testing complete pipeline flows</li> </ul>"},{"location":"developer-guide/getting-started/#documentation","title":"Documentation","text":""},{"location":"developer-guide/getting-started/#local-documentation-development","title":"Local Documentation Development","text":"<p>Preview documentation changes locally:</p> <pre><code># Serve documentation locally (auto-reloads on changes)\nmake documentation\n\n# Build documentation without serving\nmkdocs build\n</code></pre> <p>The documentation will be available at <code>http://127.0.0.1:8000/</code></p>"},{"location":"developer-guide/getting-started/#documentation-structure","title":"Documentation Structure","text":"<p>Wurzel uses MkDocs for documentation management:</p> <ul> <li>Source files: Located in <code>docs/</code></li> <li>Configuration: <code>mkdocs.yml</code></li> <li>Auto-generated docs: Built from docstrings</li> </ul>"},{"location":"developer-guide/getting-started/#development-guidelines","title":"Development Guidelines","text":""},{"location":"developer-guide/getting-started/#commit-strategy","title":"Commit Strategy","text":"<p>Wurzel maintains a clean Git history through a structured commit strategy.</p>"},{"location":"developer-guide/getting-started/#commit-message-format","title":"Commit Message Format","text":"<p>Follow this structure for commit messages:</p> <pre><code>&lt;tag&gt;: &lt;short description&gt;\n\n&lt;longer description (optional)&gt;\n</code></pre>"},{"location":"developer-guide/getting-started/#commit-types","title":"Commit Types","text":"<ul> <li>Breaking Changes: For changes that are not backward-compatible</li> <li>Use tag: <code>breaking</code></li> <li>Features: For new features or enhancements that are backward-compatible</li> <li>Use tags: <code>feat</code>, <code>feature</code></li> <li>Fixes and Improvements: For bug fixes, performance improvements, or small patches</li> <li>Use tags: <code>fix</code>, <code>hotfix</code>, <code>perf</code>, <code>patch</code></li> </ul>"},{"location":"developer-guide/getting-started/#allowed-tags","title":"Allowed Tags","text":"<p>Ensure consistency by using these approved tags:</p> <ul> <li><code>feat</code>, <code>feature</code>, <code>fix</code>, <code>hotfix</code>, <code>perf</code>, <code>patch</code></li> <li><code>build</code>, <code>chore</code>, <code>ci</code>, <code>docs</code>, <code>style</code>, <code>refactor</code>, <code>ref</code>, <code>test</code></li> </ul>"},{"location":"developer-guide/getting-started/#examples","title":"Examples","text":"<pre><code># Good commit messages\ngit commit -m \"feat: add semantic text splitter for German documents\"\ngit commit -m \"fix: resolve memory leak in embedding generation\"\ngit commit -m \"docs: update installation guide with Docker instructions\"\n\n# Bad commit messages\ngit commit -m \"updated stuff\"\ngit commit -m \"fixed bug\"\n</code></pre>"},{"location":"developer-guide/getting-started/#merge-strategy","title":"Merge Strategy","text":"<ul> <li>All commits are squashed when merging into the main branch</li> <li>This maintains a clean, readable project history</li> <li>Focus on meaningful commit messages during development</li> </ul>"},{"location":"developer-guide/getting-started/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"developer-guide/getting-started/#adding-a-new-feature","title":"Adding a New Feature","text":"<ol> <li> <p>Create a feature branch: <pre><code>git checkout -b feat/your-feature-name\n</code></pre></p> </li> <li> <p>Implement your feature following the development guides</p> </li> <li> <p>Add tests for your feature:    <pre><code># Add tests in tests/ directory\npython -m pytest tests/test_your_feature.py\n</code></pre></p> </li> <li> <p>Update documentation if needed</p> </li> <li> <p>Run quality checks: <pre><code>make lint\nmake test\n</code></pre></p> </li> <li> <p>Commit using proper format: <pre><code>git commit -m \"feat: add your feature description\"\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/getting-started/#fixing-a-bug","title":"Fixing a Bug","text":"<ol> <li> <p>Create a fix branch: <pre><code>git checkout -b fix/bug-description\n</code></pre></p> </li> <li> <p>Write a failing test that reproduces the bug</p> </li> <li> <p>Implement the fix</p> </li> <li> <p>Verify the test passes: <pre><code>python -m pytest tests/test_bug_fix.py\n</code></pre></p> </li> <li> <p>Run full test suite: <pre><code>make test\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/getting-started/#working-with-dependencies","title":"Working with Dependencies","text":""},{"location":"developer-guide/getting-started/#adding-new-dependencies","title":"Adding New Dependencies","text":"<ol> <li> <p>Add to pyproject.toml in the appropriate section:    <pre><code>dependencies = [\n    \"existing-package&gt;=1.0.0\",\n    \"new-package&gt;=2.0.0\",\n]\n</code></pre></p> </li> <li> <p>For optional dependencies: <pre><code>[project.optional-dependencies]\nyour-extra = [\"optional-package&gt;=1.0.0\"]\n</code></pre></p> </li> <li> <p>Update installation: <pre><code>make install\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/getting-started/#direct-dependencies","title":"Direct Dependencies","text":"<p>For packages that can't be installed via PyPI (like spaCy models):</p> <ol> <li> <p>Add to DIRECT_REQUIREMENTS.txt: <pre><code>https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl\n</code></pre></p> </li> <li> <p>Document in installation guide if user-facing</p> </li> </ol>"},{"location":"developer-guide/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have your development environment set up:</p> <ol> <li>Build Your First Pipeline - Learn core pipeline concepts</li> <li>Create Custom Steps - Build your own processing components</li> <li>Understand Data Contracts - Learn about type-safe data exchange</li> <li>Explore Backends - Understand deployment options</li> </ol>"},{"location":"developer-guide/getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation - Auto-generated API reference</li> <li>Example Pipelines - Real-world implementation examples</li> </ul>"},{"location":"developer-guide/installation/","title":"Installation &amp; Setup","text":"<p>This comprehensive guide covers installing Wurzel and setting up your development environment, including handling special dependencies and various deployment options.</p>"},{"location":"developer-guide/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"developer-guide/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or 3.12</li> <li>pip (Python package installer)</li> <li>Virtual environment (recommended)</li> </ul>"},{"location":"developer-guide/installation/#quick-install","title":"Quick Install","text":"<p>To get started with Wurzel, install the library using pip:</p> <pre><code>pip install wurzel\n</code></pre>"},{"location":"developer-guide/installation/#development-installation","title":"Development Installation","text":"<p>For development work, we recommend using the provided Makefile which handles all dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/telekom/wurzel.git\ncd wurzel\n\n# Install all dependencies including development tools\nmake install\n</code></pre> <p>This installs:</p> <ul> <li>Core Wurzel library</li> <li>All optional dependencies</li> <li>Development tools (linting, testing, documentation)</li> <li>Pre-commit hooks setup</li> </ul>"},{"location":"developer-guide/installation/#manual-development-setup","title":"Manual Development Setup","text":"<p>If you prefer manual installation:</p> <pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode with all extras\npip install -e .[all,lint,test,docs]\n\n# Install direct dependencies (see below)\npip install -r DIRECT_REQUIREMENTS.txt\n\n# Set up pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"developer-guide/installation/#direct-dependencies","title":"Direct Dependencies","text":"<p>Due to PyPI restrictions on direct dependencies, some components require manual installation. This primarily affects the German spaCy model used for semantic text splitting.</p> <p>\u2139\ufe0f Why Direct Dependencies? PyPI has restrictions on packages that include direct dependencies to external URLs for security reasons. The spaCy German model is hosted on GitHub releases rather than PyPI, requiring manual installation.</p>"},{"location":"developer-guide/installation/#manual-installation","title":"Manual Installation","text":"<p>If you plan to use the semantic text splitting functionality (e.g., <code>SemanticSplitter</code>), you'll need to manually install the German spaCy model:</p> <pre><code>pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl\n</code></pre>"},{"location":"developer-guide/installation/#using-direct_requirementstxt","title":"Using DIRECT_REQUIREMENTS.txt","text":"<p>If you're working with the source code, you can install from the provided requirements file:</p> <pre><code>pip install -r DIRECT_REQUIREMENTS.txt\n</code></pre>"},{"location":"developer-guide/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Wurzel supports various optional features through extras. You can install only what you need for your specific use case.</p>"},{"location":"developer-guide/installation/#vector-database-support","title":"Vector Database Support","text":"<pre><code># For Qdrant vector database\npip install wurzel[qdrant]\n\n# For Milvus vector database\npip install wurzel[milvus]\n</code></pre>"},{"location":"developer-guide/installation/#document-processing","title":"Document Processing","text":"<pre><code># For PDF document processing with Docling\npip install wurzel[docling]\n</code></pre>"},{"location":"developer-guide/installation/#backend-support","title":"Backend Support","text":"<pre><code># For Argo Workflows backend\npip install wurzel[argo]\n</code></pre>"},{"location":"developer-guide/installation/#development-tools","title":"Development Tools","text":"<pre><code># For linting and code quality tools\npip install wurzel[lint]\n\n# For testing framework and tools\npip install wurzel[test]\n\n# For documentation generation\npip install wurzel[docs]\n</code></pre>"},{"location":"developer-guide/installation/#install-everything","title":"Install Everything","text":"<pre><code># Install all optional dependencies\npip install wurzel[all]\n\n# Don't forget the direct dependencies!\npip install -r DIRECT_REQUIREMENTS.txt\n</code></pre>"},{"location":"developer-guide/installation/#docker-installation","title":"Docker Installation","text":"<p>The Docker image includes all dependencies automatically and is the easiest way to get started:</p> <pre><code># Pull the latest image\ndocker pull ghcr.io/telekom/wurzel:latest\n\n# Or build locally\ndocker build -t wurzel .\n</code></pre>"},{"location":"developer-guide/installation/#running-with-docker","title":"Running with Docker","text":"<pre><code>docker run \\\n  -e GIT_USER=wurzel-demo \\\n  -e GIT_MAIL=demo@example.com \\\n  -e DVC_DATA_PATH=/usr/app/data \\\n  -e DVC_FILE=/usr/app/dvc.yaml \\\n  -e WURZEL_PIPELINE=pipelinedemo:pipeline \\\n  ghcr.io/telekom/wurzel:latest\n</code></pre>"},{"location":"developer-guide/installation/#environment-configuration","title":"Environment Configuration","text":"<p>When running Wurzel, several environment variables can be configured to customize behavior:</p>"},{"location":"developer-guide/installation/#git-configuration","title":"Git Configuration","text":"<ul> <li><code>GIT_USER</code>: Git username for repository operations (default: <code>wurzel</code>)</li> <li><code>GIT_MAIL</code>: Git email for repository operations (default: <code>wurzel@example.com</code>)</li> </ul>"},{"location":"developer-guide/installation/#dvc-configuration","title":"DVC Configuration","text":"<ul> <li><code>DVC_DATA_PATH</code>: Path where DVC stores data files (default: <code>/usr/app/dvc-data</code>)</li> <li><code>DVC_FILE</code>: Path to the DVC pipeline definition file (default: <code>/usr/app/dvc.yaml</code>)</li> <li><code>DVC_CACHE_HISTORY_NUMBER</code>: Number of cache versions to keep (default: <code>30</code>)</li> </ul>"},{"location":"developer-guide/installation/#pipeline-configuration","title":"Pipeline Configuration","text":"<ul> <li><code>WURZEL_PIPELINE</code>: Specifies which pipeline to execute (e.g., <code>pipelinedemo:pipeline</code>)</li> </ul>"},{"location":"developer-guide/installation/#monitoring-optional","title":"Monitoring (Optional)","text":"<ul> <li><code>PROMETHEUS__GATEWAY</code>: Prometheus pushgateway URL for metrics</li> </ul> <p>For backend-specific configuration, see:</p> <ul> <li>DVC Backend Configuration</li> <li>Argo Backend Configuration</li> </ul>"},{"location":"developer-guide/installation/#verification","title":"Verification","text":""},{"location":"developer-guide/installation/#verify-basic-installation","title":"Verify Basic Installation","text":"<pre><code># Check Wurzel installation\npython -c \"import wurzel; print('Wurzel installed successfully')\"\n\n# Check CLI is available\nwurzel --help\n</code></pre>"},{"location":"developer-guide/installation/#verify-spacy-model","title":"Verify spaCy Model","text":"<pre><code># Test spaCy model loading\npython -c \"import spacy; nlp = spacy.load('de_core_news_sm'); print('spaCy model loaded successfully')\"\n</code></pre>"},{"location":"developer-guide/installation/#run-tests","title":"Run Tests","text":"<pre><code># Run the test suite\nmake test\n\n# Or manually\npython -m pytest\n</code></pre>"},{"location":"developer-guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer-guide/installation/#common-issues","title":"Common Issues","text":""},{"location":"developer-guide/installation/#spacy-model-issues","title":"spaCy Model Issues","text":"<p>If you encounter issues with the spaCy model:</p> <ol> <li>Verify the model is installed:</li> </ol> <pre><code>python -c \"import spacy; nlp = spacy.load('de_core_news_sm'); print('Model loaded successfully')\"\n</code></pre> <ol> <li>Reinstall the model if needed:</li> </ol> <pre><code>pip uninstall de-core-news-sm\npip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl\n</code></pre>"},{"location":"developer-guide/installation/#environment-issues","title":"Environment Issues","text":"<ul> <li>Python Version: Ensure you're using Python 3.11 or 3.12</li> <li>Virtual Environment: Always use a virtual environment to avoid conflicts:</li> </ul> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install wurzel\n</code></pre>"},{"location":"developer-guide/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<ul> <li>Clean Installation: Start with a fresh virtual environment</li> <li>Upgrade pip: Ensure you have the latest pip version:</li> </ul> <pre><code>python -m pip install --upgrade pip\n</code></pre>"},{"location":"developer-guide/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the examples directory for working configurations</li> <li>Review the API documentation</li> <li>Consult the troubleshooting section in backend documentation</li> <li>Open an issue on the GitHub repository</li> </ol>"},{"location":"developer-guide/installation/#next-steps","title":"Next Steps","text":"<p>Once you have Wurzel installed:</p> <ol> <li>Get Started with Development - Set up your development workflow</li> <li>Build Your First Pipeline - Learn the core concepts</li> <li>Explore Backends - Understand deployment options</li> </ol>"},{"location":"steps/docling/","title":"Docling","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step","title":"<code>docling_step</code>","text":"<p>Note: Known Limitations with EasyOCR (<code>EasyOcrOptions</code>).</p> <ol> <li>Table structure is often lost or misaligned in the OCR output.</li> <li>Spelling inaccuracies are occasionally observed (e.g., \"Verl\u00e4ngern\" \u2192 \"Verl\u00e4ngenng\").</li> <li>URLs are not parsed correctly (e.g., \"www.telekom.de/agb\" \u2192 \"www telekom delagb\").</li> </ol> <p>While investigating EasyOCR issues and testing alternative OCR engines, we observed that some documents produced distorted text with irregular whitespace. This disrupts the natural sentence flow and significantly reduces readability.</p> <p>Example: \"pra kti sche  i nform ati o nen zu  i h rer  fam i l y  card  basi c Li eber   Tel ekom   Kunde, sch\u00f6n,   dass  Si e  si ch  f \u00fcr...\"</p> <p>Despite these limitations, we have decided to proceed with EasyOCR.</p>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step-classes","title":"Classes","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer","title":"<code>CleanMarkdownRenderer</code>","text":"<p>               Bases: <code>HTMLRenderer</code></p> <p>Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up unwanted elements from Markdown input.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>class CleanMarkdownRenderer(HTMLRenderer):\n    \"\"\"Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up\n    unwanted elements from Markdown input.\n    \"\"\"\n\n    @staticmethod\n    def render_html_block(token):\n        \"\"\"Render HTML block tokens and clean up unwanted elements.\n\n        This method removes HTML comments and returns the cleaned HTML content.\n        Remove comments like &lt;!-- image --&gt;\n        \"\"\"\n        soup = BeautifulSoup(token.content, \"html.parser\")\n\n        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n            comment.extract()\n        return soup.decode_contents().strip()\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer-functions","title":"Functions","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer.render_html_block","title":"<code>render_html_block(token)</code>  <code>staticmethod</code>","text":"<p>Render HTML block tokens and clean up unwanted elements.</p> <p>This method removes HTML comments and returns the cleaned HTML content. Remove comments like </p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>@staticmethod\ndef render_html_block(token):\n    \"\"\"Render HTML block tokens and clean up unwanted elements.\n\n    This method removes HTML comments and returns the cleaned HTML content.\n    Remove comments like &lt;!-- image --&gt;\n    \"\"\"\n    soup = BeautifulSoup(token.content, \"html.parser\")\n\n    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n        comment.extract()\n    return soup.decode_contents().strip()\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep","title":"<code>DoclingStep</code>","text":"<p>               Bases: <code>TypedStep[DoclingSettings, None, list[MarkdownDataContract]]</code></p> <p>Step to return local Markdown files with enhanced PDF extraction for German.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>class DoclingStep(TypedStep[DoclingSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Step to return local Markdown files with enhanced PDF extraction for German.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.converter = self.create_converter()\n\n    def create_converter(self) -&gt; DocumentConverter:\n        \"\"\"Create and configure the document converter for PDF and DOCX.\n\n        Returns:\n            DocumentConverter: Configured document converter.\n\n        \"\"\"\n        pipeline_options = PdfPipelineOptions()\n        ocr_options = EasyOcrOptions()\n        pipeline_options.ocr_options = ocr_options\n\n        return DocumentConverter(\n            allowed_formats=self.settings.FORMATS,\n            format_options={\n                InputFormat.PDF: PdfFormatOption(\n                    pipeline_options=pipeline_options,\n                )\n            },\n        )\n\n    @staticmethod\n    def extract_keywords(md_text: str) -&gt; str:\n        \"\"\"Cleans a Markdown string using mistletoe and extracts useful content.\n\n        - Parses and renders the Markdown content into HTML using a custom HTML renderer\n        - Removes unwanted HTML comments and escaped underscores\n        - Extracts the first heading from the content (e.g., `&lt;h1&gt;` to `&lt;h6&gt;`)\n        - Converts the cleaned HTML into plain text\n\n        Args:\n            md_text (str): The raw Markdown input string.\n\n        \"\"\"\n        with MD_RENDER_LOCK, CleanMarkdownRenderer() as renderer:\n            ast = MTDocument(md_text)\n            cleaned = renderer.render(ast).replace(\"\\n\", \"\")\n            soup = BeautifulSoup(cleaned, \"html.parser\")\n            first_heading_tag = soup.find([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n            heading = first_heading_tag.get_text(strip=True) if first_heading_tag else \"\"\n\n        return heading\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        \"\"\"Run the document extraction and conversion process for German PDFs.\n\n        Args:\n            inpt (None): Input parameter (not used).\n\n        Returns:\n            List[MarkdownDataContract]: List of converted Markdown contracts.\n\n        \"\"\"\n        urls = self.settings.URLS\n        contracts = []\n\n        for url in urls:\n            try:\n                converted_contract = self.converter.convert(url)\n                md = converted_contract.document.export_to_markdown(image_placeholder=\"\")\n                keyword = self.extract_keywords(md)\n                contract_instance = {\"md\": md, \"keywords\": \" \".join([self.settings.DEFAULT_KEYWORD, keyword]), \"url\": url}\n                contracts.append(contract_instance)\n\n            except (FileNotFoundError, OSError) as e:\n                log.warning(f\"Failed to verify URL: {url}. Error: {e}\")\n                continue\n\n        return contracts\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep-functions","title":"Functions","text":""},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.create_converter","title":"<code>create_converter()</code>","text":"<p>Create and configure the document converter for PDF and DOCX.</p> <p>Returns:</p> Name Type Description <code>DocumentConverter</code> <code>DocumentConverter</code> <p>Configured document converter.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>def create_converter(self) -&gt; DocumentConverter:\n    \"\"\"Create and configure the document converter for PDF and DOCX.\n\n    Returns:\n        DocumentConverter: Configured document converter.\n\n    \"\"\"\n    pipeline_options = PdfPipelineOptions()\n    ocr_options = EasyOcrOptions()\n    pipeline_options.ocr_options = ocr_options\n\n    return DocumentConverter(\n        allowed_formats=self.settings.FORMATS,\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options,\n            )\n        },\n    )\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.extract_keywords","title":"<code>extract_keywords(md_text)</code>  <code>staticmethod</code>","text":"<p>Cleans a Markdown string using mistletoe and extracts useful content.</p> <ul> <li>Parses and renders the Markdown content into HTML using a custom HTML renderer</li> <li>Removes unwanted HTML comments and escaped underscores</li> <li>Extracts the first heading from the content (e.g., <code>&lt;h1&gt;</code> to <code>&lt;h6&gt;</code>)</li> <li>Converts the cleaned HTML into plain text</li> </ul> <p>Parameters:</p> Name Type Description Default <code>md_text</code> <code>str</code> <p>The raw Markdown input string.</p> required Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>@staticmethod\ndef extract_keywords(md_text: str) -&gt; str:\n    \"\"\"Cleans a Markdown string using mistletoe and extracts useful content.\n\n    - Parses and renders the Markdown content into HTML using a custom HTML renderer\n    - Removes unwanted HTML comments and escaped underscores\n    - Extracts the first heading from the content (e.g., `&lt;h1&gt;` to `&lt;h6&gt;`)\n    - Converts the cleaned HTML into plain text\n\n    Args:\n        md_text (str): The raw Markdown input string.\n\n    \"\"\"\n    with MD_RENDER_LOCK, CleanMarkdownRenderer() as renderer:\n        ast = MTDocument(md_text)\n        cleaned = renderer.render(ast).replace(\"\\n\", \"\")\n        soup = BeautifulSoup(cleaned, \"html.parser\")\n        first_heading_tag = soup.find([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n        heading = first_heading_tag.get_text(strip=True) if first_heading_tag else \"\"\n\n    return heading\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.run","title":"<code>run(inpt)</code>","text":"<p>Run the document extraction and conversion process for German PDFs.</p> <p>Parameters:</p> Name Type Description Default <code>inpt</code> <code>None</code> <p>Input parameter (not used).</p> required <p>Returns:</p> Type Description <code>list[MarkdownDataContract]</code> <p>List[MarkdownDataContract]: List of converted Markdown contracts.</p> Source code in <code>wurzel/steps/docling/docling_step.py</code> <pre><code>def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n    \"\"\"Run the document extraction and conversion process for German PDFs.\n\n    Args:\n        inpt (None): Input parameter (not used).\n\n    Returns:\n        List[MarkdownDataContract]: List of converted Markdown contracts.\n\n    \"\"\"\n    urls = self.settings.URLS\n    contracts = []\n\n    for url in urls:\n        try:\n            converted_contract = self.converter.convert(url)\n            md = converted_contract.document.export_to_markdown(image_placeholder=\"\")\n            keyword = self.extract_keywords(md)\n            contract_instance = {\"md\": md, \"keywords\": \" \".join([self.settings.DEFAULT_KEYWORD, keyword]), \"url\": url}\n            contracts.append(contract_instance)\n\n        except (FileNotFoundError, OSError) as e:\n            log.warning(f\"Failed to verify URL: {url}. Error: {e}\")\n            continue\n\n    return contracts\n</code></pre>"},{"location":"steps/docling/#wurzel.steps.docling.settings","title":"<code>settings</code>","text":"<p>Specific docling settings.</p>"},{"location":"steps/docling/#wurzel.steps.docling.settings-classes","title":"Classes","text":""},{"location":"steps/docling/#wurzel.steps.docling.settings.DoclingSettings","title":"<code>DoclingSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>DoclingSettings is a configuration class that inherits from the base <code>Settings</code> class. It provides customizable settings for document processing.</p> <p>Attributes:</p> Name Type Description <code>FORCE_FULL_PAGE_OCR</code> <code>bool</code> <p>A flag to enforce full-page OCR processing. Defaults to True.</p> <code>FORMATS</code> <code>list[InputFormat]</code> <p>A list of supported input formats for document processing. Supported formats include: - \"docx\" - \"asciidoc\" - \"pptx\" - \"html\" - \"image\" - \"pdf\" - \"md\" - \"csv\" - \"xlsx\" - \"xml_uspto\" - \"xml_jats\" - \"json_docling\"</p> <code>URLS</code> <code>list[str]</code> <p>A list of URLs for additional configuration or resources. Defaults to an empty list.</p> Source code in <code>wurzel/steps/docling/settings.py</code> <pre><code>class DoclingSettings(Settings):\n    \"\"\"DoclingSettings is a configuration class that inherits from the base `Settings` class.\n    It provides customizable settings for document processing.\n\n    Attributes:\n        FORCE_FULL_PAGE_OCR (bool): A flag to enforce full-page OCR processing. Defaults to True.\n        FORMATS (list[InputFormat]): A list of supported input formats for document processing.\n            Supported formats include:\n            - \"docx\"\n            - \"asciidoc\"\n            - \"pptx\"\n            - \"html\"\n            - \"image\"\n            - \"pdf\"\n            - \"md\"\n            - \"csv\"\n            - \"xlsx\"\n            - \"xml_uspto\"\n            - \"xml_jats\"\n            - \"json_docling\"\n        URLS (list[str]): A list of URLs for additional configuration or resources. Defaults to an empty list.\n\n    \"\"\"\n\n    FORCE_FULL_PAGE_OCR: bool = True\n    FORMATS: list[InputFormat] = [\n        \"docx\",\n        \"asciidoc\",\n        \"pptx\",\n        \"html\",\n        \"image\",\n        \"pdf\",\n        \"md\",\n        \"csv\",\n        \"xlsx\",\n        \"xml_uspto\",\n        \"xml_jats\",\n        \"json_docling\",\n    ]\n    URLS: list[str] = []\n    DEFAULT_KEYWORD: str = \"\"\n</code></pre>"},{"location":"steps/duplication/","title":"Deduplication","text":""},{"location":"steps/duplication/#wurzel.steps.duplication","title":"<code>duplication</code>","text":"<p>consists of DVCSteps to embedd files and save them as for example as csv.</p>"},{"location":"steps/duplication/#wurzel.steps.duplication-classes","title":"Classes","text":""},{"location":"steps/duplication/#wurzel.steps.duplication.DropDuplicationStep","title":"<code>DropDuplicationStep</code>","text":"<p>               Bases: <code>TypedStep[DropStettings, list[MarkdownDataContract], list[MarkdownDataContract]]</code></p> <p>SimpleSplitterStep to split Markdown Documents rundimentory in medium size chunks.</p> Source code in <code>wurzel/steps/duplication.py</code> <pre><code>class DropDuplicationStep(TypedStep[DropStettings, list[MarkdownDataContract], list[MarkdownDataContract]]):\n    \"\"\"SimpleSplitterStep to split Markdown Documents rundimentory in medium size chunks.\"\"\"\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n        \"\"\"Executes the split step by processing input markdown files, generating smaller splitted markdown files,\n        by preserving the headline.\n        \"\"\"\n        if self.settings.DROP_BY_FIELDS == [\"*\"]:\n            self.settings.DROP_BY_FIELDS = None\n        df = pd.DataFrame(i.model_dump() for i in inpt)\n        if not df.duplicated(self.settings.DROP_BY_FIELDS).any():\n            return inpt\n\n        filtered = df.drop_duplicates(self.settings.DROP_BY_FIELDS)\n        log.warning(\n            \"Removed duplicates\",\n            extra={\n                \"percentage\": len(filtered) / len(df),\n                \"before\": len(df),\n                \"after\": len(filtered),\n                \"by\": str(self.settings.DROP_BY_FIELDS),\n            },\n        )\n        dumped = filtered.to_dict(orient=\"records\")\n        return [MarkdownDataContract.model_construct(**f) for f in dumped]\n</code></pre>"},{"location":"steps/duplication/#wurzel.steps.duplication.DropDuplicationStep-functions","title":"Functions","text":""},{"location":"steps/duplication/#wurzel.steps.duplication.DropDuplicationStep.run","title":"<code>run(inpt)</code>","text":"<p>Executes the split step by processing input markdown files, generating smaller splitted markdown files, by preserving the headline.</p> Source code in <code>wurzel/steps/duplication.py</code> <pre><code>def run(self, inpt: list[MarkdownDataContract]) -&gt; list[MarkdownDataContract]:\n    \"\"\"Executes the split step by processing input markdown files, generating smaller splitted markdown files,\n    by preserving the headline.\n    \"\"\"\n    if self.settings.DROP_BY_FIELDS == [\"*\"]:\n        self.settings.DROP_BY_FIELDS = None\n    df = pd.DataFrame(i.model_dump() for i in inpt)\n    if not df.duplicated(self.settings.DROP_BY_FIELDS).any():\n        return inpt\n\n    filtered = df.drop_duplicates(self.settings.DROP_BY_FIELDS)\n    log.warning(\n        \"Removed duplicates\",\n        extra={\n            \"percentage\": len(filtered) / len(df),\n            \"before\": len(df),\n            \"after\": len(filtered),\n            \"by\": str(self.settings.DROP_BY_FIELDS),\n        },\n    )\n    dumped = filtered.to_dict(orient=\"records\")\n    return [MarkdownDataContract.model_construct(**f) for f in dumped]\n</code></pre>"},{"location":"steps/duplication/#wurzel.steps.duplication.DropStettings","title":"<code>DropStettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>specify DROP_BY_FIELDS to field.</p> Source code in <code>wurzel/steps/duplication.py</code> <pre><code>class DropStettings(Settings):\n    \"\"\"specify DROP_BY_FIELDS to field.\"\"\"\n\n    DROP_BY_FIELDS: list[str] = [\"md\"]\n</code></pre>"},{"location":"steps/embedding/","title":"Embedding","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step","title":"<code>step</code>","text":"<p>consists of DVCSteps to embedd files and save them as for example as csv.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step-classes","title":"Classes","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step.Embedded","title":"<code>Embedded</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>dict definition of a embedded document.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>class Embedded(TypedDict):\n    \"\"\"dict definition of a embedded document.\"\"\"\n\n    text: str\n    url: str\n    vector: list[float]\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep","title":"<code>EmbeddingStep</code>","text":"<p>               Bases: <code>SimpleSplitterStep</code>, <code>TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingResult]]</code></p> <p>Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult].</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>class EmbeddingStep(\n    SimpleSplitterStep,\n    TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingResult]],\n):\n    \"\"\"Step for consuming list[MarkdownDataContract]\n    and returning DataFrame[EmbeddingResult].\n    \"\"\"\n\n    embedding: HuggingFaceInferenceAPIEmbeddings\n    n_jobs: int\n    markdown: Markdown\n    stopwords: list[str]\n    settings: EmbeddingSettings\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.embedding = self._select_embedding()\n        self.n_jobs = max(1, (os.cpu_count() or 0) - 1)\n        # Inject net output_format into 3rd party library Markdown\n        Markdown.output_formats[\"plain\"] = self.__md_to_plain  # type: ignore[index]\n        self.markdown = Markdown(output_format=\"plain\")  # type: ignore[arg-type]\n        self.markdown.stripTopLevelTags = False\n        self.settingstopwords = self._load_stopwords()\n\n    def _load_stopwords(self) -&gt; list[str]:\n        path = self.settings.STEPWORDS_PATH\n        with path.open(encoding=\"utf-8\") as f:\n            stopwords = [w.strip() for w in f.readlines() if not w.startswith(\";\")]\n        return stopwords\n\n    def _select_embedding(self) -&gt; HuggingFaceInferenceAPIEmbeddings:\n        \"\"\"Selects the embedding model to be used for generating embeddings.\n\n        Returns:\n        -------\n        Embeddings\n            An instance of the Embeddings class.\n\n        \"\"\"\n        return PrefixedAPIEmbeddings(self.settings.API, self.settings.PREFIX_MAP)\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingResult]:\n        \"\"\"Executes the embedding step by processing input markdown files, generating embeddings,\n        and saving them to a CSV file.\n        \"\"\"\n        if len(inpt) == 0:\n            log.info(\"Got empty result in Embedding - Skipping\")\n            return DataFrame[EmbeddingResult]([])\n        splitted_md_rows = self._split_markdown(inpt)\n        rows = []\n        failed = 0\n        for row in tqdm(splitted_md_rows, desc=\"Calculate Embeddings\"):\n            try:\n                rows.append(self._get_embedding(row))\n            except EmbeddingAPIException as err:\n                log.warning(\n                    f\"Skipped because EmbeddingAPIException: {err.message}\",\n                    extra={\"markdown\": str(row)},\n                )\n                failed += 1\n        if failed:\n            log.warning(f\"{failed}/{len(splitted_md_rows)} got skipped\")\n        if failed == len(splitted_md_rows):\n            raise StepFailed(f\"all {len(splitted_md_rows)} embeddings got skipped\")\n        return DataFrame[EmbeddingResult](DataFrame[EmbeddingResult](rows))\n\n    def get_embedding_input_from_document(self, doc: MarkdownDataContract) -&gt; str:\n        \"\"\"Clean the document such that it can be used as input to the embedding model.\n\n        Parameters\n        ----------\n        doc : MarkdownDataContract\n            The document containing the page content in Markdown format.\n\n        Returns:\n        -------\n        str\n            Cleaned text that can be used as input to the embedding model.\n\n        \"\"\"\n        plain_text = self.markdown.convert(doc.md)\n        plain_text = self._replace_link(plain_text)\n\n        return plain_text\n\n    def _get_embedding(self, doc: MarkdownDataContract) -&gt; Embedded:\n        \"\"\"Generates an embedding for a given text and context.\n\n        Parameters\n        ----------\n        d : dict\n            A dictionary containing the text and context for which to generate the embedding.\n\n        Returns:\n        -------\n        dict\n            A dictionary containing the original text, its embedding, and the source URL.\n\n        \"\"\"\n        context = self.get_simple_context(doc.keywords)\n        text = self.get_embedding_input_from_document(doc) if self.settings.CLEAN_MD_BEFORE_EMBEDDING else doc.md\n        vector = self.embedding.embed_query(text)\n        return {\"text\": doc.md, \"vector\": vector, \"url\": doc.url, \"keywords\": context}\n\n    def is_stopword(self, word: str) -&gt; bool:\n        \"\"\"Stopword Detection Function.\"\"\"\n        return word.lower() in self.settingstopwords\n\n    @classmethod\n    def whitespace_word_tokenizer(cls, text: str) -&gt; list[str]:\n        \"\"\"Simple Regex based whitespace word tokenizer.\"\"\"\n        return [x for x in re.split(r\"([.,!?]+)?\\s+\", text) if x]\n\n    def get_simple_context(self, text):\n        \"\"\"Simple function to create a context from a text.\"\"\"\n        tokens = self.whitespace_word_tokenizer(text)\n        filtered_tokens = [token for token in tokens if not self.is_stopword(token)]\n        return \" \".join(filtered_tokens)\n\n    @classmethod\n    def __md_to_plain(cls, element, stream: Optional[StringIO] = None):\n        \"\"\"Converts a markdown element into plain text.\n\n        Parameters\n        ----------\n        element : Element\n            The markdown element to convert.\n        stream : StringIO, optional\n            The stream to which the plain text is written. If None, a new stream is created.\n\n        Returns:\n        -------\n        str\n            The plain text representation of the markdown element.\n\n        \"\"\"\n        if stream is None:\n            stream = StringIO()\n        if element.text:\n            stream.write(element.text)\n        for sub in element:\n            cls.__md_to_plain(sub, stream)\n        if element.tail:\n            stream.write(element.tail)\n        return stream.getvalue()\n\n    @classmethod\n    def _replace_link(cls, text: str):\n        \"\"\"Replaces URLs in the text with a placeholder.\n\n        Parameters\n        ----------\n        text : str\n            The text in which URLs will be replaced.\n\n        Returns:\n        -------\n        str\n            The text with URLs replaced by 'LINK'.\n\n        \"\"\"\n        # Extract URL from a string\n        url_extract_pattern = (\n            \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&amp;\\\\/=]*)\"  # pylint: disable=line-too-long\n        )\n        links = sorted(re.findall(url_extract_pattern, text), key=len, reverse=True)\n        for link in links:\n            text = text.replace(link, \"LINK\")\n        return text\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep-functions","title":"Functions","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.__md_to_plain","title":"<code>__md_to_plain(element, stream=None)</code>  <code>classmethod</code>","text":"<p>Converts a markdown element into plain text.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.__md_to_plain--parameters","title":"Parameters","text":"<p>element : Element     The markdown element to convert. stream : StringIO, optional     The stream to which the plain text is written. If None, a new stream is created.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.__md_to_plain--returns","title":"Returns:","text":"<p>str     The plain text representation of the markdown element.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>@classmethod\ndef __md_to_plain(cls, element, stream: Optional[StringIO] = None):\n    \"\"\"Converts a markdown element into plain text.\n\n    Parameters\n    ----------\n    element : Element\n        The markdown element to convert.\n    stream : StringIO, optional\n        The stream to which the plain text is written. If None, a new stream is created.\n\n    Returns:\n    -------\n    str\n        The plain text representation of the markdown element.\n\n    \"\"\"\n    if stream is None:\n        stream = StringIO()\n    if element.text:\n        stream.write(element.text)\n    for sub in element:\n        cls.__md_to_plain(sub, stream)\n    if element.tail:\n        stream.write(element.tail)\n    return stream.getvalue()\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_embedding_input_from_document","title":"<code>get_embedding_input_from_document(doc)</code>","text":"<p>Clean the document such that it can be used as input to the embedding model.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_embedding_input_from_document--parameters","title":"Parameters","text":"<p>doc : MarkdownDataContract     The document containing the page content in Markdown format.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_embedding_input_from_document--returns","title":"Returns:","text":"<p>str     Cleaned text that can be used as input to the embedding model.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def get_embedding_input_from_document(self, doc: MarkdownDataContract) -&gt; str:\n    \"\"\"Clean the document such that it can be used as input to the embedding model.\n\n    Parameters\n    ----------\n    doc : MarkdownDataContract\n        The document containing the page content in Markdown format.\n\n    Returns:\n    -------\n    str\n        Cleaned text that can be used as input to the embedding model.\n\n    \"\"\"\n    plain_text = self.markdown.convert(doc.md)\n    plain_text = self._replace_link(plain_text)\n\n    return plain_text\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_simple_context","title":"<code>get_simple_context(text)</code>","text":"<p>Simple function to create a context from a text.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def get_simple_context(self, text):\n    \"\"\"Simple function to create a context from a text.\"\"\"\n    tokens = self.whitespace_word_tokenizer(text)\n    filtered_tokens = [token for token in tokens if not self.is_stopword(token)]\n    return \" \".join(filtered_tokens)\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.is_stopword","title":"<code>is_stopword(word)</code>","text":"<p>Stopword Detection Function.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def is_stopword(self, word: str) -&gt; bool:\n    \"\"\"Stopword Detection Function.\"\"\"\n    return word.lower() in self.settingstopwords\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.run","title":"<code>run(inpt)</code>","text":"<p>Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingResult]:\n    \"\"\"Executes the embedding step by processing input markdown files, generating embeddings,\n    and saving them to a CSV file.\n    \"\"\"\n    if len(inpt) == 0:\n        log.info(\"Got empty result in Embedding - Skipping\")\n        return DataFrame[EmbeddingResult]([])\n    splitted_md_rows = self._split_markdown(inpt)\n    rows = []\n    failed = 0\n    for row in tqdm(splitted_md_rows, desc=\"Calculate Embeddings\"):\n        try:\n            rows.append(self._get_embedding(row))\n        except EmbeddingAPIException as err:\n            log.warning(\n                f\"Skipped because EmbeddingAPIException: {err.message}\",\n                extra={\"markdown\": str(row)},\n            )\n            failed += 1\n    if failed:\n        log.warning(f\"{failed}/{len(splitted_md_rows)} got skipped\")\n    if failed == len(splitted_md_rows):\n        raise StepFailed(f\"all {len(splitted_md_rows)} embeddings got skipped\")\n    return DataFrame[EmbeddingResult](DataFrame[EmbeddingResult](rows))\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.whitespace_word_tokenizer","title":"<code>whitespace_word_tokenizer(text)</code>  <code>classmethod</code>","text":"<p>Simple Regex based whitespace word tokenizer.</p> Source code in <code>wurzel/steps/embedding/step.py</code> <pre><code>@classmethod\ndef whitespace_word_tokenizer(cls, text: str) -&gt; list[str]:\n    \"\"\"Simple Regex based whitespace word tokenizer.\"\"\"\n    return [x for x in re.split(r\"([.,!?]+)?\\s+\", text) if x]\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector","title":"<code>step_multivector</code>","text":"<p>consists of DVCSteps to embedd files and save them as for example as csv.</p>"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector-classes","title":"Classes","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep","title":"<code>EmbeddingMultiVectorStep</code>","text":"<p>               Bases: <code>EmbeddingStep</code>, <code>TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingMultiVectorResult]]</code></p> <p>Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingMultiVectorResult].</p> Source code in <code>wurzel/steps/embedding/step_multivector.py</code> <pre><code>class EmbeddingMultiVectorStep(\n    EmbeddingStep,\n    TypedStep[\n        EmbeddingSettings,\n        list[MarkdownDataContract],\n        DataFrame[EmbeddingMultiVectorResult],\n    ],\n):\n    \"\"\"Step for consuming list[MarkdownDataContract]\n    and returning DataFrame[EmbeddingMultiVectorResult].\n    \"\"\"\n\n    def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingMultiVectorResult]:\n        \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects,\n        generating embeddings for each document, and returning the results as a DataFrame.\n\n        Args:\n            inpt (list[MarkdownDataContract]): A list of markdown data contracts to process.\n\n        Returns:\n            DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results.\n\n        Raises:\n            StepFailed: If all input documents fail to generate embeddings.\n\n        Logs:\n            - Warnings for documents skipped due to EmbeddingAPIException.\n            - A summary warning if some or all documents are skipped.\n\n        \"\"\"\n\n        def process_document(doc):\n            try:\n                return self._get_embedding(doc)\n            except EmbeddingAPIException as err:\n                log.warning(\n                    f\"Skipped because EmbeddingAPIException: {err.message}\",\n                    extra={\"markdown\": str(doc)},\n                )\n                return None\n\n        results = Parallel(backend=\"threading\", n_jobs=self.settings.N_JOBS)(delayed(process_document)(doc) for doc in inpt)\n\n        rows = [res for res in results if res is not None]\n        failed = len(results) - len(rows)\n\n        if failed:\n            log.warning(f\"{failed}/{len(inpt)} got skipped\")\n        if failed == len(inpt):\n            raise StepFailed(f\"All {len(inpt)} embeddings got skipped\")\n\n        return DataFrame[EmbeddingMultiVectorResult](DataFrame[EmbeddingMultiVectorResult](rows))\n\n    def _get_embedding(self, doc: MarkdownDataContract) -&gt; _EmbeddedMultiVector:\n        \"\"\"Generates an embedding for a given text and context.\n\n        Parameters\n        ----------\n        d : dict\n            A dictionary containing the text and context for which to generate the embedding.\n\n        Returns:\n        -------\n        dict\n            A dictionary containing the original text, its embedding, and the source URL.\n\n        \"\"\"\n\n        def prepare_plain(document: MarkdownDataContract) -&gt; str:\n            plain_text = self.markdown.convert(document.md)\n            plain_text = self._replace_link(plain_text)\n            return plain_text\n\n        try:\n            splitted_md_rows = self._split_markdown([doc])\n        except SplittException as err:\n            raise EmbeddingAPIException(\"splitting failed\") from err\n        vectors = [self.embedding.embed_query(prepare_plain(split)) for split in splitted_md_rows]\n        if not vectors:\n            raise EmbeddingAPIException(\"Embedding failed for all splits\")\n\n        context = self.get_simple_context(doc.keywords)\n\n        return {\n            \"text\": doc.md,\n            \"vectors\": vectors,\n            \"url\": doc.url,\n            \"keywords\": context,\n            \"splits\": [split.md for split in splitted_md_rows],\n        }\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep-functions","title":"Functions","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep.run","title":"<code>run(inpt)</code>","text":"<p>Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>inpt</code> <code>list[MarkdownDataContract]</code> <p>A list of markdown data contracts to process.</p> required <p>Returns:</p> Type Description <code>DataFrame[EmbeddingMultiVectorResult]</code> <p>DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results.</p> <p>Raises:</p> Type Description <code>StepFailed</code> <p>If all input documents fail to generate embeddings.</p> Logs <ul> <li>Warnings for documents skipped due to EmbeddingAPIException.</li> <li>A summary warning if some or all documents are skipped.</li> </ul> Source code in <code>wurzel/steps/embedding/step_multivector.py</code> <pre><code>def run(self, inpt: list[MarkdownDataContract]) -&gt; DataFrame[EmbeddingMultiVectorResult]:\n    \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects,\n    generating embeddings for each document, and returning the results as a DataFrame.\n\n    Args:\n        inpt (list[MarkdownDataContract]): A list of markdown data contracts to process.\n\n    Returns:\n        DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results.\n\n    Raises:\n        StepFailed: If all input documents fail to generate embeddings.\n\n    Logs:\n        - Warnings for documents skipped due to EmbeddingAPIException.\n        - A summary warning if some or all documents are skipped.\n\n    \"\"\"\n\n    def process_document(doc):\n        try:\n            return self._get_embedding(doc)\n        except EmbeddingAPIException as err:\n            log.warning(\n                f\"Skipped because EmbeddingAPIException: {err.message}\",\n                extra={\"markdown\": str(doc)},\n            )\n            return None\n\n    results = Parallel(backend=\"threading\", n_jobs=self.settings.N_JOBS)(delayed(process_document)(doc) for doc in inpt)\n\n    rows = [res for res in results if res is not None]\n    failed = len(results) - len(rows)\n\n    if failed:\n        log.warning(f\"{failed}/{len(inpt)} got skipped\")\n    if failed == len(inpt):\n        raise StepFailed(f\"All {len(inpt)} embeddings got skipped\")\n\n    return DataFrame[EmbeddingMultiVectorResult](DataFrame[EmbeddingMultiVectorResult](rows))\n</code></pre>"},{"location":"steps/embedding/#wurzel.steps.embedding.settings","title":"<code>settings</code>","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.settings-classes","title":"Classes","text":""},{"location":"steps/embedding/#wurzel.steps.embedding.settings.EmbeddingSettings","title":"<code>EmbeddingSettings</code>","text":"<p>               Bases: <code>SplitterSettings</code></p> <p>EmbeddingSettings is a configuration class for embedding-related settings.</p> <p>Attributes:</p> Name Type Description <code>API</code> <code>Url</code> <p>The API endpoint for embedding operations.</p> <code>NORMALIZE</code> <code>bool</code> <p>A flag indicating whether to normalize embeddings. Defaults to False.</p> <code>BATCH_SIZE</code> <code>int</code> <p>The batch size for processing embeddings. Must be greater than 0. Defaults to 100.</p> <code>TOKEN_COUNT_MIN</code> <code>int</code> <p>The minimum token count for processing. Must be greater than 0. Defaults to 1.</p> <code>TOKEN_COUNT_MAX</code> <code>int</code> <p>The maximum token count for processing. Must be greater than 1. Defaults to 256.</p> <code>TOKEN_COUNT_BUFFER</code> <code>int</code> <p>The buffer size for token count. Must be greater than 0. Defaults to 32.</p> <code>STEPWORDS_PATH</code> <code>Path</code> <p>The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\".</p> <code>N_JOBS</code> <code>int</code> <p>The number of parallel jobs to use. Must be greater than 0. Defaults to 1.</p> <code>PREFIX_MAP</code> <code>dict[Pattern, str]</code> <p>A mapping of regex patterns to string prefixes. This is validated and transformed using the <code>_wrap_validator_model_mapping</code> method.</p> <code>CLEAN_MD_BEFORE_EMBEDDING</code> <code>bool</code> <p>If true Markdown content is cleaned before sending to embedding model. Defaults to False.</p> <code>TOKENIZER_MODEL</code> <code>str</code> <p>Name of tokenizer model measuring token count (tiktoken, spacy, or huggingface). Defaults to \"gpt-3.5-turbo\".</p> <p>Methods:</p> Name Description <code>_wrap_validator_model_mapping</code> <p>dict[str, str], handler): A static method to wrap and validate the model mapping. It converts string regex keys in the input dictionary to compiled regex patterns and applies a handler function to the result.</p> Source code in <code>wurzel/steps/embedding/settings.py</code> <pre><code>class EmbeddingSettings(SplitterSettings):\n    \"\"\"EmbeddingSettings is a configuration class for embedding-related settings.\n\n    Attributes:\n        API (Url): The API endpoint for embedding operations.\n        NORMALIZE (bool): A flag indicating whether to normalize embeddings. Defaults to False.\n        BATCH_SIZE (int): The batch size for processing embeddings. Must be greater than 0. Defaults to 100.\n        TOKEN_COUNT_MIN (int): The minimum token count for processing. Must be greater than 0. Defaults to 1.\n        TOKEN_COUNT_MAX (int): The maximum token count for processing. Must be greater than 1. Defaults to 256.\n        TOKEN_COUNT_BUFFER (int): The buffer size for token count. Must be greater than 0. Defaults to 32.\n        STEPWORDS_PATH (Path): The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\".\n        N_JOBS (int): The number of parallel jobs to use. Must be greater than 0. Defaults to 1.\n        PREFIX_MAP (dict[re.Pattern, str]): A mapping of regex patterns to string prefixes.\n            This is validated and transformed using the `_wrap_validator_model_mapping` method.\n        CLEAN_MD_BEFORE_EMBEDDING (bool): If true Markdown content is cleaned before sending to embedding model. Defaults to False.\n        TOKENIZER_MODEL (str): Name of tokenizer model measuring token count (tiktoken, spacy, or huggingface). Defaults to \"gpt-3.5-turbo\".\n\n    Methods:\n        _wrap_validator_model_mapping(input_dict: dict[str, str], handler):\n            A static method to wrap and validate the model mapping. It converts string regex keys\n            in the input dictionary to compiled regex patterns and applies a handler function to the result.\n\n    \"\"\"\n\n    @staticmethod\n    def _wrap_validator_model_mapping(input_dict: dict[str, str], handler):\n        new_dict = {}\n        for regex, prefix in input_dict.items():\n            if isinstance(regex, str):\n                new_dict[re.compile(regex)] = prefix\n            else:\n                new_dict.update({regex: prefix})\n        return handler(new_dict)\n\n    API: Url\n    NORMALIZE: bool = False\n    BATCH_SIZE: int = Field(100, gt=0)\n    TOKEN_COUNT_MIN: int = Field(1, gt=0)\n    TOKEN_COUNT_MAX: int = Field(256, gt=1)\n    TOKEN_COUNT_BUFFER: int = Field(32, gt=0)\n    STEPWORDS_PATH: Path = Path(\"data/german_stopwords_full.txt\")\n    N_JOBS: int = Field(1, gt=0)\n    PREFIX_MAP: Annotated[dict[re.Pattern, str], WrapValidator(_wrap_validator_model_mapping)] = Field(\n        default={\"e5-\": \"query: \", \"DPR|dpr\": \"\"}\n    )\n    CLEAN_MD_BEFORE_EMBEDDING: bool = True\n    TOKENIZER_MODEL: str = Field(\"gpt-3.5-turbo\", description=\"The tokenizer model to use for splitting documents.\")\n</code></pre>"},{"location":"steps/manual_markdown/","title":"Manual Markdown","text":""},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown","title":"<code>manual_markdown</code>","text":""},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown-classes","title":"Classes","text":""},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown.ManualMarkdownSettings","title":"<code>ManualMarkdownSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Settings fro ManMdstep.</p> Source code in <code>wurzel/steps/manual_markdown.py</code> <pre><code>class ManualMarkdownSettings(Settings):\n    \"\"\"Settings fro ManMdstep.\"\"\"\n\n    FOLDER_PATH: Path\n</code></pre>"},{"location":"steps/manual_markdown/#wurzel.steps.manual_markdown.ManualMarkdownStep","title":"<code>ManualMarkdownStep</code>","text":"<p>               Bases: <code>TypedStep[ManualMarkdownSettings, None, list[MarkdownDataContract]]</code></p> <p>Data Source for md files from a configurable path.</p> Source code in <code>wurzel/steps/manual_markdown.py</code> <pre><code>class ManualMarkdownStep(TypedStep[ManualMarkdownSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Data Source for md files from a configurable path.\"\"\"\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        return [\n            MarkdownDataContract.from_file(fp, url_prefix=self.__class__.__name__ + \"/\") for fp in self.settings.FOLDER_PATH.rglob(\"*.md\")\n        ]\n</code></pre>"},{"location":"steps/milvus/","title":"Milvus","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.step","title":"<code>step</code>","text":"<p>containing the DVCStep sending embedding data into milvus.</p>"},{"location":"steps/milvus/#wurzel.steps.milvus.step-classes","title":"Classes","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.step.MilvusConnectorStep","title":"<code>MilvusConnectorStep</code>","text":"<p>               Bases: <code>TypedStep[MilvusSettings, DataFrame[EmbeddingResult], Result]</code></p> <p>Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.</p> Source code in <code>wurzel/steps/milvus/step.py</code> <pre><code>class MilvusConnectorStep(TypedStep[MilvusSettings, DataFrame[EmbeddingResult], MilvusResult]):  # pragma: no cover\n    \"\"\"Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\"\n\n    milvus_timeout: float = 20.0\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        # milvus stuff passed as environment\n        # because we need to enject them into the DVC step during runtime,\n        # not during DVC pipeline definition time\n        uri = f\"http://{self.settings.HOST}:{self.settings.PORT}\"\n        if not self.settings.PASSWORD or not self.settings.USER:\n            log.warning(\"MILVUS_HOST, MILVUS_USER or MILVUS_PASSWORD for Milvus not provided. Thus running in non-credential Mode\")\n        self.client: MilvusClient = MilvusClient(\n            uri=uri,\n            user=self.settings.USER,\n            password=self.settings.PASSWORD.get_secret_value(),\n            timeout=self.milvus_timeout,\n        )\n        self.collection_index: IndexParams = IndexParams(**self.settings.INDEX_PARAMS)\n        self.collection_history_len = self.settings.COLLECTION_HISTORY_LEN\n\n        self.collection_prefix = self.settings.COLLECTION\n\n    def __del__(self):\n        if getattr(self, \"client\", None):\n            self.client.close()\n\n    def run(self, inpt: DataFrame[EmbeddingResult]) -&gt; MilvusResult:\n        self._insert_embeddings(inpt)\n        try:\n            old = self.__construct_last_collection_name()\n        except NoPreviousCollection:\n            old = \"\"\n        self._retire_collection()\n        return MilvusResult(new=self.__construct_current_collection_name(), old=old)\n\n    def _insert_embeddings(self, data: pd.DataFrame):\n        collection_name = self.__construct_next_collection_name()\n        log.info(f\"Creating milvus collection {collection_name}\")\n        collection_schema = CollectionSchema(\n            fields=[\n                FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=3000),\n                FieldSchema(\n                    name=\"vector\",\n                    dtype=DataType.FLOAT_VECTOR,\n                    dim=len(data[\"vector\"].loc[0]),\n                ),\n                FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=300),\n            ],\n            description=\"Collection for storing Milvus embeddings\",\n        )\n\n        log.info(\"schema created\")\n        self.client.create_collection(collection_name=collection_name, schema=collection_schema)\n        log.info(\"collection created\")\n        log.info(f\"Inserting embedding {len(data)} into collection {collection_name}\")\n        result: dict = self.client.insert(collection_name=collection_name, data=data.to_dict(\"records\"))\n        if result[\"insert_count\"] != len(data):\n            raise StepFailed(\n                f\"Failed to insert df into collection '{collection_name}'.{result['insert_count']}/{len(data)} where successful\"\n            )\n        log.info(f\"Successfully inserted {len(data)} vectors into collection '{collection_name}'\")\n        self.client.create_index(collection_name=collection_name, index_params=self.collection_index)\n        log.info(f\"Successfully craeted index {self.collection_index} into collection '{collection_name}\")\n        self.client.load_collection(collection_name)\n        log.info(f\"Successfully loaded the collection {collection_name}' into collection '{collection_name}'\")\n        try:\n            self.client.release_collection(self.__construct_last_collection_name())\n        except NoPreviousCollection:\n            pass\n        self._update_alias(collection_name)\n\n    def _retire_collection(self):\n        collections_versioned: dict[int, str] = self._get_collection_versions()\n        to_delete = sorted(collections_versioned.keys())[: -self.collection_history_len]\n        if not to_delete:\n            return\n\n        for col_v in to_delete:\n            col = collections_versioned[col_v]\n            log.info(f\"deleting {col} collection caused by retirement\")\n            self.client.drop_collection(col, timeout=self.milvus_timeout)\n\n    def _update_alias(self, collection_name):\n        try:\n            self.client.create_alias(\n                collection_name=collection_name,\n                alias=self.collection_prefix,\n                timeout=self.milvus_timeout,\n            )\n        except MilvusException:\n            self.client.alter_alias(\n                collection_name=collection_name,\n                alias=self.collection_prefix,\n                timeout=self.milvus_timeout,\n            )\n\n    def __construct_next_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections:\n            return f\"{self.collection_prefix}_v1\"\n        previous_version = max(previous_collections.keys())\n        log.info(f\"Found version v{previous_version}\")\n        return f\"{self.collection_prefix}_v{previous_version + 1}\"\n\n    def __construct_last_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections or len(previous_collections) &lt;= 1:\n            raise NoPreviousCollection(f\"Milvus does not contain a previous collection for {self.collection_prefix}\")\n        previous_version = sorted(previous_collections.keys())[-2]\n        log.info(f\"Found previous version v{previous_version}\")\n        return f\"{self.collection_prefix}_v{previous_version}\"\n\n    def __construct_current_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections or len(previous_collections) &lt; 1:\n            raise NoPreviousCollection(f\"Milvus does not contain a previous collection for {self.collection_prefix}\")\n        previous_version = sorted(previous_collections.keys())[-1]\n        log.info(f\"Found previous version v{previous_version}\")\n        return f\"{self.collection_prefix}_v{previous_version}\"\n\n    def _get_collection_versions(self) -&gt; dict[int, str]:\n        previous_collections = self.client.list_collections(timeout=self.milvus_timeout)\n        versioned_collections = {\n            int(previous.split(\"_v\")[-1]): previous for previous in previous_collections if self.collection_prefix in previous\n        }\n        return versioned_collections\n</code></pre>"},{"location":"steps/milvus/#wurzel.steps.milvus.settings","title":"<code>settings</code>","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.settings-classes","title":"Classes","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings","title":"<code>MilvusSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>MilvusSettings is a configuration class for managing settings related to MilvusDB.</p> <p>Attributes:</p> Name Type Description <code>HOST</code> <code>str</code> <p>The hostname or IP address of the Milvus server. Defaults to \"localhost\".</p> <code>PORT</code> <code>int</code> <p>The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530.</p> <code>COLLECTION</code> <code>str</code> <p>The name of the collection in MilvusDB.</p> <code>COLLECTION_HISTORY_LEN</code> <code>int</code> <p>The length of the collection history. Defaults to 10.</p> <code>SEARCH_PARAMS</code> <code>dict</code> <p>Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}.</p> <code>INDEX_PARAMS</code> <code>dict</code> <p>Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\",                     \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}.</p> <code>USER</code> <code>str</code> <p>The username for authentication with MilvusDB.</p> <code>PASSWORD</code> <code>SecretStr</code> <p>The password for authentication with MilvusDB.</p> <code>SECURED</code> <code>bool</code> <p>Indicates whether the connection to MilvusDB is secured. Defaults to False.</p> <p>Methods:</p> Name Description <code>parse_json</code> <p>Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.</p> Source code in <code>wurzel/steps/milvus/settings.py</code> <pre><code>class MilvusSettings(Settings):\n    \"\"\"MilvusSettings is a configuration class for managing settings related to MilvusDB.\n\n    Attributes:\n        HOST (str): The hostname or IP address of the Milvus server. Defaults to \"localhost\".\n        PORT (int): The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530.\n        COLLECTION (str): The name of the collection in MilvusDB.\n        COLLECTION_HISTORY_LEN (int): The length of the collection history. Defaults to 10.\n        SEARCH_PARAMS (dict): Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}.\n        INDEX_PARAMS (dict): Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\",\n                                \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}.\n        USER (str): The username for authentication with MilvusDB.\n        PASSWORD (SecretStr): The password for authentication with MilvusDB.\n        SECURED (bool): Indicates whether the connection to MilvusDB is secured. Defaults to False.\n\n    Methods:\n        parse_json(cls, v): Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.\n\n    \"\"\"\n\n    HOST: str = \"localhost\"\n    PORT: int = Field(19530, gt=0, le=65535)\n    COLLECTION: str\n    COLLECTION_HISTORY_LEN: int = 10\n    SEARCH_PARAMS: dict = {\"metric_type\": \"IP\", \"params\": {}}\n    INDEX_PARAMS: dict = {\n        \"index_type\": \"FLAT\",\n        \"field_name\": \"vector\",\n        \"metric_type\": \"IP\",\n        \"params\": {},\n    }\n    USER: str\n    PASSWORD: SecretStr\n    SECURED: bool = False\n\n    @field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n    @classmethod\n    # pylint: disable-next=R0801\n    def parse_json(cls, v):\n        \"\"\"Validation for json.\"\"\"\n        if isinstance(v, str):\n            return json.loads(v)\n        return v\n</code></pre>"},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings-functions","title":"Functions","text":""},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings.parse_json","title":"<code>parse_json(v)</code>  <code>classmethod</code>","text":"<p>Validation for json.</p> Source code in <code>wurzel/steps/milvus/settings.py</code> <pre><code>@field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n@classmethod\n# pylint: disable-next=R0801\ndef parse_json(cls, v):\n    \"\"\"Validation for json.\"\"\"\n    if isinstance(v, str):\n        return json.loads(v)\n    return v\n</code></pre>"},{"location":"steps/qdrant/","title":"Qdrant","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step","title":"<code>step</code>","text":"<p>containing the DVCStep sending embedding data into Qdrant.</p>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step-classes","title":"Classes","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep","title":"<code>QdrantConnectorStep</code>","text":"<p>               Bases: <code>TypedStep[QdrantSettings, DataFrame[EmbeddingResult], DataFrame[QdrantResult]]</code></p> <p>Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.</p> Source code in <code>wurzel/steps/qdrant/step.py</code> <pre><code>class QdrantConnectorStep(TypedStep[QdrantSettings, DataFrame[EmbeddingResult], DataFrame[QdrantResult]]):\n    \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\"\n\n    _timeout: int = 20\n    s: QdrantSettings\n    client: QdrantClient\n    collection_name: str\n    result_class = QdrantResult\n    vector_key = \"vector\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        # Qdrant stuff passed as environment\n        # because we need to enject them into the DVC step during runtime,\n        # not during DVC pipeline definition time\n        # uri = \":memory:\"\n        log.info(f\"connecting to {self.settings.URI}\")\n        if not self.settings.APIKEY:\n            log.warning(\"QDRANT__APIKEY for Qdrant not provided. Thus running in non-credential Mode\")\n        self.client = QdrantClient(\n            location=self.settings.URI,\n            api_key=self.settings.APIKEY.get_secret_value(),\n            timeout=self._timeout,\n        )\n        self.collection_name = self.__construct_next_collection_name()\n        self.id_iter = self.__id_gen()\n\n    def __del__(self):\n        if getattr(self, \"client\", None):\n            self.client.close()\n\n    def finalize(self) -&gt; None:\n        self._create_indices()\n        self._update_alias()\n        self._retire_collections()\n        return super().finalize()\n\n    def __id_gen(self):\n        i = 0\n        while True:\n            i += 1\n            yield i\n\n    def run(self, inpt: DataFrame[EmbeddingResult]) -&gt; DataFrame[QdrantResult]:\n        if not self.client.collection_exists(self.collection_name):\n            self._create_collection(len(inpt[\"vector\"].loc[0]))\n        df_result = self._insert_embeddings(inpt)\n        return df_result\n\n    def _create_collection(self, size: int):\n        log.debug(f\"Creating Qdrant collection {self.collection_name}\")\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(size=size, distance=self.settings.DISTANCE),\n            replication_factor=self.settings.REPLICATION_FACTOR,\n        )\n\n    def _get_entry_payload(self, row: dict[str, object]) -&gt; dict[str, object]:\n        \"\"\"Create the payload for the entry.\"\"\"\n        payload = {\n            \"url\": row[\"url\"],\n            \"text\": row[\"text\"],\n            **self.get_available_hashes(row[\"text\"]),\n            \"keywords\": row[\"keywords\"],\n            \"history\": str(step_history.get()),\n        }\n        return payload\n\n    def _create_point(self, row: dict) -&gt; models.PointStruct:\n        \"\"\"Creates a Qdrant PointStruct object from a given row dictionary.\n\n        Args:\n            row (dict): A dictionary representing a data entry, expected to contain at least the vector data under `self.vector_key`.\n\n        Returns:\n            models.PointStruct: An instance of PointStruct with a unique id, vector, and payload extracted from the row.\n\n        Raises:\n            KeyError: If the required vector key is not present in the row.\n\n        \"\"\"\n        payload = self._get_entry_payload(row)\n\n        return models.PointStruct(\n            id=next(self.id_iter),  # type: ignore[arg-type]\n            vector=row[self.vector_key],\n            payload=payload,\n        )\n\n    def _upsert_points(self, points: list[models.PointStruct]):\n        \"\"\"Inserts a list of points into the Qdrant collection in batches.\n\n        Args:\n            points (list[models.PointStruct]): The list of point structures to upsert into the collection.\n\n        Raises:\n            StepFailed: If any batch fails to be inserted into the collection.\n\n        Logs:\n            Logs a message for each successfully inserted batch, including the collection name and number of points.\n\n        \"\"\"\n        for point_chunk in _batch(points, self.settings.BATCH_SIZE):\n            operation_info = self.client.upsert(\n                collection_name=self.collection_name,\n                wait=True,\n                points=point_chunk,\n            )\n            if operation_info.status != \"completed\":\n                raise StepFailed(f\"Failed to insert df chunk into collection '{self.collection_name}' {operation_info}\")\n            log.info(\n                \"Successfully inserted vector_chunk\",\n                extra={\"collection\": self.collection_name, \"count\": len(point_chunk)},\n            )\n\n    def _build_result_dataframe(self, points: list[models.PointStruct]):\n        \"\"\"Constructs a DataFrame from a list of PointStruct objects.\n\n        Each PointStruct's payload is unpacked into the resulting dictionary, along with its vector, collection name, and ID.\n        The resulting list of dictionaries is used to create a DataFrame of the specified result_class.\n\n        Args:\n            points (list[models.PointStruct]): A list of PointStruct objects containing payload, vector, and id information.\n\n        \"\"\"\n        result_data = [\n            {\n                **entry.payload,\n                self.vector_key: entry.vector,\n                \"collection\": self.collection_name,\n                \"id\": entry.id,\n            }\n            for entry in points\n        ]\n        return DataFrame[self.result_class](result_data)\n\n    def _insert_embeddings(self, data: DataFrame[EmbeddingResult]):\n        log.info(\"Inserting embeddings\", extra={\"count\": len(data), \"collection\": self.collection_name})\n\n        points = [self._create_point(row) for _, row in data.iterrows()]\n\n        self._upsert_points(points)\n\n        return self._build_result_dataframe(points)\n\n    def _create_indices(self):\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"keywords\",\n            field_schema=models.TextIndexParams(\n                type=models.TextIndexType.TEXT,\n                tokenizer=models.TokenizerType.WHITESPACE,\n            ),\n        )\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"url\",\n            field_schema=models.TextIndexParams(\n                type=models.TextIndexType.TEXT,\n                tokenizer=models.TokenizerType.PREFIX,\n                min_token_len=3,\n            ),\n        )\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"text\",\n            field_schema=models.TextIndexParams(\n                type=models.TextIndexType.TEXT,\n                tokenizer=models.TokenizerType.MULTILINGUAL,\n            ),\n        )\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"history\",\n            field_schema=models.TextIndexParams(type=models.TextIndexType.TEXT, tokenizer=models.TokenizerType.WORD),\n        )\n\n    def _retire_collections(self):\n        collections_versioned: dict[int, str] = self._get_collection_versions()\n        to_delete = list(collections_versioned.keys())[: -self.settings.COLLECTION_HISTORY_LEN]\n        if not to_delete:\n            return\n\n        for col_v in to_delete:\n            col = collections_versioned[col_v]\n            log.info(f\"deleting {col} collection caused by retirement\")\n            self.client.delete_collection(col)\n\n    def _update_alias(self):\n        success = self.client.update_collection_aliases(\n            change_aliases_operations=[\n                models.CreateAliasOperation(\n                    create_alias=models.CreateAlias(\n                        collection_name=self.collection_name,\n                        alias_name=self.settings.COLLECTION,\n                    )\n                )\n            ]\n        )\n        if not success:\n            raise CustomQdrantException(\"Alias Update failed\")\n\n    def __construct_next_collection_name(self) -&gt; str:\n        previous_collections = self._get_collection_versions()\n        if not previous_collections:\n            return f\"{self.settings.COLLECTION}_v1\"\n        previous_version = max(previous_collections.keys())\n        log.info(f\"Found version v{previous_version}\")\n        return f\"{self.settings.COLLECTION}_v{previous_version + 1}\"\n\n    def _get_collection_versions(self) -&gt; dict[int, str]:\n        previous_collections = self.client.get_collections().collections\n        versioned_collections = {\n            int(previous.name.split(\"_v\")[-1]): previous.name\n            for previous in previous_collections\n            if f\"{self.settings.COLLECTION}_v\" in previous.name\n        }\n        return dict(sorted(versioned_collections.items()))\n\n    @staticmethod\n    def get_available_hashes(text: str, encoding: str = \"utf-8\") -&gt; dict:\n        \"\"\"Compute `n` hashes for a given input text based.\n        The number `n` depends on the optionally installed python libs.\n        For now only TLSH (Trend Micro Locality Sensitive Hash) is supported\n        ## TLSH\n        Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons.\n\n        Args:\n            text (str): Input text\n            encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\".\n\n        Returns:\n            dict[str, str]: keys: `text_&lt;algo&gt;_hash` hash as string ! Dict might be empty!\n\n        \"\"\"\n        hashes = {}\n        encoded_text = text.encode(encoding)\n        if HAS_TLSH:\n            # pylint: disable=no-name-in-module, import-outside-toplevel\n            from tlsh import hash as tlsh_hash\n\n            hashes[\"text_tlsh_hash\"] = tlsh_hash(encoded_text)\n        hashes[\"text_sha256_hash\"] = sha256(encoded_text).hexdigest()\n        return hashes\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep-functions","title":"Functions","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep.get_available_hashes","title":"<code>get_available_hashes(text, encoding='utf-8')</code>  <code>staticmethod</code>","text":"<p>Compute <code>n</code> hashes for a given input text based. The number <code>n</code> depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported</p>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep.get_available_hashes--tlsh","title":"TLSH","text":"<p>Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <code>encoding</code> <code>str</code> <p>Input text will encoded to bytes using this encoding. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, str]: keys: <code>text_&lt;algo&gt;_hash</code> hash as string ! Dict might be empty!</p> Source code in <code>wurzel/steps/qdrant/step.py</code> <pre><code>@staticmethod\ndef get_available_hashes(text: str, encoding: str = \"utf-8\") -&gt; dict:\n    \"\"\"Compute `n` hashes for a given input text based.\n    The number `n` depends on the optionally installed python libs.\n    For now only TLSH (Trend Micro Locality Sensitive Hash) is supported\n    ## TLSH\n    Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons.\n\n    Args:\n        text (str): Input text\n        encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\".\n\n    Returns:\n        dict[str, str]: keys: `text_&lt;algo&gt;_hash` hash as string ! Dict might be empty!\n\n    \"\"\"\n    hashes = {}\n    encoded_text = text.encode(encoding)\n    if HAS_TLSH:\n        # pylint: disable=no-name-in-module, import-outside-toplevel\n        from tlsh import hash as tlsh_hash\n\n        hashes[\"text_tlsh_hash\"] = tlsh_hash(encoded_text)\n    hashes[\"text_sha256_hash\"] = sha256(encoded_text).hexdigest()\n    return hashes\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector","title":"<code>step_multi_vector</code>","text":"<p>containing the DVCStep sending embedding data into Qdrant.</p>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector-classes","title":"Classes","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector.QdrantConnectorMultiVectorStep","title":"<code>QdrantConnectorMultiVectorStep</code>","text":"<p>               Bases: <code>QdrantConnectorStep</code>, <code>TypedStep[QdrantSettings, DataFrame[EmbeddingMultiVectorResult], DataFrame[QdrantMultiVectorResult]]</code></p> <p>Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.</p> Source code in <code>wurzel/steps/qdrant/step_multi_vector.py</code> <pre><code>class QdrantConnectorMultiVectorStep(\n    QdrantConnectorStep,\n    TypedStep[\n        QdrantSettings,\n        DataFrame[EmbeddingMultiVectorResult],\n        DataFrame[QdrantMultiVectorResult],\n    ],\n):\n    \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\"\n\n    vector_key = \"vectors\"\n    result_class = QdrantMultiVectorResult\n\n    def _create_collection(self, size: int):\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(\n                size=size,\n                distance=self.settings.DISTANCE,\n                multivector_config=models.MultiVectorConfig(comparator=models.MultiVectorComparator.MAX_SIM),\n            ),\n            replication_factor=self.settings.REPLICATION_FACTOR,\n        )\n\n    def run(self, inpt: DataFrame[EmbeddingMultiVectorResult]) -&gt; DataFrame[QdrantMultiVectorResult]:\n        log.debug(f\"Creating Qdrant collection {self.collection_name}\")\n        if not self.client.collection_exists(self.collection_name):\n            self._create_collection(len(inpt[\"vectors\"].loc[0][0]))\n        df_result = self._insert_embeddings(inpt)\n        return df_result\n\n    def _get_entry_payload(self, row: dict[str, object]) -&gt; dict[str, object]:\n        \"\"\"Create the payload for the entry.\"\"\"\n        payload = super()._get_entry_payload(row)\n        payload[\"splits\"] = row[\"splits\"]\n        return payload\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings","title":"<code>settings</code>","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings-classes","title":"Classes","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings","title":"<code>QdrantSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>QdrantSettings is a configuration class for managing settings related to the Qdrant database.</p> <p>Attributes:</p> Name Type Description <code>DISTANCE</code> <code>Distance</code> <p>The distance metric to be used, default is Distance.DOT.</p> <code>URI</code> <code>str</code> <p>The URI for the Qdrant database, default is \"http://localhost:6333\".</p> <code>COLLECTION</code> <code>str</code> <p>The name of the collection in the Qdrant database.</p> <code>COLLECTION_HISTORY_LEN</code> <code>int</code> <p>The length of the collection history, default is 10.</p> <code>SEARCH_PARAMS</code> <code>dict</code> <p>Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}.</p> <code>INDEX_PARAMS</code> <code>dict</code> <p>Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\".</p> <code>APIKEY</code> <code>SecretStr</code> <p>The API key for authentication, default is an empty SecretStr.</p> <code>REPLICATION_FACTOR</code> <code>int</code> <p>The replication factor for the database, default is 3, must be greater than 0.</p> <code>BATCH_SIZE</code> <code>int</code> <p>The batch size for operations, default is 1024, must be greater than 0.</p> <p>Methods:</p> Name Description <code>parse_json</code> <p>Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.</p> Source code in <code>wurzel/steps/qdrant/settings.py</code> <pre><code>class QdrantSettings(Settings):\n    \"\"\"QdrantSettings is a configuration class for managing settings related to the Qdrant database.\n\n    Attributes:\n        DISTANCE (Distance): The distance metric to be used, default is Distance.DOT.\n        URI (str): The URI for the Qdrant database, default is \"http://localhost:6333\".\n        COLLECTION (str): The name of the collection in the Qdrant database.\n        COLLECTION_HISTORY_LEN (int): The length of the collection history, default is 10.\n        SEARCH_PARAMS (dict): Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}.\n        INDEX_PARAMS (dict): Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\".\n        APIKEY (SecretStr): The API key for authentication, default is an empty SecretStr.\n        REPLICATION_FACTOR (int): The replication factor for the database, default is 3, must be greater than 0.\n        BATCH_SIZE (int): The batch size for operations, default is 1024, must be greater than 0.\n\n    Methods:\n        parse_json(v):\n            Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS.\n    \"\"\"\n\n    DISTANCE: Distance = Distance.DOT\n    URI: str = \"http://localhost:6333\"\n    COLLECTION: str\n    COLLECTION_HISTORY_LEN: int = 10\n    SEARCH_PARAMS: dict = {\"metric_type\": \"IP\", \"params\": {}}\n    INDEX_PARAMS: dict = {\n        \"index_type\": \"FLAT\",\n        \"field_name\": \"vector\",\n        \"distance\": \"Dot\",\n        \"params\": {},\n    }\n    APIKEY: SecretStr = SecretStr(\"\")\n    REPLICATION_FACTOR: int = Field(default=3, gt=0)\n    BATCH_SIZE: int = Field(default=1024, gt=0)\n\n    @field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n    @classmethod\n    def parse_json(cls, v):\n        \"\"\"Validation for json.\"\"\"\n        if isinstance(v, str):\n            return json.loads(v)\n        return v\n</code></pre>"},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings-functions","title":"Functions","text":""},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings.parse_json","title":"<code>parse_json(v)</code>  <code>classmethod</code>","text":"<p>Validation for json.</p> Source code in <code>wurzel/steps/qdrant/settings.py</code> <pre><code>@field_validator(\"SEARCH_PARAMS\", \"INDEX_PARAMS\", mode=\"before\")\n@classmethod\ndef parse_json(cls, v):\n    \"\"\"Validation for json.\"\"\"\n    if isinstance(v, str):\n        return json.loads(v)\n    return v\n</code></pre>"},{"location":"steps/scraperapi/","title":"ScraperAPI","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step","title":"<code>step</code>","text":"<p>interacts with the scraperAPI service and converts the retrieved Documents to Markdown.</p>"},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step-classes","title":"Classes","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step.ScraperAPIStep","title":"<code>ScraperAPIStep</code>","text":"<p>               Bases: <code>TypedStep[ScraperAPISettings, list[UrlItem], list[MarkdownDataContract]]</code></p> <p>ScraperAPIStep uses the ScraperAPI service to srape the html by the given url through list[UrlItem]. this html gets filtered and transformed to MarkdownDataContract.</p> Source code in <code>wurzel/steps/scraperapi/step.py</code> <pre><code>class ScraperAPIStep(TypedStep[ScraperAPISettings, list[UrlItem], list[MarkdownDataContract]]):\n    \"\"\"ScraperAPIStep uses the ScraperAPI service to srape the html by the given url through list[UrlItem].\n    this html gets filtered and transformed to MarkdownDataContract.\n    \"\"\"\n\n    def run(self, inpt: list[UrlItem]) -&gt; list[MarkdownDataContract]:\n        def fetch_and_process(url_item: UrlItem, recursion_depth=0):\n            session = requests.Session()\n            retries = Retry(\n                total=self.settings.RETRY, backoff_factor=0.1, raise_on_status=False, status_forcelist=[403, 500, 502, 503, 504]\n            )\n            session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n            payload = {\n                \"api_key\": self.settings.TOKEN.get_secret_value(),\n                \"url\": url_item.url,\n                \"device_type\": self.settings.DEVICE_TYPE,\n                \"follow_redirect\": str(self.settings.FOLLOW_REDIRECT).lower(),\n                \"wait_for_selector\": self.settings.WAIT_FOR_SELECTOR,\n                \"country_code\": self.settings.COUNTRY_CODE,\n                \"render\": str(self.settings.RENDER).lower(),\n                \"premium\": str(self.settings.PREMIUM).lower(),\n                \"ultra_premium\": str(self.settings.ULTRA_PREMIUM).lower(),\n                \"screenshot\": str(self.settings.SCREENSHOT).lower(),\n                \"max_cost\": str(self.settings.MAX_COST),\n            }\n            try:\n                r = None  # for short error handling\n                r = session.get(self.settings.API, params=payload, timeout=self.settings.TIMEOUT)\n                r.raise_for_status()\n            except requests.exceptions.ReadTimeout:\n                log.warning(\n                    \"Crawling failed due to timeout\",\n                    extra={\"url\": url_item.url},\n                )\n                return None\n            except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError):\n                log.warning(\n                    \"Crawling failed\",\n                    extra={\"url\": url_item.url, \"status\": r.status_code if r else None, \"retries\": self.settings.RETRY},\n                )\n                return None\n\n            try:\n                md = to_markdown(self._filter_body(r.text), self.settings.HTML2MD_SETTINGS)\n            except (KeyError, IndexError):\n                if recursion_depth &gt; self.settings.RETRY:\n                    log.warning(\"xpath retry failed\", extra={\"filter\": self.settings.XPATH, \"url\": url_item.url})\n                    return None\n                log.warning(\n                    \"website does not have the searched xpath, retrying\", extra={\"filter\": self.settings.XPATH, \"url\": url_item.url}\n                )\n                return fetch_and_process(url_item, recursion_depth=recursion_depth + 1)\n\n            progress_bar.update(1)\n            return MarkdownDataContract(md=md, url=url_item.url, keywords=url_item.title)\n\n        with tqdm(total=len(inpt), desc=\"Processing URLs\") as progress_bar:\n            results = Parallel(n_jobs=self.settings.CONCURRENCY_NUM, backend=\"threading\")(delayed(fetch_and_process)(item) for item in inpt)\n\n        filtered_results = [res for res in results if res]\n        if not filtered_results:\n            raise StepFailed(\"no results from scraperAPI\")\n\n        return filtered_results\n\n    def __init__(self) -&gt; None:\n        logging.getLogger(\"urllib3\").setLevel(\"ERROR\")\n        super().__init__()\n\n    def finalize(self) -&gt; None:\n        logging.getLogger(\"urllib3\").setLevel(\"WARNING\")\n\n        return super().finalize()\n\n    def _filter_body(self, html: str) -&gt; str:\n        tree: lxml.html = lxml.html.fromstring(html)\n        tree = tree.xpath(self.settings.XPATH)[0]\n        return html2str(tree)\n</code></pre>"},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.step-functions","title":"Functions","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.settings","title":"<code>settings</code>","text":"<p>interacts with the scraperAPI service and converts the retrieved Documents to Markdown.</p>"},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.settings-classes","title":"Classes","text":""},{"location":"steps/scraperapi/#wurzel.steps.scraperapi.settings.ScraperAPISettings","title":"<code>ScraperAPISettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Settings of ScraperAPIStep. Mainly the list of https://docs.scraperapi.com/python/credits-and-requests.</p> Source code in <code>wurzel/steps/scraperapi/settings.py</code> <pre><code>class ScraperAPISettings(Settings):\n    \"\"\"Settings of ScraperAPIStep. Mainly the list of https://docs.scraperapi.com/python/credits-and-requests.\"\"\"\n\n    API: str = \"https://api.scraperapi.com/\"\n    RETRY: int = Field(ge=0, default=5)\n    TOKEN: SecretStr\n    TIMEOUT: int = 61.0\n    XPATH: str = \"//main\"\n    CONCURRENCY_NUM: int = Field(gt=0, default=1)\n    DEVICE_TYPE: str = \"desktop\"\n    FOLLOW_REDIRECT: bool = True\n    WAIT_FOR_SELECTOR: str = \"#cookies-notification-accept-cookie\"\n    COUNTRY_CODE: str = \"en\"\n    RENDER: bool = True\n    PREMIUM: bool = False\n    ULTRA_PREMIUM: bool = False\n    SCREENSHOT: bool = False\n    MAX_COST: int = Field(gt=0, default=30)\n    HTML2MD_SETTINGS: MarkdownConverterSettings = Field(\n        default_factory=MarkdownConverterSettings, description=\"Settings for the Markdown converter.\"\n    )\n</code></pre>"},{"location":"steps/sftp/","title":"SFTP Manual Markdown","text":""},{"location":"steps/sftp/#wurzel.steps.sftp","title":"<code>sftp</code>","text":""},{"location":"steps/sftp/#wurzel.steps.sftp-classes","title":"Classes","text":""},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownSettings","title":"<code>SFTPManualMarkdownSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>Settings for SFTP Manual Markdown Step.</p> <p>These settings configure the SFTP connection and file retrieval parameters. All settings can be provided via environment variables with the prefix SFTPMANUALMARKDOWNSTEP__ (e.g., SFTPMANUALMARKDOWNSTEP__HOST=sftp.example.com)</p> Source code in <code>wurzel/steps/sftp/sftp_manual_markdown.py</code> <pre><code>class SFTPManualMarkdownSettings(Settings):\n    \"\"\"Settings for SFTP Manual Markdown Step.\n\n    These settings configure the SFTP connection and file retrieval parameters.\n    All settings can be provided via environment variables with the prefix\n    SFTPMANUALMARKDOWNSTEP__ (e.g., SFTPMANUALMARKDOWNSTEP__HOST=sftp.example.com)\n    \"\"\"\n\n    HOST: str = Field(..., description=\"SFTP server hostname or IP address\")\n    PORT: int = Field(22, description=\"SFTP server port\")\n    USERNAME: str = Field(..., description=\"SFTP username\")\n    PASSWORD: SecretStr = SecretStr(\"\")\n    PRIVATE_KEY_PATH: Optional[Path] = Field(None, description=\"Path to SSH private key file\")\n    PRIVATE_KEY_PASSPHRASE: SecretStr = SecretStr(\"\")\n    REMOTE_PATH: str = Field(..., description=\"Remote path on SFTP server to search for .md files\")\n    RECURSIVE: bool = Field(True, description=\"Whether to search recursively for .md files\")\n    TIMEOUT: float = Field(30.0, description=\"Connection timeout in seconds\")\n</code></pre>"},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownStep","title":"<code>SFTPManualMarkdownStep</code>","text":"<p>               Bases: <code>TypedStep[SFTPManualMarkdownSettings, None, list[MarkdownDataContract]]</code></p> <p>Data Source for Markdown files from an SFTP server.</p> <p>This step connects to an SFTP server using Paramiko and retrieves all Markdown (.md) files from the specified remote path. It works similarly to ManualMarkdownStep but loads files from a remote SFTP server instead of the local filesystem.</p> <p>Features: - Supports password and key-based authentication - Recursive directory traversal - Automatic connection management - Preserves file metadata</p> <p>Example usage: <pre><code>from wurzel.steps.sftp import SFTPManualMarkdownStep, SFTPManualMarkdownSettings\n\nsettings = SFTPManualMarkdownSettings(HOST=\"sftp.example.com\", USERNAME=\"user\", PASSWORD=\"password\", REMOTE_PATH=\"/documents\")\nstep = SFTPManualMarkdownStep(settings=settings)\nmarkdown_docs = step.run(None)\n</code></pre></p> Source code in <code>wurzel/steps/sftp/sftp_manual_markdown.py</code> <pre><code>class SFTPManualMarkdownStep(TypedStep[SFTPManualMarkdownSettings, None, list[MarkdownDataContract]]):\n    \"\"\"Data Source for Markdown files from an SFTP server.\n\n    This step connects to an SFTP server using Paramiko and retrieves all Markdown (.md) files\n    from the specified remote path. It works similarly to ManualMarkdownStep but loads files\n    from a remote SFTP server instead of the local filesystem.\n\n    Features:\n    - Supports password and key-based authentication\n    - Recursive directory traversal\n    - Automatic connection management\n    - Preserves file metadata\n\n    Example usage:\n    ```python\n    from wurzel.steps.sftp import SFTPManualMarkdownStep, SFTPManualMarkdownSettings\n\n    settings = SFTPManualMarkdownSettings(HOST=\"sftp.example.com\", USERNAME=\"user\", PASSWORD=\"password\", REMOTE_PATH=\"/documents\")\n    step = SFTPManualMarkdownStep(settings=settings)\n    markdown_docs = step.run(None)\n    ```\n    \"\"\"\n\n    def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n        \"\"\"Execute the step to retrieve Markdown files from SFTP server.\n\n        Args:\n            inpt: None (this is a leaf step)\n\n        Returns:\n            list[MarkdownDataContract]: List of loaded Markdown documents\n\n        Raises:\n            paramiko.SSHException: If connection or authentication fails\n            IOError: If file operations fail\n        \"\"\"\n        logger.info(\n            f\"Connecting to SFTP server {self.settings.HOST}:{self.settings.PORT}\",\n            extra={\"host\": self.settings.HOST, \"port\": self.settings.PORT, \"remote_path\": self.settings.REMOTE_PATH},\n        )\n\n        # Establish SFTP connection\n        transport = None\n        sftp = None\n        try:\n            # Create SSH transport\n            transport = paramiko.Transport((self.settings.HOST, self.settings.PORT))\n            transport.connect(\n                username=self.settings.USERNAME,\n                password=self.settings.PASSWORD.get_secret_value() if self.settings.PASSWORD.get_secret_value() != \"\" else None,\n                pkey=self._load_private_key() if self.settings.PRIVATE_KEY_PATH else None,\n            )\n\n            # Create SFTP client\n            sftp = paramiko.SFTPClient.from_transport(transport)\n\n            if sftp is None:\n                raise OSError(\"Failed to create SFTP client\")\n\n            # Find all .md files\n            md_files = self._find_markdown_files(sftp, self.settings.REMOTE_PATH)\n\n            logger.info(f\"Found {len(md_files)} Markdown files on SFTP server\", extra={\"file_count\": len(md_files)})\n\n            # Load each file into MarkdownDataContract\n            results: list[MarkdownDataContract] = []\n            for remote_file in md_files:\n                try:\n                    contract = self._load_markdown_from_sftp(sftp, remote_file)\n                    if contract:\n                        results.append(contract)\n                except (OSError, paramiko.SSHException) as e:\n                    logger.error(f\"Failed to load file {remote_file}: {e}\", extra={\"remote_file\": remote_file, \"error\": str(e)})\n\n            logger.info(\n                f\"Successfully loaded {len(results)} Markdown files from SFTP\",\n                extra={\"loaded_count\": len(results), \"total_found\": len(md_files)},\n            )\n            if len(results) == 0:\n                raise StepFailed(\"No Markdown files found or failed to load any\")\n\n            return results\n\n        finally:\n            # Clean up connections\n            if sftp:\n                sftp.close()\n            if transport:\n                transport.close()\n\n    def _load_private_key(self) -&gt; paramiko.PKey:\n        \"\"\"Load SSH private key from file.\n\n        Returns:\n            paramiko.PKey: Loaded private key\n\n        Raises:\n            paramiko.SSHException: If key cannot be loaded\n        \"\"\"\n        key_path = self.settings.PRIVATE_KEY_PATH\n        passphrase = (\n            self.settings.PRIVATE_KEY_PASSPHRASE.get_secret_value()\n            if self.settings.PRIVATE_KEY_PASSPHRASE.get_secret_value() != \"\"\n            else None\n        )\n\n        # Try different key types\n        for key_class in (paramiko.RSAKey, paramiko.Ed25519Key, paramiko.ECDSAKey):\n            try:\n                return key_class.from_private_key_file(str(key_path), password=passphrase)\n            except paramiko.SSHException:\n                continue\n\n        raise paramiko.SSHException(f\"Could not load private key from {key_path}\")\n\n    def _find_markdown_files(self, sftp: paramiko.SFTPClient, remote_path: str) -&gt; list[str]:\n        \"\"\"Recursively find all .md files in the remote path.\n\n        Args:\n            sftp: Active SFTP client connection\n            remote_path: Path to search for .md files\n\n        Returns:\n            list[str]: List of remote file paths\n        \"\"\"\n        md_files = []\n\n        try:\n            # List directory contents\n            for entry in sftp.listdir_attr(remote_path):\n                full_path = str(PurePosixPath(remote_path) / entry.filename)\n\n                # Check if it's a directory\n                if self._is_directory(entry):\n                    if self.settings.RECURSIVE:\n                        # Recursively search subdirectories\n                        md_files.extend(self._find_markdown_files(sftp, full_path))\n                elif entry.filename.endswith(\".md\"):\n                    # It's a markdown file\n                    md_files.append(full_path)\n\n        except OSError as e:\n            logger.warning(f\"Could not access directory {remote_path}: {e}\", extra={\"remote_path\": remote_path, \"error\": str(e)})\n\n        return md_files\n\n    def _is_directory(self, attr: paramiko.SFTPAttributes) -&gt; bool:\n        \"\"\"Check if an SFTP file attribute represents a directory.\n\n        Args:\n            attr: SFTP file attributes\n\n        Returns:\n            bool: True if it's a directory\n        \"\"\"\n        if attr.st_mode is None:\n            return False\n        return stat.S_ISDIR(attr.st_mode)\n\n    def _load_markdown_from_sftp(self, sftp: paramiko.SFTPClient, remote_file: str) -&gt; Optional[MarkdownDataContract]:\n        \"\"\"Load a Markdown file from SFTP and convert to MarkdownDataContract.\n\n        Args:\n            sftp: Active SFTP client connection\n            remote_file: Remote file path\n\n        Returns:\n            Optional[MarkdownDataContract]: Loaded contract or None if failed\n        \"\"\"\n        try:\n            # Download file to temporary location and use MarkdownDataContract.from_file\n            with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".md\", delete=False, encoding=\"utf-8\") as tmp_file:\n                # Read content from SFTP\n                with sftp.open(remote_file, \"r\") as remote:\n                    content = remote.read().decode(\"utf-8\")\n\n                # Write to temp file\n                tmp_file.write(content)\n                tmp_path = Path(tmp_file.name)\n\n            try:\n                # Use MarkdownDataContract.from_file to handle metadata parsing\n                url_prefix = f\"{self.__class__.__name__}/{remote_file}\"\n                contract = MarkdownDataContract.from_file(tmp_path, url_prefix=\"\")\n\n                # Override URL if not set in metadata to use remote file path\n                if contract.url == str(tmp_path.absolute()):\n                    contract.url = url_prefix\n\n                return contract\n            finally:\n                # Clean up temporary file\n                tmp_path.unlink(missing_ok=True)\n\n        except (OSError, paramiko.SSHException) as e:\n            raise StepFailed(f\"Failed to load markdown file {remote_file}: {e}\") from e\n</code></pre>"},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownStep-functions","title":"Functions","text":""},{"location":"steps/sftp/#wurzel.steps.sftp.SFTPManualMarkdownStep.run","title":"<code>run(inpt)</code>","text":"<p>Execute the step to retrieve Markdown files from SFTP server.</p> <p>Parameters:</p> Name Type Description Default <code>inpt</code> <code>None</code> <p>None (this is a leaf step)</p> required <p>Returns:</p> Type Description <code>list[MarkdownDataContract]</code> <p>list[MarkdownDataContract]: List of loaded Markdown documents</p> <p>Raises:</p> Type Description <code>SSHException</code> <p>If connection or authentication fails</p> <code>IOError</code> <p>If file operations fail</p> Source code in <code>wurzel/steps/sftp/sftp_manual_markdown.py</code> <pre><code>def run(self, inpt: None) -&gt; list[MarkdownDataContract]:\n    \"\"\"Execute the step to retrieve Markdown files from SFTP server.\n\n    Args:\n        inpt: None (this is a leaf step)\n\n    Returns:\n        list[MarkdownDataContract]: List of loaded Markdown documents\n\n    Raises:\n        paramiko.SSHException: If connection or authentication fails\n        IOError: If file operations fail\n    \"\"\"\n    logger.info(\n        f\"Connecting to SFTP server {self.settings.HOST}:{self.settings.PORT}\",\n        extra={\"host\": self.settings.HOST, \"port\": self.settings.PORT, \"remote_path\": self.settings.REMOTE_PATH},\n    )\n\n    # Establish SFTP connection\n    transport = None\n    sftp = None\n    try:\n        # Create SSH transport\n        transport = paramiko.Transport((self.settings.HOST, self.settings.PORT))\n        transport.connect(\n            username=self.settings.USERNAME,\n            password=self.settings.PASSWORD.get_secret_value() if self.settings.PASSWORD.get_secret_value() != \"\" else None,\n            pkey=self._load_private_key() if self.settings.PRIVATE_KEY_PATH else None,\n        )\n\n        # Create SFTP client\n        sftp = paramiko.SFTPClient.from_transport(transport)\n\n        if sftp is None:\n            raise OSError(\"Failed to create SFTP client\")\n\n        # Find all .md files\n        md_files = self._find_markdown_files(sftp, self.settings.REMOTE_PATH)\n\n        logger.info(f\"Found {len(md_files)} Markdown files on SFTP server\", extra={\"file_count\": len(md_files)})\n\n        # Load each file into MarkdownDataContract\n        results: list[MarkdownDataContract] = []\n        for remote_file in md_files:\n            try:\n                contract = self._load_markdown_from_sftp(sftp, remote_file)\n                if contract:\n                    results.append(contract)\n            except (OSError, paramiko.SSHException) as e:\n                logger.error(f\"Failed to load file {remote_file}: {e}\", extra={\"remote_file\": remote_file, \"error\": str(e)})\n\n        logger.info(\n            f\"Successfully loaded {len(results)} Markdown files from SFTP\",\n            extra={\"loaded_count\": len(results), \"total_found\": len(md_files)},\n        )\n        if len(results) == 0:\n            raise StepFailed(\"No Markdown files found or failed to load any\")\n\n        return results\n\n    finally:\n        # Clean up connections\n        if sftp:\n            sftp.close()\n        if transport:\n            transport.close()\n</code></pre>"},{"location":"steps/splitter/","title":"Splitter","text":"<p>The splitter step (also known as chunking) takes a long Markdown document (<code>*.md</code>) as the input and returns smaller splits (or chunks) that can easier processed by an embedding model or language model. The splitter keeps the length of the output chunks below a defined threshold (token limit) and tries to split without breaking the document context, e.g., split only at the end of a sentence and not within a sentence.</p>"},{"location":"steps/splitter/#semantic-splitter","title":"Semantic Splitter","text":"<p>Semantic document elements (e.g., headings) are repeated.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter","title":"<code>SemanticSplitter</code>","text":"<p>Splitter implementation.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter.__init__","title":"<code>__init__(token_limit=256, token_limit_buffer=32, token_limit_min=64, sentence_splitter_model='de_core_news_sm', repeat_table_header_row=True, tokenizer_model='gpt-3.5-turbo')</code>","text":"<p>Initializes the SemanticSplitter class with specified token limits and a sentence splitter model.</p> <p>Parameters:</p> Name Type Description Default <code>token_limit</code> <code>int</code> <p>The maximum number of tokens allowed. Defaults to 256.</p> <code>256</code> <code>token_limit_buffer</code> <code>int</code> <p>The buffer size for token limit to allow flexibility. Defaults to 32.</p> <code>32</code> <code>token_limit_min</code> <code>int</code> <p>The minimum number of tokens required. Defaults to 64.</p> <code>64</code> <code>sentence_splitter_model</code> <code>str</code> <p>The name of the sentence splitter model. Defaults to \"de_core_news_sm\".</p> <code>'de_core_news_sm'</code> <code>repeat_table_header_row</code> <code>bool</code> <p>If a table is splitted, the header is repeated. Defaults to True.</p> <code>True</code> <code>tokenizer_model</code> <code>str</code> <p>The name of the tokenizer model to use for encoding. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the specified sentence splitter cannot be loaded.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter.split_markdown_document","title":"<code>split_markdown_document(doc)</code>","text":"<p>Split a Markdown Document into Snippets.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.semantic_splitter.SemanticSplitter.text_sentences","title":"<code>text_sentences(text)</code>","text":"<p>Split a text into sentences using a sentence splitter model.</p> <p>This does not use a Regex based approach on purpose as they break with punctuation very easily see: https://stackoverflow.com/a/61254146</p>"},{"location":"steps/splitter/#table-splitter","title":"Table Splitter","text":"<p>For Markdown tables, a custom logic is implemented that preserves the table structure by repeating the header row if a split occurs within a table. So subsequent chunks maintain the semantic table information from the header row. By default, tables are never broken in the middle of a row; if a single row exceeds the budget, it is split at column boundaries instead and full-header is repeated.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil","title":"<code>MarkdownTableSplitterUtil</code>  <code>dataclass</code>","text":"<p>A class to split markdown tables into token-bounded chunks.</p> <p>This class encapsulates the logic for splitting large markdown tables while preserving table structure. Tables are never broken in the middle of a row; if a single row exceeds the max length, it is split at column boundaries instead and the full header is repeated.</p> <p>Example: <pre><code>&gt;&gt;&gt; from wurzel.utils.tokenizers import Tokenizer\n&gt;&gt;&gt; tokenizer = Tokenizer.from_name(\"cl100k_base\")\n&gt;&gt;&gt; splitter = MarkdownTableSplitterUtil(token_limit=8000, tokenizer=tokenizer)\n&gt;&gt;&gt; chunks = splitter.split(markdown_text)\n&gt;&gt;&gt; len(chunks)\n3\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>token_limit</code> <code>int</code> <p>Maximum tokens per chunk (model tokens, not characters).</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer used for counting tokens.</p> required <code>repeat_header_row</code> <code>bool</code> <p>If True, repeat the header row in each chunk. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>chunks</code> <code>list[str]</code> <p>Completed chunks of markdown.</p> <code>buf</code> <code>list[str]</code> <p>Current buffer of lines.</p> <code>buf_tok</code> <code>int</code> <p>Current token count in buffer.</p> <code>min_safety_token_limit</code> <code>int</code> <p>A minimum of 10 tokens is a safety threshold to ensure the splitter can always fit at least a minimal table structure in a chunk.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration after initialization.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.markdown_table_splitter.MarkdownTableSplitterUtil.split","title":"<code>split(md)</code>","text":"<p>Split a markdown document into token-bounded chunks while respecting tables.</p> <p>Parameters:</p> Name Type Description Default <code>md</code> <code>str</code> <p>str Markdown document.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Chunks whose token counts are &lt;= token_limit.</p>"},{"location":"steps/splitter/#sentence-splitter","title":"Sentence Splitter","text":"<p>The semantic splitter avoids splitting within sentences and to achieve this it relies on a sentence splitter. The sentence splitter takes longer text as input and splits the text into individual sentences. There are different implementations available.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter","title":"<code>RegexSentenceSplitter</code>","text":"<p>               Bases: <code>SentenceSplitter</code></p> <p>A sentence splitter based on regular expressions.</p> <p>NOTE: Using the regex splitter is not recommended since it based on very simple heuristics.</p> <p>Heuristics: - Split after sentence-ending punctuation (. ! ? \u2026) and any closing quotes/brackets. - Only split if the next non-space token looks like a sentence start   (capital letter or digit, optionally after an opening quote/paren). - Merge back false positives caused by common abbreviations, initials,   dotted acronyms (e.g., U.S.), decimals (e.g., 3.14), ordinals (No. 5), and ellipses.</p> <p>Notes: - Tweak <code>self.abbreviations</code> for your domain/corpus. - For chatty/poetic text where sentences may start lowercase, relax   <code>self._split_re</code>'s lookahead (see comment in init).</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a regex sentence splitter (compile regex, set abbreviations).</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.RegexSentenceSplitter.get_sentences","title":"<code>get_sentences(text)</code>","text":"<p>Split text into sentences.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter","title":"<code>SpacySentenceSplitter</code>","text":"<p>               Bases: <code>SentenceSplitter</code></p> <p>Adapter for Spacy sentence splitter.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter.__init__","title":"<code>__init__(nlp)</code>","text":"<p>Initialize a SpacySentenceSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>nlp</code> <code>Language</code> <p>A Spacy model from spacy.load().</p> required"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SpacySentenceSplitter.get_sentences","title":"<code>get_sentences(text)</code>","text":"<p>Split text into sentences.</p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter","title":"<code>SaTSentenceSplitter</code>","text":"<p>               Bases: <code>SentenceSplitter</code></p> <p>Adapter for wtpsplit's SaT sentence splitter.</p> <p>SaT (Segment any Text) is a state-of-the-art sentence splitter. Depending on the selected model you may want to use a GPU for faster inference.</p> <p>Available models and benchmark results:  https://github.com/segment-any-text/wtpsplit</p> <p>Example usage: <pre><code>splitter = SentenceSplitter.from_name(\"sat-3l\")\nsplitter.get_sentences(\"This is a test This is another test.\")\n# returns [\"This is a test \", \"This is another test.\"]\n</code></pre></p>"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter-functions","title":"Functions","text":""},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter.__init__","title":"<code>__init__(model_name_or_model)</code>","text":"<p>Initialize a SaTSentenceSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_model</code> <code>str</code> <p>A string or Path (Hugging Face ID or local directory path)</p> required"},{"location":"steps/splitter/#wurzel.utils.splitters.sentence_splitter.SaTSentenceSplitter.get_sentences","title":"<code>get_sentences(text)</code>","text":"<p>Split text into sentences.</p>"}]}