# SPDX-FileCopyrightText: 2025 Deutsche Telekom AG (opensource@telekom.de)
#
# SPDX-License-Identifier: Apache-2.0

import inspect
from pathlib import Path
from typing import TYPE_CHECKING, Any, TypedDict

import yaml
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict

import wurzel
import wurzel.cli
from wurzel.core import TypedStep
from wurzel.executors.backend.backend import Backend
from wurzel.executors.backend.values import load_values
from wurzel.executors.base_executor import BaseStepExecutor

if TYPE_CHECKING:
    from collections.abc import Iterable

    from wurzel.executors.middlewares.base import BaseMiddleware


class DvcDict(TypedDict):
    """Internal representation of a DVC pipeline stage.

    This dictionary maps directly to the format used in `dvc.yaml` for each stage.

    Fields:
        cmd (str): The command to execute this step.
        deps (list[str]): File paths that are inputs/dependencies to this step.
        outs (list[str]): Output file paths generated by this step.
        always_changed (bool): Indicates if this step should always re-run (typically True for leaf steps).
    """

    cmd: str
    deps: list[str]
    outs: list[str]
    always_changed: bool


class DvcConfig(BaseModel):
    """DVC pipeline configuration from YAML values."""

    dataDir: Path = Path("./data")
    encapsulateEnv: bool = True


class DvcTemplateValues(BaseModel):
    """YAML values file parsed into strongly typed configuration for DVC."""

    dvc: dict[str, DvcConfig] = Field(default_factory=dict)


def select_pipeline(values: DvcTemplateValues, pipeline_name: str | None) -> DvcConfig:
    """Select a pipeline configuration either by name or falling back to the first entry."""
    if pipeline_name:
        try:
            return values.dvc[pipeline_name]
        except KeyError as exc:
            raise ValueError(f"pipeline '{pipeline_name}' not found in values") from exc
    if values.dvc:
        first_key = next(iter(values.dvc))
        return values.dvc[first_key]
    return DvcConfig()


class DvcBackendSettings(BaseSettings):
    """Settings object for DVC backend configuration, injectable via environment variables.

    Environment Variables:
        - DVCBACKEND__DATA_DIR: Directory path to place generated output artifacts.
        - DVCBACKEND__ENCAPSULATE_ENV: Whether to encapsulate the environment (True/False).

    Attributes:
        DATA_DIR (Path): Output directory for generated step artifacts.
        ENCAPSULATE_ENV (bool): Flag to determine if environment encapsulation is used in CLI generation.

    """

    model_config = SettingsConfigDict(env_prefix="DVCBACKEND__")
    DATA_DIR: Path = Path("./data")
    ENCAPSULATE_ENV: bool = True


class DvcBackend(Backend):
    """DVC-specific backend implementation for the Wurzel abstraction layer.

    This adapter generates DVC-compatible `dvc.yaml` files from typed step definitions.
    It recursively resolves all step dependencies and constructs CLI commands for DVC execution.

    Inherits from Backend (which inherits from BaseStepExecutor), providing both step
    execution capabilities and artifact generation for DVC pipelines.

    Args:
        config (DvcConfig | None): Optional config from YAML values.
        settings (DvcBackendSettings | None): Optional settings object; if not provided,
            defaults will be loaded from environment or defaults.
        executer (BaseStepExecutor): Executor class used to wrap the CLI call (deprecated, kept for compatibility).
        dont_encapsulate: If True, don't encapsulate environment variables
        middlewares: List of middleware names or instances to use
        load_middlewares_from_env: Whether to load middlewares from MIDDLEWARES env var

    """

    @classmethod
    def is_available(cls) -> bool:
        """DVC backend has no optional dependencies."""
        return True

    def __init__(
        self,
        config: DvcConfig | None = None,
        *,
        settings: DvcBackendSettings | None = None,
        executer: type[BaseStepExecutor] = BaseStepExecutor,
        dont_encapsulate: bool = False,
        middlewares: list[str] | list["BaseMiddleware"] | None = None,
        load_middlewares_from_env: bool = False,
    ) -> None:
        """Initialize DvcBackend.

        Args:
            config: DVC-specific config from YAML values
            settings: DVC-specific settings for pipeline generation
            executer: Executor class used for CLI generation (deprecated, kept for compatibility)
            dont_encapsulate: If True, don't encapsulate environment variables
            middlewares: List of middleware names or instances to use
            load_middlewares_from_env: Whether to load middlewares from MIDDLEWARES env var
        """
        super().__init__(
            executer, dont_encapsulate=dont_encapsulate, middlewares=middlewares, load_middlewares_from_env=load_middlewares_from_env
        )
        self.settings = settings or DvcBackendSettings()
        self.config = config or DvcConfig(
            dataDir=self.settings.DATA_DIR,
            encapsulateEnv=self.settings.ENCAPSULATE_ENV,
        )
        self.executor: type[BaseStepExecutor] = executer

    @classmethod
    def from_values(cls, files: "Iterable[Path]", workflow_name: str | None = None) -> "DvcBackend":
        """Instantiate the backend from values files.

        Args:
            files: Iterable of paths to YAML values files
            workflow_name: Optional workflow name to select from values file

        Returns:
            DvcBackend: Instance configured from the merged values files
        """
        values = load_values(files, DvcTemplateValues)
        config = select_pipeline(values, workflow_name)
        return cls(config=config)

    def _generate_dict(
        self,
        step: TypedStep,
    ) -> dict[str, DvcDict]:
        """Recursively generates a dictionary representing a full DVC pipeline,
        including all dependencies of the given step.

        Each step is represented as a `stage` entry in the DVC pipeline, with corresponding
        `cmd`, `deps`, `outs`, and `always_changed` fields.

        Args:
            step (TypedStep): The root step from which to generate the pipeline DAG.

        Returns:
            dict[str, DvcDict]: A dictionary mapping step names to their DVC stage definitions.

        """
        result: dict[str, Any] = {}
        outputs_of_deps: list[Path] = []

        for o_step in step.required_steps:
            dep_result = self._generate_dict(o_step)
            result |= dep_result
            outputs_of_deps += dep_result[o_step.__class__.__name__]["outs"]

        output_path = self.config.dataDir / step.__class__.__name__

        cli_call = wurzel.cli.generate_cli_call(
            step.__class__,
            inputs=outputs_of_deps,
            output=output_path,
            backend=self.__class__,
            encapsulate_env=self.config.encapsulateEnv,
        )

        # Prepend WURZEL_RUN_ID environment variable to the command
        # DVC will generate a unique ID at runtime using timestamp
        cmd = f'WURZEL_RUN_ID="${{WURZEL_RUN_ID:-dvc-$(date +%Y%m%d-%H%M%S)-$$}}" {cli_call}'

        return result | {
            step.__class__.__name__: {
                "cmd": cmd,
                "deps": [inspect.getfile(step.__class__), *outputs_of_deps],
                "outs": [output_path],
                "always_changed": step.is_leaf(),  # Forces re-run for leaf steps
            }
        }

    def generate_artifact(
        self,
        step: TypedStep,
    ) -> str:
        """Converts the full step graph into a valid `dvc.yaml` file content.

        Ensures all paths (in `outs` and `deps`) are converted to strings for YAML serialization.

        Args:
            step (TypedStep): Root step of the pipeline.

        Returns:
            str: A YAML string containing the full DVC pipeline definition.

        """
        data = self._generate_dict(step)

        # Convert all Path objects to strings for YAML compatibility
        for k in data:
            for key in ["outs", "deps"]:
                data[k][key] = [str(p) for p in data[k][key]]

        return yaml.dump({"stages": data})
